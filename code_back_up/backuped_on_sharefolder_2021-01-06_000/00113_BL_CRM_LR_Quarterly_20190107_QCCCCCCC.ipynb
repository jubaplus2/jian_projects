{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To check the 1st date of SP's transaction \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='crmnewscore_QC_20190123.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "activemos = '2017-12-29'\n",
    "lapsed = '2017-06-29'\n",
    "lastdate = '2018-12-29'\n",
    "\n",
    "\n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update/checking_crm_newscore_20190107/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "# Adding control members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-01-23 17:47:54.183319\n",
      "2 2019-01-23 17:58:28.603764\n",
      "3 2019-01-23 18:02:06.105138\n",
      "4 2019-01-23 18:07:57.197270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a21e32facda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcount_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_transaction_amt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_transaction_amt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_transaction_units'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_transaction_units'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#To check the 1st date\n",
    "\n",
    "chunksize_num = 10**7\n",
    "filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "dftrans_before_20180922=pd.DataFrame()\n",
    "count_i=0\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str):\n",
    "    chunk['total_transaction_amt']=chunk['total_transaction_amt'].astype(float)\n",
    "    chunk['total_transaction_units']=chunk['total_transaction_units'].astype(float)\n",
    "    chunk = chunk[['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_units',\n",
    "                   'total_transaction_amt']].drop_duplicates()\n",
    "    dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "    count_i+=1\n",
    "    print(count_i,datetime.datetime.now())\n",
    "    \n",
    "\n",
    "'''\n",
    "dftrans_before_20180922 = pd.read_csv('/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update/crm_newscore_0922/combinedtransactions_0922.csv',dtype=str)\n",
    "#dftrans = dftrans[dftrans['transaction_date']>=lapsed]\n",
    "dftrans_before_20180922['total_transaction_amt']=dftrans_before_20180922['total_transaction_amt'].astype(float)\n",
    "dftrans_before_20180922['total_transaction_units']=dftrans_before_20180922['total_transaction_units'].astype(float)\n",
    "\n",
    "dftrans_before_20180922 = dftrans_before_20180922[['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_units',\n",
    "                   'total_transaction_amt']].drop_duplicates()\n",
    "'''\n",
    "\n",
    "del chunk\n",
    "\n",
    "dftrans_before_20180922=dftrans_before_20180922.drop_duplicates()\n",
    "\n",
    "print(\"Deduped\",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "weeks_after_20180922=[datetime.date(2018,9,29)+datetime.timedelta(days=x*7) for x in range(999)]\n",
    "possible_recent_folders=[\"/home/jian/BigLots/MediaStorm_\"+str(x)+\"/\" for x in weeks_after_20180922]\n",
    "recent_file_list=[]\n",
    "for dirc in possible_recent_folders:\n",
    "    list_recent=[x for x in list(recursive_file_gen(dirc)) if (\"DailySales\" in x) & (\".txt\" in x) ]\n",
    "    recent_file_list=recent_file_list+list_recent\n",
    "recent_file_df=pd.DataFrame({\"path\":recent_file_list,\"date\":[datetime.datetime.strptime(x.split(\"DailySales\")[1][:8],\"%Y%m%d\").date()-datetime.timedelta(days=3) for x in recent_file_list]},index=[x for x in range(len(recent_file_list))])\n",
    "\n",
    "\n",
    "list_1_after_201806=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\")) if (\"DailySales\" in x) & (\".txt\" in x) ]\n",
    "folder_date=[datetime.datetime.strptime(x.split(\"/\")[len(x.split(\"/\"))-2].split(\"_\")[1],\"%Y-%m-%d\").date() for x in list_1_after_201806]\n",
    "df_1_after_201806=pd.DataFrame({\"date\":folder_date,\"path\":list_1_after_201806},index=[x for x in range(len(list_1_after_201806))])\n",
    "df_1_after_201806['date'].apply(lambda x: x.weekday()).unique()\n",
    "df_1_after_201806=df_1_after_201806.sort_values(\"date\").reset_index()\n",
    "del df_1_after_201806['index']\n",
    "new_dailysales_files=df_1_after_201806.append(recent_file_df)\n",
    "\n",
    "new_dailysales_files=new_dailysales_files[new_dailysales_files['date']>datetime.date(2018,9,22)]\n",
    "new_dailysales_files=new_dailysales_files[new_dailysales_files['date']<=datetime.date(2018,12,29)]\n",
    "\n",
    "print(len(new_dailysales_files))\n",
    "new_dailysales_files=new_dailysales_files['path'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 14\n",
      "1 done 2019-01-17 14:13:22.048161\n",
      "2 done 2019-01-17 14:14:21.367281\n",
      "3 done 2019-01-17 14:14:59.648293\n",
      "4 done 2019-01-17 14:15:45.503732\n",
      "5 done 2019-01-17 14:16:35.916216\n",
      "6 done 2019-01-17 14:17:28.580887\n",
      "7 done 2019-01-17 14:18:25.035073\n",
      "8 done 2019-01-17 14:19:30.540994\n",
      "9 done 2019-01-17 14:21:00.269744\n",
      "10 done 2019-01-17 14:22:30.249166\n",
      "11 done 2019-01-17 14:24:08.629091\n",
      "12 done 2019-01-17 14:25:48.781103\n",
      "13 done 2019-01-17 14:27:47.376541\n",
      "14 done 2019-01-17 14:29:13.889194\n"
     ]
    }
   ],
   "source": [
    "combined_rewards_transaction_after_20180922_agg=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,nrows = None,sep= '|',dtype =str)\n",
    "    df['subclass_transaction_amt']=df['subclass_transaction_amt'].astype(float)\n",
    "    df['subclass_transaction_units']=df['subclass_transaction_units'].astype(float)\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df_sales=df.groupby(['location_id','transaction_dt','customer_id_hashed'])['subclass_transaction_amt','subclass_transaction_units'].sum().reset_index().rename(columns={\"subclass_transaction_amt\":\"total_transaction_amt\",\"subclass_transaction_units\":\"total_transaction_units\"})\n",
    "    df_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates().groupby(['location_id','transaction_dt','customer_id_hashed'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"transactions\"})\n",
    "    df=pd.merge(df_sales,df_trans,on=['location_id','transaction_dt','customer_id_hashed'],how=\"left\")\n",
    "    combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "    \n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"transaction_dt\":\"transaction_date\"})\n",
    "\n",
    "# combined_rewards_transaction_after_20180922_agg.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update/combined_agged_rewards_transactions_20180929_20181229.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the store for an id\n",
    "\n",
    "df_2018_transaction_by_id_1=dftrans_before_20180922[dftrans_before_20180922['transaction_date'].apply(lambda x: x[:4]==\"2018\")]\n",
    "df_2018_transaction_by_id_1=df_2018_transaction_by_id_1[df_2018_transaction_by_id_1['total_transaction_amt']>0]\n",
    "df_2018_transaction_by_id_1=df_2018_transaction_by_id_1.groupby(['customer_id_hashed','location_id'])['total_transaction_amt'].count().to_frame().reset_index()\n",
    "df_2018_transaction_by_id_1=df_2018_transaction_by_id_1.rename(columns={\"total_transaction_amt\":\"trans\"})\n",
    "\n",
    "\n",
    "df_2018_transaction_by_id_2=combined_rewards_transaction_after_20180922_agg[combined_rewards_transaction_after_20180922_agg['total_transaction_amt']>0]\n",
    "df_2018_transaction_by_id_2=df_2018_transaction_by_id_2.groupby(['customer_id_hashed','location_id'])['transactions'].sum().to_frame().reset_index().rename(columns={\"transactions\":\"trans\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2018_transaction_by_id=df_2018_transaction_by_id_1.append(df_2018_transaction_by_id_2)\n",
    "df_2018_transaction_by_id=df_2018_transaction_by_id.groupby(['customer_id_hashed','location_id'])['trans'].sum().to_frame().reset_index()\n",
    "df_2018_transaction_by_id=df_2018_transaction_by_id.sort_values(['customer_id_hashed','trans'],ascending=[True,False]).drop_duplicates(['customer_id_hashed'])\n",
    "df_2018_transaction_by_id.to_csv(folder_write+\"id_by_store_based_on_2018_trans.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>location_id</th>\n",
       "      <th>trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001dadc0265bf9d250566d74e0006323f18b5826641...</td>\n",
       "      <td>4061</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed location_id  trans\n",
       "0  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        1292      1\n",
       "3  000001dadc0265bf9d250566d74e0006323f18b5826641...        4061      3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_2018_transaction_by_id_1\n",
    "del df_2018_transaction_by_id_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-26\n",
      "2018-12-29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###get recency\n",
    "dfrecency=combined_rewards_transaction_after_20180922_agg[['customer_id_hashed','transaction_date']].append(dftrans_before_20180922[['customer_id_hashed','transaction_date']]) #Allready combined\n",
    "dfrecency = dfrecency[['customer_id_hashed','transaction_date']].drop_duplicates()\n",
    "print (min(dfrecency['transaction_date']))\n",
    "print (max(dfrecency['transaction_date']))\n",
    "dfrecency = dfrecency.sort_values(['transaction_date'],ascending = False)\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfrecency['transaction_date'] = pd.to_datetime(dfrecency['transaction_date'])\n",
    "dfrecency['recency'] =  datetime.datetime.strptime(str(lastdate), '%Y-%m-%d').date() - dfrecency['transaction_date']\n",
    "dfrecency['recency'] = dfrecency['recency'].apply(lambda x:x.days)\n",
    "dfrecency['recency'] = np.ceil((dfrecency['recency']+1)/30)\n",
    "\n",
    "dfrecency = dfrecency[['customer_id_hashed','recency']]\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency2.csv',index = False)\n",
    "\n",
    "print(\"saved dfrecency2: \",dfrecency.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "dfiddetail2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail = dfiddetail.append(dfiddetail2,ignore_index = True)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "dfiddetail = dfiddetail.drop_duplicates('email_address_hash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrans_before_20180922['transactions'] = 1\n",
    "dftrans_before_20180922 = dftrans_before_20180922[['customer_id_hashed','total_transaction_amt',\n",
    "                   'total_transaction_units','transactions']].groupby(['customer_id_hashed']).sum().reset_index()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg[['customer_id_hashed','total_transaction_amt',\n",
    "                   'total_transaction_units','transactions']].groupby(['customer_id_hashed']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrans_before_20180922=dftrans_before_20180922.append(combined_rewards_transaction_after_20180922_agg) #Allready combined\n",
    "dftrans_before_20180922 = dftrans_before_20180922[['customer_id_hashed','total_transaction_amt',\n",
    "                   'total_transaction_units','transactions']].groupby(['customer_id_hashed']).sum().reset_index()\n",
    "\n",
    "dftotal=dftrans_before_20180922 #Allready combined\n",
    "\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency,on = 'customer_id_hashed',how='outer')\n",
    "del dfrecency\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal = dftotal.sort_values(['transactions','recency','total_transaction_amt'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Transindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['total_transaction_amt','recency','transactions'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Amtindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['recency','transactions','total_transaction_amt'],ascending = [1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'recencyindex'})\n",
    "\n",
    "c_ids = len(dftotal.index)\n",
    "logging.info('total customers from transaction and amt: ')\n",
    "logging.info(c_ids)\n",
    "c_ids = np.ceil(c_ids/5.0)\n",
    "\n",
    "dftotal['Transindex'] = np.ceil((dftotal['Transindex']+1)/c_ids)\n",
    "dftotal['Amtindex'] = np.ceil((dftotal['Amtindex']+1)/c_ids)\n",
    "dftotal['recencyindex'] = np.ceil((dftotal['recencyindex']+1)/c_ids)\n",
    "\n",
    "dftotal['RFM'] = dftotal['recencyindex']*100 + dftotal['Transindex']*10 + dftotal['Amtindex']\n",
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'total_transaction_amt'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm.csv',index = False)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "dftotal = pd.read_csv(folder_write + 'dfrfm.csv')\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "dftotal = dftotal[['customer_id_hashed','frmindex']]\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "dfrecency = pd.read_csv(folder_write + 'dfrecency.csv')\n",
    "dfrecency['active'] = np.where(dfrecency['transaction_date']>=activemos,'active',\n",
    "                    np.where(dfrecency['transaction_date']>=lapsed,'lapsed','other'))\n",
    "dfrecency['active'].unique()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency[['customer_id_hashed','active']],on = 'customer_id_hashed')\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].astype('str')\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].str[0:5]\n",
    "dfiddetail['customer_zip_code'].fillna('00000',inplace = True)\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].apply(lambda x:x.zfill(5))\n",
    "print(len(dfiddetail.index))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "print(\"totalids_trans:\",len(dftotal.index))\n",
    "dftotal = pd.merge(dftotal,dfiddetail,on = 'customer_id_hashed')\n",
    "print(\"totalids_trans_mergewithmaster:\",len(dftotal.index))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "dftotal = pd.merge(dftotal,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "dftotal['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_wemail.csv',index = False)\n",
    "print(\"Final wemailcsv:\",dftotal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detailed_scores_df=pd.read_csv(folder_write + 'dfrfm_wemail.csv',dtype=str)\n",
    "detailed_scores_df=dftotal\n",
    "detailed_scores_df['frmindex']=detailed_scores_df['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "detailed_scores_df['customer_zip_code']=detailed_scores_df['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "detailed_scores_df['frmindex']=detailed_scores_df['frmindex'].apply(lambda x:\"D\"+x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random 500000 ids as control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-01-17 11:23:46.639555\n",
      "2 2019-01-17 11:23:48.224337\n",
      "3 2019-01-17 11:23:49.963513\n",
      "4 2019-01-17 11:23:52.130970\n",
      "5 2019-01-17 11:23:54.260448\n",
      "6 2019-01-17 11:23:56.401059\n",
      "7 2019-01-17 11:23:58.224527\n",
      "8 2019-01-17 11:24:00.060439\n",
      "9 2019-01-17 11:24:01.875253\n",
      "10 2019-01-17 11:24:03.793053\n",
      "11 2019-01-17 11:24:05.792031\n",
      "12 2019-01-17 11:24:07.853883\n",
      "13 2019-01-17 11:24:09.988217\n",
      "14 2019-01-17 11:24:12.199976\n",
      "15 2019-01-17 11:24:14.248272\n",
      "16 2019-01-17 11:24:16.330673\n",
      "17 2019-01-17 11:24:18.434372\n",
      "18 2019-01-17 11:24:20.578801\n",
      "19 2019-01-17 11:24:22.737708\n",
      "20 2019-01-17 11:24:24.942990\n",
      "21 2019-01-17 11:24:27.225624\n",
      "22 2019-01-17 11:24:29.586495\n",
      "23 2019-01-17 11:24:31.837453\n",
      "24 2019-01-17 11:24:34.126162\n",
      "25 2019-01-17 11:24:36.809908\n",
      "26 2019-01-17 11:24:39.751901\n",
      "27 2019-01-17 11:24:42.536332\n",
      "28 2019-01-17 11:24:45.223852\n",
      "29 2019-01-17 11:24:47.964946\n",
      "30 2019-01-17 11:24:50.785773\n",
      "31 2019-01-17 11:24:53.537397\n",
      "32 2019-01-17 11:24:56.266120\n",
      "33 2019-01-17 11:24:58.958613\n",
      "34 2019-01-17 11:25:01.670770\n",
      "35 2019-01-17 11:25:04.373708\n",
      "36 2019-01-17 11:25:07.009579\n",
      "37 2019-01-17 11:25:10.669409\n",
      "38 2019-01-17 11:25:14.724441\n",
      "39 2019-01-17 11:25:18.162005\n",
      "40 2019-01-17 11:25:21.670524\n",
      "41 2019-01-17 11:25:25.147431\n",
      "42 2019-01-17 11:25:28.577298\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "total_rows=len(detailed_scores_df)\n",
    "\n",
    "test_all_df=pd.DataFrame()\n",
    "control_all_df=pd.DataFrame()\n",
    "\n",
    "i_counter=0\n",
    "\n",
    "for comb,group in detailed_scores_df.groupby(['active','zipcodegroup','frmindex']):\n",
    "    random_list=random.sample(range(len(group)), int(np.round(len(group)/total_rows*500000)))\n",
    "\n",
    "    group=group.reset_index()\n",
    "    del group['index']\n",
    "    group=group.reset_index()\n",
    "    df_control=group[group['index'].isin(random_list)]\n",
    "    df_test=group[~group['index'].isin(random_list)]\n",
    "    \n",
    "    test_all_df=test_all_df.append(df_test)\n",
    "    control_all_df=control_all_df.append(df_control)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all_df['segment']=\"T_\"+test_all_df['active']+\"_chain_\"+test_all_df['zipcodegroup']+\"_\"+test_all_df['frmindex']+\"_2019Q1\"\n",
    "control_all_df['segment']=\"C_\"+control_all_df['active']+\"_chain_\"+control_all_df['zipcodegroup']+\"_\"+control_all_df['frmindex']+\"_2019Q1\"\n",
    "\n",
    "\n",
    "test_all_df['HML_group']=np.where(test_all_df['frmindex'].isin(['D01','D02','D03']),\"H\",\n",
    "                                 np.where(test_all_df['frmindex'].isin(['D04','D05','D06']),\"M\",\"L\"))\n",
    "control_all_df['HML_group']=np.where(control_all_df['frmindex'].isin(['D01','D02','D03']),\"H\",\n",
    "                                 np.where(control_all_df['frmindex'].isin(['D04','D05','D06']),\"M\",\"L\"))\n",
    "\n",
    "test_all_df['segment_new']=\"T_\"+test_all_df['active']+test_all_df['zipcodegroup']+\"_\"+test_all_df['HML_group']+\"_2019Q1\"\n",
    "control_all_df['segment_new']=\"C_\"+control_all_df['active']+control_all_df['zipcodegroup']+\"_\"+control_all_df['HML_group']+\"_2019Q1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all_df.to_csv(folder_write+\"all_test.csv\",index=False)\n",
    "control_all_df.to_csv(folder_write+\"all_control.csv\",index=False)\n",
    "\n",
    "folder_write_inner = folder_write+'/by_group/'\n",
    "try:\n",
    "    os.stat(folder_write_inner)\n",
    "except:\n",
    "    os.mkdir(folder_write_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-01-17 11:38:18.450303\n",
      "2 2019-01-17 11:38:26.170574\n",
      "3 2019-01-17 11:38:35.170873\n",
      "4 2019-01-17 11:38:42.044620\n",
      "5 2019-01-17 11:38:50.695546\n",
      "6 2019-01-17 11:38:57.581566\n",
      "7 2019-01-17 11:39:02.339104\n",
      "8 2019-01-17 11:39:06.218479\n",
      "9 2019-01-17 11:39:08.795167\n",
      "10 2019-01-17 11:39:10.933404\n",
      "11 2019-01-17 11:39:13.120461\n",
      "12 2019-01-17 11:39:15.299168\n",
      "13 2019-01-17 11:39:17.533514\n",
      "14 2019-01-17 11:39:19.385378\n",
      "15 2019-01-17 11:39:20.485718\n",
      "16 2019-01-17 11:39:21.463754\n",
      "17 2019-01-17 11:39:22.279139\n",
      "18 2019-01-17 11:39:23.155093\n",
      "19 2019-01-17 11:39:23.942558\n",
      "20 2019-01-17 11:39:24.872037\n",
      "21 2019-01-17 11:39:25.838834\n",
      "22 2019-01-17 11:39:26.790691\n",
      "23 2019-01-17 11:39:27.299259\n",
      "24 2019-01-17 11:39:27.889435\n",
      "25 2019-01-17 11:39:32.141296\n",
      "26 2019-01-17 11:39:36.524750\n",
      "27 2019-01-17 11:39:37.647353\n",
      "28 2019-01-17 11:39:38.321829\n",
      "29 2019-01-17 11:39:39.349712\n",
      "30 2019-01-17 11:39:40.543704\n",
      "31 2019-01-17 11:39:40.821287\n",
      "32 2019-01-17 11:39:40.998980\n",
      "33 2019-01-17 11:39:41.485993\n",
      "34 2019-01-17 11:39:42.224325\n",
      "35 2019-01-17 11:39:42.371212\n",
      "36 2019-01-17 11:39:42.475716\n",
      "37 2019-01-17 11:39:50.006412\n",
      "38 2019-01-17 11:39:58.282594\n",
      "39 2019-01-17 11:40:00.482275\n",
      "40 2019-01-17 11:40:02.713942\n",
      "41 2019-01-17 11:40:03.785929\n",
      "42 2019-01-17 11:40:04.912620\n"
     ]
    }
   ],
   "source": [
    "i_counter=0\n",
    "for seg,group in test_all_df.groupby(['segment']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment']]\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-01-17 11:40:06.719322\n",
      "2 2019-01-17 11:40:06.906427\n",
      "3 2019-01-17 11:40:07.113988\n",
      "4 2019-01-17 11:40:07.266616\n",
      "5 2019-01-17 11:40:07.467941\n",
      "6 2019-01-17 11:40:07.626507\n",
      "7 2019-01-17 11:40:07.727921\n",
      "8 2019-01-17 11:40:07.808633\n",
      "9 2019-01-17 11:40:07.854179\n",
      "10 2019-01-17 11:40:07.899033\n",
      "11 2019-01-17 11:40:07.945304\n",
      "12 2019-01-17 11:40:07.985915\n",
      "13 2019-01-17 11:40:08.032385\n",
      "14 2019-01-17 11:40:08.074066\n",
      "15 2019-01-17 11:40:08.099599\n",
      "16 2019-01-17 11:40:08.123096\n",
      "17 2019-01-17 11:40:08.142270\n",
      "18 2019-01-17 11:40:08.165594\n",
      "19 2019-01-17 11:40:08.187836\n",
      "20 2019-01-17 11:40:08.210726\n",
      "21 2019-01-17 11:40:08.232505\n",
      "22 2019-01-17 11:40:08.254725\n",
      "23 2019-01-17 11:40:08.268687\n",
      "24 2019-01-17 11:40:08.283985\n",
      "25 2019-01-17 11:40:08.386008\n",
      "26 2019-01-17 11:40:08.487732\n",
      "27 2019-01-17 11:40:08.514707\n",
      "28 2019-01-17 11:40:08.531700\n",
      "29 2019-01-17 11:40:08.557099\n",
      "30 2019-01-17 11:40:08.586050\n",
      "31 2019-01-17 11:40:08.595026\n",
      "32 2019-01-17 11:40:08.600859\n",
      "33 2019-01-17 11:40:08.613816\n",
      "34 2019-01-17 11:40:08.632488\n",
      "35 2019-01-17 11:40:08.637758\n",
      "36 2019-01-17 11:40:08.641924\n",
      "37 2019-01-17 11:40:08.813858\n",
      "38 2019-01-17 11:40:08.992403\n",
      "39 2019-01-17 11:40:09.038929\n",
      "40 2019-01-17 11:40:09.085609\n",
      "41 2019-01-17 11:40:09.109384\n",
      "42 2019-01-17 11:40:09.134584\n"
     ]
    }
   ],
   "source": [
    "i_counter=0\n",
    "for seg,group in control_all_df.groupby(['segment']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment']]\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
