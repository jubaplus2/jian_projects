{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "# Add the new folders of Jian later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "last_saturday=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()+2)\n",
    "\n",
    "write_folder=\"/home/jian/celery/Append_to_celery_LiveRamp/monthly_update/output_\"+str(last_saturday)+\"/\"\n",
    "try:\n",
    "    os.stat(write_folder)\n",
    "except:\n",
    "    os.mkdir(write_folder)\n",
    "\n",
    "week_beginning=last_saturday-datetime.timedelta(days=28)\n",
    "\n",
    "recent_3_weeks=[last_saturday-datetime.timedelta(days=x*7) for x in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Checking for the recent weeks that haven't moved to the folder\n",
    "possible_recent_folders=[\"/home/jian/BigLots/MediaStorm_\"+str(x)+\"/\" for x in recent_3_weeks]\n",
    "recent_file_list=[]\n",
    "for dirc in possible_recent_folders:\n",
    "    list_recent=[x for x in list(recursive_file_gen(dirc)) if (\"MasterBiWeekly\" in x) & (\".txt\" in x) ]\n",
    "    recent_file_list=recent_file_list+list_recent\n",
    "recent_file_df=pd.DataFrame({\"path\":recent_file_list,\"date\":[datetime.datetime.strptime(x.split(\"MasterBiWeekly\")[1][:8],\"%Y%m%d\").date()-datetime.timedelta(days=3) for x in recent_file_list]},index=[x for x in range(len(recent_file_list))])\n",
    "\n",
    "\n",
    "list_1_after_201806=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\")) if (\"MasterBiWeekly\" in x) & (\".txt\" in x) ]\n",
    "folder_date=[datetime.datetime.strptime(x.split(\"/\")[len(x.split(\"/\"))-2].split(\"_\")[1],\"%Y-%m-%d\").date() for x in list_1_after_201806]\n",
    "df_1_after_201806=pd.DataFrame({\"date\":folder_date,\"path\":list_1_after_201806},index=[x for x in range(len(list_1_after_201806))])\n",
    "df_1_after_201806['date'].apply(lambda x: x.weekday()).unique()\n",
    "df_1_after_201806=df_1_after_201806.sort_values(\"date\").reset_index()\n",
    "df_1_after_201806=df_1_after_201806[df_1_after_201806['date']>week_beginning]\n",
    "del df_1_after_201806['index']\n",
    "\n",
    "new_2_biweekly_files=df_1_after_201806.append(recent_file_df)\n",
    "\n",
    "print(len(new_2_biweekly_files))\n",
    "new_2_biweekly_files=new_2_biweekly_files['path'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440423\n",
      "440423\n"
     ]
    }
   ],
   "source": [
    "if len(new_2_biweekly_files)==2:\n",
    "    df_all_new_master = pd.DataFrame()\n",
    "    for file in new_2_biweekly_files:\n",
    "        df = pd.read_csv(file,nrows = None,sep= '|',usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'],\n",
    "                         dtype =str)\n",
    "        df_all_new_master = df_all_new_master.append(df,ignore_index = True)\n",
    "    print(len(df_all_new_master.index))\n",
    "    df_all_new_master = df_all_new_master.drop_duplicates('email_address_hash')\n",
    "    print(len(df_all_new_master.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02 15:11:54.477794 440415\n",
      "2019-01-02 15:11:55.060427 440415\n",
      "2019-01-02 15:11:55.578597 440414\n",
      "2019-01-02 15:11:56.759163 440411\n",
      "2019-01-02 15:11:56.920722 440411\n",
      "2019-01-02 15:11:57.063515 440411\n",
      "2019-01-02 15:11:57.164023 440411\n",
      "2019-01-02 15:11:57.391369 440410\n",
      "2019-01-02 15:12:02.654643 440404\n",
      "2019-01-02 15:12:03.825948 440404\n",
      "2019-01-02 15:12:03.943858 440404\n",
      "2019-01-02 15:12:04.788939 440402\n",
      "2019-01-02 15:12:05.642445 440400\n",
      "2019-01-02 15:12:06.177549 440400\n",
      "2019-01-02 15:12:09.104883 440391\n",
      "2019-01-02 15:12:10.040544 440391\n",
      "2019-01-02 15:12:14.186004 440377\n",
      "2019-01-02 15:12:14.511591 440377\n",
      "2019-01-02 15:12:14.739704 440377\n",
      "2019-01-02 15:12:14.892085 440377\n",
      "2019-01-02 15:12:15.070138 440377\n",
      "2019-01-02 15:12:15.296827 440375\n",
      "2019-01-02 15:12:16.836934 440374\n",
      "2019-01-02 15:12:22.549095 440367\n",
      "2019-01-02 15:12:24.626808 440363\n",
      "2019-01-02 15:12:29.939186 440359\n",
      "2019-01-02 15:12:33.527380 440355\n",
      "2019-01-02 15:12:34.961897 440354\n",
      "2019-01-02 15:12:35.522908 440354\n",
      "2019-01-02 15:12:36.062540 440354\n",
      "2019-01-02 15:12:36.328281 440354\n",
      "2019-01-02 15:12:36.585311 440354\n",
      "2019-01-02 15:12:36.699101 440354\n",
      "2019-01-02 15:12:40.260410 440352\n",
      "2019-01-02 15:12:40.541056 440352\n",
      "2019-01-02 15:12:40.895018 440352\n",
      "2019-01-02 15:12:41.772446 440350\n",
      "2019-01-02 15:12:42.361078 440350\n",
      "2019-01-02 15:12:42.556127 440350\n",
      "2019-01-02 15:12:43.742924 440349\n",
      "2019-01-02 15:12:45.452512 440347\n",
      "2019-01-02 15:12:47.795385 440347\n",
      "2019-01-02 15:12:48.316246 440347\n",
      "2019-01-02 15:12:48.537763 440347\n",
      "2019-01-02 15:12:49.462362 440344\n",
      "2019-01-02 15:12:49.669691 440344\n",
      "2019-01-02 15:12:50.034398 440343\n",
      "2019-01-02 15:12:50.300423 440343\n",
      "2019-01-02 15:12:50.785942 440342\n",
      "2019-01-02 15:12:55.458170 440335\n",
      "2019-01-02 15:12:56.167515 440332\n",
      "2019-01-02 15:12:56.429087 440332\n",
      "2019-01-02 15:12:56.561053 440332\n",
      "2019-01-02 15:12:57.106882 440328\n",
      "2019-01-02 15:12:57.233277 440328\n",
      "2019-01-02 15:12:57.339319 440328\n",
      "2019-01-02 15:13:00.681189 440323\n",
      "2019-01-02 15:13:00.915876 440323\n",
      "2019-01-02 15:13:00.993975 440322\n",
      "2019-01-02 15:13:01.180373 440322\n",
      "2019-01-02 15:13:01.861801 440319\n",
      "2019-01-02 15:13:02.954113 440318\n",
      "2019-01-02 15:13:04.060948 440317\n",
      "2019-01-02 15:13:05.024388 440317\n",
      "2019-01-02 15:13:05.222379 440317\n",
      "2019-01-02 15:13:05.348148 440317\n",
      "2019-01-02 15:13:05.444544 440317\n",
      "2019-01-02 15:13:06.268858 440317\n",
      "2019-01-02 15:13:06.379067 440317\n",
      "2019-01-02 15:13:06.793808 440317\n",
      "2019-01-02 15:13:07.069852 440317\n",
      "2019-01-02 15:13:07.651015 440317\n",
      "2019-01-02 15:13:09.368604 440317\n",
      "2019-01-02 15:13:09.826972 440317\n",
      "2019-01-02 15:13:10.416207 440317\n",
      "2019-01-02 15:13:10.972208 440317\n",
      "2019-01-02 15:13:11.366865 440317\n",
      "2019-01-02 15:13:11.859752 440317\n",
      "2019-01-02 15:13:12.679499 440317\n",
      "2019-01-02 15:13:13.105937 440317\n",
      "2019-01-02 15:13:13.696078 440317\n",
      "2019-01-02 15:13:13.924753 440317\n",
      "2019-01-02 15:13:14.120965 440317\n",
      "2019-01-02 15:13:14.375637 440317\n",
      "2019-01-02 15:13:15.320613 440317\n",
      "2019-01-02 15:13:15.855400 440317\n"
     ]
    }
   ],
   "source": [
    "# Remove previous week ids\n",
    "\n",
    "all_files_from_SP=list(recursive_file_gen(\"/home/jian/celery/Append_to_celery_LiveRamp/Previous_Ids_From_Sp/\"))\n",
    "all_files_from_SP=[x for x in all_files_from_SP if \"Copy of 48 Stores_727.csv\" not in x]\n",
    "for Sp_file in all_files_from_SP:\n",
    "    df = pd.read_csv(Sp_file,usecols = ['email_address_hash'])\n",
    "    previous_email_set=set(df['email_address_hash'].tolist())\n",
    "    df_all_new_master = df_all_new_master[~df_all_new_master['email_address_hash'].isin(previous_email_set)]\n",
    "    print(datetime.datetime.now(),len(df_all_new_master.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440317\n"
     ]
    }
   ],
   "source": [
    "df_all_new_master = df_all_new_master.drop_duplicates('email_address_hash')\n",
    "df_all_new_master = df_all_new_master.drop_duplicates('customer_id_hashed')\n",
    "print(len(df_all_new_master.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'P' nan 'T']\n",
      "['S' 'P' 'T']\n",
      "P 308687\n",
      "S 84154\n",
      "T 47476\n"
     ]
    }
   ],
   "source": [
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "df_all_new_master = pd.merge(df_all_new_master,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(df_all_new_master['zipcodegroup'].unique())\n",
    "df_all_new_master['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(df_all_new_master['zipcodegroup'].unique())\n",
    "\n",
    "last_saturday_str=\"_\"+str(last_saturday).split(\"-\")[1]+str(last_saturday).split(\"-\")[2]+str(last_saturday).split(\"-\")[0][2:]+\"_\"\n",
    "\n",
    "for revenue_flag in ['P','S','T']:\n",
    "    df = df_all_new_master[df_all_new_master['zipcodegroup']==revenue_flag]\n",
    "    df = df[['customer_id_hashed','email_address_hash','customer_zip_code']]\n",
    "    df['segment'] = 'NewReward' + last_saturday_str + revenue_flag\n",
    "    print(revenue_flag,len(df.index))\n",
    "    df.to_csv(write_folder+'NewReward_'+str(last_saturday)+'_'+revenue_flag+'.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
