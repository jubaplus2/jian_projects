{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import glob\n",
    "import logging\n",
    "import gc\n",
    "folderpath = '/home/jian/BiglotsCode/outputs/'\n",
    "# lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=10))\n",
    "lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=12))\n",
    "# Tuesday_today_str=str(datetime.datetime.now().date())[0:4]+str(datetime.datetime.now().date())[5:7]+str(datetime.datetime.now().date())[8:10]\n",
    "Tuesday_today_str ='20180717'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bb7a0c288036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewsalespath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/client/BigLotsData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"MediaStormSalesWeekly\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTuesday_today_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnewtrafficpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/client/BigLotsData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"MediaStormTrafficWeekly\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTuesday_today_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnewinventorypath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/client/BigLotsData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"MediaStormInventoryWeekly\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTuesday_today_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "newsalespath=glob.glob(\"/home/client/BigLotsData/\"+\"MediaStormSalesWeekly\"+Tuesday_today_str+\"*.txt\")[0]\n",
    "newtrafficpath=glob.glob(\"/home/client/BigLotsData/\"+\"MediaStormTrafficWeekly\"+Tuesday_today_str+\"*.txt\")[0]\n",
    "newinventorypath=glob.glob(\"/home/client/BigLotsData/\"+\"MediaStormInventoryWeekly\"+Tuesday_today_str+\"*.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "closed_onlinestorelist = ['6990','145']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='BL_weekly_run_for_week_end_date'+str(datetime.datetime.now().date()-datetime.timedelta(days=3))+'_files.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tradearea_all = pd.read_csv('/home/jian/BiglotsCode/OtherInput/New_TA_info.csv',dtype = 'str')\n",
    "df_tradearea_all['trade_area_code']=df_tradearea_all['Ta_Info'].apply(lambda x: x.split(\" | \")[0])\n",
    "df_tradearea_all=df_tradearea_all[['location_id','trade_area_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sales data column header matches:\n",
      "[ True  True  True  True  True  True  True  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsales = pd.read_csv(folderpath + 'combinedsales'+ lastweeksdate + '.csv',sep = '|',dtype = 'str')\n",
    "df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new sales data column header matches:\")\n",
    "logging.info(\"new sales data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "       'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "       'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "'''print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "       'gross_transaction_cnt', 'class_code_id', 'class_gross_sales_amt'])'''\n",
    "df['subclass_gross_sales_amt']=df['subclass_gross_sales_amt'].astype(float)\n",
    "df=df.groupby(['location_id','week_end_dt','fiscal_week_nbr','gross_sales_amt','gross_transaction_cnt','class_code_id'])['subclass_gross_sales_amt'].sum().to_frame().reset_index()\n",
    "df=df.rename(columns={\"subclass_gross_sales_amt\":\"class_gross_sales_amt\"})\n",
    "\n",
    "\n",
    "\n",
    "dfsales = dfsales.append(df,ignore_index = True)\n",
    "a = (len(dfsales.index))\n",
    "dfsales = dfsales.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "       'class_code_id'])\n",
    "b = (len(dfsales.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "    logging.info(\"\")\n",
    "else:\n",
    "    print(\"last week traffic data duplication deduped\")\n",
    "    logging.info(\"last week traffic data duplication deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-07-14'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recentweek = (max(dfsales['week_end_dt']))\n",
    "recentweek\n",
    "logging.info(\"recentweek: \"+str(recentweek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales.to_csv(folderpath + 'combinedsales'+ recentweek + '.csv',index = False,sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales = dfsales[~dfsales['location_id'].isin(closed_onlinestorelist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputpath = folderpath +'Output_' + recentweek +'/'\n",
    "try:\n",
    "    os.stat(outputpath)\n",
    "except:\n",
    "    os.mkdir(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with ? sales/transaction: 0\n"
     ]
    }
   ],
   "source": [
    "dfnodata = dfsales[(dfsales['class_gross_sales_amt'] == '?')&\\\n",
    "                   (dfsales['week_end_dt'] == recentweek)]\n",
    "# dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "print(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))\n",
    "logging.info(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales['week_end_dt'] = pd.to_datetime(dfsales['week_end_dt'])\n",
    "dfsales = dfsales[dfsales['class_gross_sales_amt']!='?']\n",
    "dfsales = dfsales.reset_index(drop = True)\n",
    "\n",
    "dfsales['gross_sales_amt'] = dfsales['gross_sales_amt'].astype('float')\n",
    "dfsales['gross_transaction_cnt'] = dfsales['gross_transaction_cnt'].astype('float')\n",
    "dfsales['class_gross_sales_amt'] = dfsales['class_gross_sales_amt'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweeklist = dfsales[['week_end_dt','fiscal_week_nbr']].drop_duplicates()\n",
    "dfweeklist = dfweeklist.sort_values('week_end_dt',ascending = False)\n",
    "dfweeklist.reset_index(drop = True,inplace = True)\n",
    "dfweeklist.reset_index(inplace = True)\n",
    "\n",
    "dfweeklist_wow = dfweeklist.copy()\n",
    "dfweeklist_wow['index'] = dfweeklist_wow['index'] - 1\n",
    "dfweeklist_wow = dfweeklist_wow[['index','week_end_dt']]\n",
    "dfweeklist_wow.columns = ['index','weeklastweek']\n",
    "\n",
    "dfweeklist = dfweeklist[dfweeklist['index']<104]\n",
    "dfweeklist.reset_index(drop = True,inplace = True)\n",
    "dfweeklist['year'] = np.ceil((dfweeklist['index'] + 1)/52)\n",
    "\n",
    "dfweeklist1 = dfweeklist[dfweeklist['year'] == 1]\n",
    "dfweeklist1 = dfweeklist1[['index', 'week_end_dt', 'fiscal_week_nbr']]\n",
    "dfweeklist2 = dfweeklist[dfweeklist['year'] == 2]\n",
    "dfweeklist2 = dfweeklist2[['week_end_dt', 'fiscal_week_nbr']]\n",
    "dfweeklist2.columns = ['weeklastyear', 'fiscal_week_nbr']\n",
    "dfweeklist1['rank']=dfweeklist1['week_end_dt'].rank(ascending=True)\n",
    "dfweeklist2['rank']=dfweeklist2['weeklastyear'].rank(ascending=True)\n",
    "\n",
    "del dfweeklist2['fiscal_week_nbr']\n",
    "\n",
    "dfweeklist = pd.merge(dfweeklist1,dfweeklist2,on ='rank' )\n",
    "dfweeklist = pd.merge(dfweeklist,dfweeklist_wow,on ='index')\n",
    "del dfweeklist1,dfweeklist2,dfweeklist_wow\n",
    "\n",
    "dfweeklist.to_csv(outputpath + 'weeklist.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek_date = (max(dfsales['week_end_dt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfcheck = dfsales[dfsales['week_end_dt'] == recentweek_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stores with zero sales/transaction: 0\n",
      "stores gross sales can not match sum of class sales: 2\n",
      "stores with zero class sales: 11\n"
     ]
    }
   ],
   "source": [
    "dfcheck_total1 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "a = (len(dfcheck_total1.index))\n",
    "dfcheck_total1 = dfcheck_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "b = (len(dfcheck_total1.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "    logging.info(\"\")\n",
    "else:\n",
    "    print(\"last week sales multiple gross sales/trasaction in the same store\")\n",
    "    logging.info(\"last week sales multiple gross sales/trasaction in the same store\")\n",
    "\n",
    "dfcheck_total2 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfcheck_total2.reset_index(inplace = True)\n",
    "\n",
    "dfcheck_total = pd.merge(dfcheck_total1,dfcheck_total2,\n",
    "                        on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                        how = 'outer')\n",
    "\n",
    "del dfcheck_total1,dfcheck_total2\n",
    "\n",
    "dfcheck_zero = dfcheck_total[(dfcheck_total['class_gross_sales_amt']<=0)|\\\n",
    "                            (dfcheck_total['gross_transaction_cnt']<=0) ]\n",
    "\n",
    "dfcheck_zero.to_csv(outputpath + 'zerosales.csv',index = False)\n",
    "print(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "logging.info(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "del dfcheck_zero\n",
    "\n",
    "dfcheck_total['TotalDiff'] = dfcheck_total['gross_sales_amt']-dfcheck_total['class_gross_sales_amt']\n",
    "dfcheck_total['TotalDiff'] = dfcheck_total['TotalDiff'].round()\n",
    "dfcheck_totalnonmatch = dfcheck_total[dfcheck_total['TotalDiff']!=0]\n",
    "print(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "logging.info(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "dfcheck_totalnonmatch.to_csv(outputpath + 'totalnonmatch.csv',index = False)\n",
    "del dfcheck_totalnonmatch\n",
    "\n",
    "dfcheck_zeroclass = dfcheck[(dfcheck['class_gross_sales_amt']==0)]\n",
    "dfcheck_zeroclass.to_csv(outputpath + 'zeroclasssales.csv',index = False)\n",
    "print(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "logging.info(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "del dfcheck_zeroclass\n",
    "del dfcheck\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total1 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "dfsales_total1 = dfsales_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "dfsales_total2 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfsales_total2.reset_index(inplace = True)\n",
    "\n",
    "dfsales_total = pd.merge(dfsales_total1,dfsales_total2,\n",
    "                        on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                        how = 'outer')\n",
    "del dfsales_total1,dfsales_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfstore_Nov15_2017 = pd.read_table('/home/jian/Big_Lots/OtherInput/MediaStormStoreList_Nov15.txt',\n",
    "                        sep = '|',dtype = 'str')\n",
    "dfstore_Jul07_2018 = pd.read_table('/home/jian/Big_Lots/OtherInput/MediaStormStores_20180703.txt',\n",
    "                        sep = '|',dtype = 'str')\n",
    "dfstore_Jul07_2018=dfstore_Jul07_2018[dfstore_Nov15_2017.columns.tolist()]\n",
    "dfstore_Nov15_2017=dfstore_Nov15_2017[~dfstore_Nov15_2017['location_id'].isin(dfstore_Jul07_2018['location_id'])]\n",
    "dfstore=dfstore_Jul07_2018.append(dfstore_Nov15_2017)\n",
    "\n",
    "dfstore['open_dt'] = pd.to_datetime(dfstore['open_dt'])\n",
    "dfstore['open_dtwd'] = dfstore['open_dt'].dt.dayofweek\n",
    "dfstore['open_wk'] = np.where(dfstore['open_dtwd']<=5,\n",
    "                       dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(5-x.weekday()))),\n",
    "                       dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(12-x.weekday()))))\n",
    "\n",
    "dma = pd.read_csv('/home/jian/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')\n",
    "dfstore_exc = dfstore\n",
    "dfstore_exc['zip_cd'] = dfstore_exc['zip_cd'].str[0:5]\n",
    "dfstore_exc = pd.merge(dfstore_exc,dma,on = 'zip_cd',how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores w/o detailed info: \n",
      "['61' '290' '455' '1084' '1230' '1422' '1550' '1750' '4479' '5098' '5177'\n",
      " '824' '5133' '4099' '4113' '4165' '4280' '4362' '1913' '1967' '1148'\n",
      " '1182' '280' '388' '507' '5363' '4675' '5364' '4677']\n"
     ]
    }
   ],
   "source": [
    "dfstorematch = dfsales_total[['location_id']].drop_duplicates()\n",
    "dfstorematch = pd.merge(dfstorematch,dfstore[['location_id','address_line_1']],\n",
    "                        on = 'location_id',how = 'left')\n",
    "dfstorematch['address_line_1'].fillna('empty',inplace = True)\n",
    "dfstorematch = dfstorematch[dfstorematch['address_line_1']=='empty']\n",
    "print(\"stores w/o detailed info: \")\n",
    "print(dfstorematch['location_id'].unique())\n",
    "logging.info(\"stores w/o detailed info: \")\n",
    "logging.info(dfstorematch['location_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfstorematch['location_id'].unique())\n",
    "logging.info(\"len of stores w/o detailed info: \"+str(len(dfstorematch['location_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_week_closed_stores=['61','290','455','1084','1230','1422','1550','1750','4479','5098','5177','824','5133','4099',\n",
    "                         '4113','4165','4280','4362','1913','1967','1148','1182','280','388','507','5363','4675','5364','4677']\n",
    "# 5363 2018-04-28\n",
    "# 4675 2018-06-02 not updated about TA info\n",
    "# 5364 2018-06-19 not updated about TA info\n",
    "# 4677 2018-06-26 not updated about TA info\n",
    "sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores)\n",
    "logging.info(\"check the closed stores list same as last week:\")\n",
    "logging.info(sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores]\n",
    "logging.info(\"new stores: \")\n",
    "logging.info([x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dfstorematch\n",
    "dfsales_total = pd.merge(dfsales_total,dfstore[['location_id','open_wk']],\n",
    "                        on = 'location_id',how = 'left')\n",
    "dfsales_total['open_wk'].fillna(datetime.datetime.strptime(str(20200101), '%Y%m%d').date(),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new traffic data column header matches:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftraffic = pd.read_csv(folderpath + 'combinedtraffic'+ lastweeksdate + '.csv',\n",
    "               sep = '|',dtype = 'str')\n",
    "\n",
    "df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new traffic data column header matches:\")\n",
    "logging.info(\"new traffic data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "       'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "       'traffic_day_6', 'traffic_day_7'])\n",
    "logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "       'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "       'traffic_day_6', 'traffic_day_7'])\n",
    "dftraffic = dftraffic.append(df,ignore_index = True)\n",
    "a = (len(dftraffic.index))\n",
    "dftraffic = dftraffic.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "b = (len(dftraffic.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "    logging.info(\"\")\n",
    "else:\n",
    "    print(\"last week traffic data duplication deduped\")\n",
    "    logging.info(\"last week traffic data duplication deduped\")\n",
    "dftraffic.to_csv(folderpath + 'combinedtraffic'+ recentweek + '.csv',index = False,sep = '|')\n",
    "\n",
    "dftraffic['traffic_week'] = 0 \n",
    "for i in ['traffic_day_1','traffic_day_2', 'traffic_day_3', 'traffic_day_4',\n",
    "          'traffic_day_5', 'traffic_day_6', 'traffic_day_7']:\n",
    "    dftraffic[i] = dftraffic[i].astype('float')\n",
    "    dftraffic['traffic_week'] = dftraffic['traffic_week'] +dftraffic[i]\n",
    "dftraffic['week_end_dt'] = pd.to_datetime(dftraffic['week_end_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new inventory data column header matches:\n",
      "[ True  True  True  True False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfinventory = pd.read_csv(folderpath + 'combinedinventory'+ lastweeksdate + '.csv',\n",
    "               sep = '|',dtype = 'str')\n",
    "\n",
    "df = pd.read_csv(newinventorypath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new inventory data column header matches:\")\n",
    "logging.info(\"new inventory data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "\n",
    "df.columns = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand']\n",
    "dfinventory = dfinventory.append(df,ignore_index = True)\n",
    "a = (len(dfinventory.index))\n",
    "dfinventory = dfinventory.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id'])\n",
    "b = (len(dfinventory.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "    logging.info(\"\")\n",
    "else:\n",
    "    print(\"last week inventory data duplication deduped\")\n",
    "    logging.info(\"last week inventory data duplication deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory.to_csv(folderpath + 'combinedinventory'+ recentweek + '.csv',index = False,sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory['week_end_dt'] = pd.to_datetime(dfinventory['week_end_dt'])\n",
    "dfinventory['on_hand'] = dfinventory['on_hand'].astype('float')\n",
    "dfinventory_total = dfinventory.groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfinventory_total.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total,\n",
    "                                dfweeklist[['week_end_dt', 'fiscal_week_nbr','weeklastyear','weeklastweek']],\n",
    "                                on= ['week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "dfsales_total_lastyear = pd.merge(dfsales_total,\n",
    "                                 dfweeklist[['weeklastyear']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "dfsales_total_lastyear = dfsales_total_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "dfsales_total_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastyear,\n",
    "                                on = ['location_id','weeklastyear'],how = 'outer')\n",
    "\n",
    "dfsales_total_recent_null_open_wk=dfsales_total_recent[pd.isnull(dfsales_total_recent['open_wk'])]\n",
    "dfsales_total_recent_valid_open_wk=dfsales_total_recent[~pd.isnull(dfsales_total_recent['open_wk'])]\n",
    "\n",
    "del dfsales_total_recent_null_open_wk['open_wk']\n",
    "open_wk_temp=dfstore[['location_id','open_wk']]\n",
    "dfsales_total_recent_null_open_wk=pd.merge(dfsales_total_recent_null_open_wk,open_wk_temp,on=\"location_id\",how=\"left\")\n",
    "dfsales_total_recent_null_open_wk=dfsales_total_recent_null_open_wk[dfsales_total_recent_valid_open_wk.columns.tolist()]\n",
    "dfsales_total_recent_null_open_wk['open_wk'].fillna(datetime.datetime.strptime(str(20200101), '%Y%m%d').date(),inplace = True)\n",
    "dfsales_total_recent_null_open_wk['week_end_dt']=dfsales_total_recent_null_open_wk['weeklastyear'].apply(lambda x: x+datetime.timedelta(days=364))\n",
    "dfsales_total_recent=dfsales_total_recent_valid_open_wk.append(dfsales_total_recent_null_open_wk)\n",
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "dfsales_total_recent['Store_Category'] = np.where(dfsales_total_recent['open_wk']>=dfsales_total_recent['weeklastyear'],'New',\n",
    "                                                  np.where((dfsales_total_recent['gross_transaction_cnt_ly']==0)&(dfsales_total_recent['class_gross_sales_amt_ly']==0),'Converted',\n",
    "                                                           np.where((dfsales_total_recent['gross_transaction_cnt']==0)&(dfsales_total_recent['class_gross_sales_amt']==0),'Converted',\n",
    "                                                                    'Complete')\n",
    "                                                          )\n",
    "                                                 )\n",
    "\n",
    "dfsales_total_lastweek = pd.merge(dfsales_total,\n",
    "                                 dfweeklist[['weeklastweek']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastweek')\n",
    "\n",
    "dfsales_total_lastweek = dfsales_total_lastweek[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastweek']]\n",
    "dfsales_total_lastweek.columns = ['location_id','gross_transaction_cnt_lw', 'class_gross_sales_amt_lw','weeklastweek']\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastweek,\n",
    "                                on = ['location_id','weeklastweek'],how = 'left')\n",
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "dfsales_total_recent.to_csv(folderpath + 'check debug 0719.csv',index=False)\n",
    "\n",
    "\n",
    "dfsales_total_recent['week_end_dt'] = np.where(dfsales_total_recent['week_end_dt']=='1970-01-01',dfsales_total_recent['weeklastyear'] + pd.DateOffset(364),\n",
    "                                               dfsales_total_recent['week_end_dt'])\n",
    "dfsales_total_recent['weeklastyear'] = np.where(dfsales_total_recent['weeklastyear']=='1970-01-01',dfsales_total_recent['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                                dfsales_total_recent['weeklastyear'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with no 2017&2016 sales and transaction data: 186\n",
      "Last Week: stores with no 2017&2016 sales and transaction data: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dfallstorelist = dfstore[~dfstore['location_id'].isin(closed_onlinestorelist)]\n",
    "dfallstorelist.reset_index(drop = True, inplace = True)\n",
    "dfweeklist2 = dfweeklist.copy()\n",
    "dfallstorelist['concat'] = 1\n",
    "dfweeklist2['concat'] = 1\n",
    "dfallstorelist = pd.merge(dfallstorelist,dfweeklist2,on='concat')\n",
    "dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "dfallstorelist = pd.merge(dfallstorelist,dfsales_total_recent,on=['location_id','week_end_dt'],\n",
    "                         how = 'left')\n",
    "\n",
    "dfallstorelist.fillna(0,inplace = True)\n",
    "dfallstorelist = dfallstorelist[(dfallstorelist['gross_sales_amt']==0)&\\\n",
    "                               (dfallstorelist['gross_transaction_cnt']==0)&\\\n",
    "                               (dfallstorelist['class_gross_sales_amt']==0)&\\\n",
    "                               (dfallstorelist['gross_transaction_cnt_ly']==0)&\\\n",
    "                               (dfallstorelist['class_gross_sales_amt_ly']==0)]\n",
    "dfallstorelist = dfallstorelist.sort_values(['week_end_dt','location_id'],ascending = [0,1])\n",
    "dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "\n",
    "x_temp=dfsales_total_recent[['week_end_dt','fiscal_week_nbr']]\n",
    "y_temp=x_temp.drop_duplicates()\n",
    "y_temp=y_temp[y_temp['fiscal_week_nbr']!=\"0\"]\n",
    "y_temp=y_temp[y_temp['fiscal_week_nbr']!=0]\n",
    "dfallstorelist = pd.merge(dfallstorelist,y_temp,on=\"week_end_dt\",how=\"left\")\n",
    "dfallstorelist = pd.merge(dfallstorelist,dfstore_exc,on=\"location_id\",how='left')\n",
    "dfallstorelist = pd.merge(dfallstorelist,df_tradearea_all,on=\"location_id\",how='left')\n",
    "del dfallstorelist['open_dtwd']\n",
    "del dfallstorelist['open_wk']\n",
    "\n",
    "# dfallstorelist.to_csv(outputpath + 'nobothyeardatastores.csv',index = False)\n",
    "print(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "logging.info(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "test = dfallstorelist[dfallstorelist['week_end_dt']==recentweek_date]\n",
    "print(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "logging.info(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "# del test,dfweeklist2,dfallstorelist\n",
    "\n",
    "\n",
    "# For later use to add index\n",
    "\n",
    "Recent_52_Week_nbr=dfweeklist[['week_end_dt','index']]\n",
    "Recent_52_Week_nbr['52_Weeks_nbr']=52-Recent_52_Week_nbr['index']\n",
    "del Recent_52_Week_nbr['index']\n",
    "\n",
    "# Recent_52_Week_nbr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic,\n",
    "                                on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "dftraffic2 = dftraffic[['location_id', 'week_end_dt','traffic_day_1',\n",
    "       'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "       'traffic_day_6', 'traffic_day_7','traffic_week']]\n",
    "dftraffic2.columns = ['location_id', 'weeklastyear','traffic_day_1_ly',\n",
    "       'traffic_day_2_ly', 'traffic_day_3_ly', 'traffic_day_4_ly', 'traffic_day_5_ly',\n",
    "       'traffic_day_6_ly', 'traffic_day_7_ly','traffic_week_ly']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic2,\n",
    "                                on=['location_id', 'weeklastyear'],how = 'left')\n",
    "del dftraffic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total,\n",
    "                                on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "dfinventory_total2 = dfinventory_total[['location_id', 'week_end_dt','on_hand']]\n",
    "dfinventory_total2.columns = ['location_id', 'weeklastyear','on_hand_ly']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total2,\n",
    "                                on=['location_id', 'weeklastyear'],how = 'left')\n",
    "del dfinventory_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek_last=datetime.datetime.strptime(recentweek, '%Y-%m-%d').date()\n",
    "recentweek_last=recentweek_last+datetime.timedelta(days=(-84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent['yoysales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_ly'] - 1\n",
    "dfsales_total_recent['yoytrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_ly'] - 1\n",
    "dfsales_total_recent['wowsales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_lw'] - 1\n",
    "dfsales_total_recent['wowtrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_lw'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with high yoy change: 1604\n",
      "Last Week: stores with high yoy change: 7\n"
     ]
    }
   ],
   "source": [
    "dfsales_total_recent_delete = dfsales_total_recent[(dfsales_total_recent['Store_Category']=='Complete')&\\\n",
    "                                                   ((abs(dfsales_total_recent['yoysales'])>0.2)&\\\n",
    "                                                   (abs(dfsales_total_recent['yoytrans'])>0.2))]#|\\\n",
    "                                                   #(abs(dfsales_total_recent['wowsales'])>0.2)|\\\n",
    "                                                   #(abs(dfsales_total_recent['wowtrans'])>0.2))]\n",
    "dfsales_total_recent_delete = dfsales_total_recent_delete.sort_values(['week_end_dt','location_id'],\n",
    "                                                                     ascending = [0,1])\n",
    "\n",
    "dfsales_total_recent_delete=pd.merge(dfsales_total_recent_delete,dfstore_exc,on=\"location_id\",how='left')\n",
    "dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,df_tradearea_all,on='location_id',how='left')\n",
    "\n",
    "del dfsales_total_recent_delete['open_dtwd']\n",
    "del dfsales_total_recent_delete['open_wk_y']\n",
    "dfsales_total_recent_delete.rename(index=str, columns={\"open_wk_x\": \"open_wk\"})\n",
    "\n",
    "dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "# dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores.csv',index = False)\n",
    "print(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "logging.info(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "test = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "print(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "logging.info(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfsales_total_recent =  dfsales_total_recent[(dfsales_total_recent['gross_transaction_cnt']!=0)|\\\n",
    "#                                             (dfsales_total_recent['class_gross_sales_amt']!=0)]\n",
    "dfsales_total_recent = dfsales_total_recent[(dfsales_total_recent['Store_Category']!='Complete')|\\\n",
    "                                            ((dfsales_total_recent['Store_Category']=='Complete')&\\\n",
    "                                            (abs(dfsales_total_recent['yoysales'])<=0.2)|\\\n",
    "                                            (abs(dfsales_total_recent['yoytrans'])<=0.2))]#&\\\n",
    "                                            #(abs(dfsales_total_recent['wowsales'])<=0.2)&\\\n",
    "                                            #(abs(dfsales_total_recent['wowtrans'])<=0.2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dfweeklist2 = dfweeklist[['week_end_dt']]\n",
    "dfweeklist2['week_end_dt_8w'] = dfweeklist2['week_end_dt']+pd.DateOffset(-84)\n",
    "# Name \"week_end_dt_8w\" reflects 12 weeks, not 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweeklist_12plus = dfsales[['week_end_dt']].drop_duplicates()\n",
    "dfweeklist_12plus = dfweeklist_12plus.sort_values('week_end_dt',ascending = False)\n",
    "dfweeklist_12plus.reset_index(drop = True,inplace = True)\n",
    "dfweeklist_12plus.reset_index(inplace = True)\n",
    "dfweeklist_12plus = dfweeklist_12plus[dfweeklist_12plus['index']<64]\n",
    "dfweeklist_12plus['weeklastyear'] = dfweeklist_12plus['week_end_dt'] + pd.DateOffset(-364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_12plus = pd.merge(dfsales_total,dfweeklist_12plus,on= ['week_end_dt'])\n",
    "\n",
    "dfsales_12plus_lastyear = pd.merge(dfsales_total,\n",
    "                                 dfweeklist_12plus[['weeklastyear']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "dfsales_12plus_lastyear = dfsales_12plus_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "dfsales_12plus_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "dfsales_12plus = pd.merge(dfsales_12plus,dfsales_12plus_lastyear,\n",
    "                                on = ['location_id','weeklastyear'],how = 'left')\n",
    "dfsales_12plus.fillna(0,inplace = True)\n",
    "\n",
    "dfsales_12plus['Store_Category'] = np.where(dfsales_12plus['open_wk']>=dfsales_12plus['weeklastyear'],'New',\n",
    "                                         np.where((dfsales_12plus['gross_transaction_cnt_ly']==0)&(dfsales_12plus['class_gross_sales_amt_ly']==0),\n",
    "                                         'Converted',\n",
    "                                         np.where((dfsales_12plus['gross_transaction_cnt']==0)&(dfsales_12plus['class_gross_sales_amt']==0),\n",
    "                                        'Converted','Complete')))\n",
    "dfsales_12plus['yoysales'] = dfsales_12plus['class_gross_sales_amt']/dfsales_12plus['class_gross_sales_amt_ly'] - 1\n",
    "dfsales_12plus['yoytrans'] = dfsales_12plus['gross_transaction_cnt']/dfsales_12plus['gross_transaction_cnt_ly'] - 1\n",
    "dfsales_12plus = dfsales_12plus[(dfsales_12plus['Store_Category']!='Complete')|\\\n",
    "                                            ((dfsales_12plus['Store_Category']=='Complete')&\\\n",
    "                                            (abs(dfsales_12plus['yoysales'])<=0.2)|\\\n",
    "                                            (abs(dfsales_12plus['yoytrans'])<=0.2))]#&\\\n",
    "dfsales_12plus = dfsales_12plus[['location_id','week_end_dt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finishing reading here\n",
    "dfsales_rankingall = pd.DataFrame()\n",
    "for i in range(52):\n",
    "    cweekdate = dfweeklist2['week_end_dt'][i]\n",
    "    recentweek_last = dfweeklist2['week_end_dt_8w'][i]\n",
    "    dfsales_ranking = dfsales_total[dfsales_total['week_end_dt']>recentweek_last]\n",
    "    dfsales_ranking = dfsales_ranking[dfsales_ranking['week_end_dt']<=cweekdate]\n",
    "    dfsales_ranking = pd.merge(dfsales_ranking,dfsales_12plus,\n",
    "                           on = ['location_id', 'week_end_dt'])\n",
    "    dfsales_ranking = pd.merge(dfsales_ranking,\n",
    "                               dftraffic[['location_id', 'week_end_dt', 'fiscal_week_nbr','traffic_week']],\n",
    "                               on =['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "    dfsales_ranking.fillna(0,inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking[['location_id','class_gross_sales_amt','traffic_week']].groupby('location_id').sum()\n",
    "    dfsales_ranking['Rev/Traffic'] = dfsales_ranking['class_gross_sales_amt']/dfsales_ranking['traffic_week']\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.sort_values('class_gross_sales_amt',ascending = False)\n",
    "    dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.rename(columns = {'index':'rev_index'})\n",
    "    dfsales_ranking = dfsales_ranking.replace(np.inf, 0)\n",
    "    \n",
    "    dfsales_ranking = dfsales_ranking.sort_values('Rev/Traffic',ascending = False)\n",
    "    dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.rename(columns = {'index':'traffi_index'})\n",
    "    \n",
    "    cols = len(dfsales_ranking.index)\n",
    "    dfsales_ranking['Store_Revenue_Rank'] = np.where(dfsales_ranking['rev_index']/cols <=0.2,'H',\n",
    "                                                    np.where(dfsales_ranking['rev_index']/cols <=0.8,'M','L'))\n",
    "    dfsales_ranking['Store_Revenue/Traffic_Rank'] = np.where(dfsales_ranking['traffi_index']/cols <=0.2,'H',\n",
    "                                                    np.where(dfsales_ranking['traffi_index']/cols <=0.8,'M','L'))\n",
    "    dfsales_ranking['week_end_dt'] = cweekdate\n",
    "    dfsales_rankingall = dfsales_rankingall.append(dfsales_ranking,ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "dfsales_total_recent.reset_index(drop = True,inplace = True)\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,\n",
    "                                dfsales_rankingall[['location_id','week_end_dt','Store_Revenue_Rank','Store_Revenue/Traffic_Rank']],\n",
    "                                on = ['location_id','week_end_dt'],how = 'left')\n",
    "dfsales_total_recent['Store_Revenue_Rank'].fillna('NA',inplace = True)\n",
    "dfsales_total_recent['Store_Revenue/Traffic_Rank'].fillna('NA',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent['AOV'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['gross_transaction_cnt']\n",
    "dfsales_total_recent['AOV_ly'] = dfsales_total_recent['class_gross_sales_amt_ly']/dfsales_total_recent['gross_transaction_cnt_ly']\n",
    "dfsales_total_recent['Trans/Traffic'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['traffic_week']\n",
    "dfsales_total_recent['Trans/Traffic_ly'] = dfsales_total_recent['gross_transaction_cnt_ly']/dfsales_total_recent['traffic_week_ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Complete', 'New', 'Converted'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsales_total_recent['Store_Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "df_complete = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Complete']\n",
    "df_complete.reset_index(drop = True, inplace = True)\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "columnheader = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    columnheader = columnheader + [i,a,b]\n",
    "    df_complete[b] = df_complete[i]/df_complete[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dfstore['open_dtwd']\n",
    "del dfstore['open_wk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dma = pd.read_csv('/home/jian/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')\n",
    "dfstore['zip_cd'] = dfstore['zip_cd'].str[0:5]\n",
    "dfstore = pd.merge(dfstore,dma,on = 'zip_cd',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete = df_complete[columnheader]\n",
    "df_complete = pd.merge(df_complete,dfstore,on='location_id',how = 'left')\n",
    "df_complete = df_complete.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = dfsales_total_recent[dfsales_total_recent['Store_Category']=='New']\n",
    "df_new = df_new[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                 'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                 'Trans/Traffic','on_hand','Store_Category']]\n",
    "df_new = pd.merge(df_new,dfstore_exc,on='location_id',how = 'left')\n",
    "df_new = df_new.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "df_new = pd.merge(df_new,df_tradearea_all,on='location_id',how='left')\n",
    "del df_new['open_wk']\n",
    "del df_new['open_dtwd']\n",
    "del df_new['Store_Category']\n",
    "df_new = pd.merge(df_new,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "\n",
    "# df_new.to_csv(outputpath + 'output2_new.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Complete', 'New', 'Converted'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsales_total_recent['Store_Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "dfnodata=pd.merge(dfnodata,dfstore_exc,on=\"location_id\",how='left')\n",
    "dfnodata = pd.merge(dfnodata,df_tradearea_all,on='location_id',how='left')\n",
    "dfnodata = pd.merge(dfnodata,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "\n",
    "# dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "\n",
    "\n",
    "\n",
    "df_converted = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Converted']\n",
    "del df_converted['fiscal_week_nbr']\n",
    "df_converted['week_end_dt'] = np.where(df_converted['week_end_dt']=='1970-01-01',\n",
    "                                       df_converted['weeklastyear'] + pd.DateOffset(364),\n",
    "                                       df_converted['week_end_dt'])\n",
    "df_converted['weeklastyear'] = np.where(df_converted['weeklastyear']=='1970-01-01',\n",
    "                                       df_converted['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                       df_converted['weeklastyear'])\n",
    "\n",
    "df_converted = pd.merge(df_converted,dfweeklist[['week_end_dt','fiscal_week_nbr']],\n",
    "                       on = 'week_end_dt',how='left')\n",
    "df_converted = df_converted[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                 'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                 'Trans/Traffic','on_hand','Store_Category',\n",
    "                 'weeklastyear','gross_transaction_cnt_ly','class_gross_sales_amt_ly']]\n",
    "df_converted = pd.merge(df_converted,dfstore_exc,on='location_id',how = 'left')\n",
    "df_converted = df_converted.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "\n",
    "df_converted = pd.merge(df_converted,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "\n",
    "\n",
    "del df_converted['Store_Category']\n",
    "df_converted = pd.merge(df_converted,df_tradearea_all,on='location_id',how='left')\n",
    "\n",
    "del df_converted['open_wk']\n",
    "del df_converted['open_dtwd']\n",
    "\n",
    "\n",
    "# df_converted.to_csv(outputpath + 'output3_converted.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TA_Information = pd.read_csv(\"/home/jian/BiglotsCode/OtherInput/New_TA_info.csv\")\n",
    "TA_Information['trade_area_code']=TA_Information['Ta_Info'].apply(lambda x: x.split(\" | \")[0])\n",
    "TA_Information=TA_Information[['trade_area_code','Ta_Info']]\n",
    "TA_Information=TA_Information.drop_duplicates()\n",
    "df_ta_info_na=pd.DataFrame(data = {'trade_area_code':['N0'], 'Ta_Info':['NA']})\n",
    "TA_Information=TA_Information.append(df_ta_info_na,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete = pd.merge(df_complete,df_tradearea_all,on ='location_id',how = 'left')\n",
    "df_complete['trade_area_code'].fillna('N0',inplace = True)\n",
    "\n",
    "df_complete = pd.merge(df_complete,Recent_52_Week_nbr,on='week_end_dt',how='left')\n",
    "\n",
    "\n",
    "df_complete = pd.merge(df_complete,TA_Information,on='trade_area_code',how='left')\n",
    "df_complete_new_list = df_complete.columns.tolist()\n",
    "df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "df_complete = df_complete[df_complete_new_list]\n",
    "\n",
    "df_complete['Store_Info'] = df_complete['location_id'].map(str)+\" | \"+df_complete['city_nm']+\\\n",
    "                        \" | \"+df_complete['state_nm']\n",
    "df_complete_new_list = df_complete.columns.tolist()\n",
    "df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "df_complete = df_complete[df_complete_new_list]\n",
    "\n",
    "\n",
    "# df_complete['revenue_trigger'] = np.where(abs(df_complete['class_gross_sales_amtYoYDiff'])>0.1,1,0)\n",
    "# df_complete['transaction_trigger'] = np.where(abs(df_complete['gross_transaction_cntYoYDiff'])>0.1,1,0)\n",
    "# df_complete['conversion_trigger'] = np.where(abs(df_complete['Trans/TrafficYoYDiff'])>0.1,1,0)\n",
    "\n",
    "# df_complete.to_csv(outputpath + 'output1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rewrite for adding Store Key and Trade Area Key\n",
    "\n",
    "Write_df_list = [df_converted,dfnodata,df_new,dfsales_total_recent_delete,dfallstorelist]\n",
    "for df in Write_df_list:\n",
    "\n",
    "    df['trade_area_code'].fillna(\"N0\",inplace = True)\n",
    "    df['Store_Numeric']=df['location_id'].astype('str')\n",
    "    df['TA_Numeric']=df['trade_area_code'].apply(lambda x: x[1:len(x)])\n",
    "                                                 \n",
    "    df['Store_Numeric']= df['Store_Numeric'].apply(lambda x: x.zfill(4))\n",
    "    df['Store_Key_1']=df['Store_Numeric'].str[0:2]\n",
    "    df['Store_Key_2']=df['Store_Numeric'].str[2:4]\n",
    "    df['Store_Key_1']=df['Store_Key_1'].astype('int')\n",
    "    df['Store_Key_2']=df['Store_Key_2'].astype('int')\n",
    "    \n",
    "    \n",
    "    df['TA_Numeric']= df['TA_Numeric'].apply(lambda x: x.zfill(4))\n",
    "    df['TA_Key_1']=df['TA_Numeric'].str[0:2]\n",
    "    df['TA_Key_2']=df['TA_Numeric'].str[2:4]\n",
    "    df['TA_Key_1']=df['TA_Key_1'].astype('int')\n",
    "    df['TA_Key_2']=df['TA_Key_2'].astype('int')\n",
    "\n",
    "    del df['TA_Numeric']\n",
    "    del df['Store_Numeric']\n",
    "\n",
    "    \n",
    "Write_df_list2 = [df_converted,dfnodata,df_new,dfsales_total_recent_delete,dfallstorelist]\n",
    "# TA_Information['trade_area_code']=TA_Information['trade_area_code'].astype('int')\n",
    "Write_df_list3 = []\n",
    "for df in Write_df_list2:\n",
    "    df['trade_area_code'].fillna(\"N0\",inplace = True)\n",
    "    # df['trade_area_code']=df['trade_area_code'].astype('int')\n",
    "    \n",
    "    df = pd.merge(df,TA_Information,on='trade_area_code',how='left')\n",
    "    \n",
    "    df_complete_new_list = df.columns.tolist()\n",
    "    df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "    df = df[df_complete_new_list]\n",
    "\n",
    "    df['Store_Info'] = df['location_id'].map(str)+\" | \"+df['city_nm'].map(str)+\\\n",
    "                        \" | \"+df['state_nm'].map(str)\n",
    "    df_complete_new_list = df.columns.tolist()\n",
    "    df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "    df = df[df_complete_new_list]    \n",
    "    Write_df_list3.append(df)\n",
    "    \n",
    "\n",
    "df_converted=Write_df_list3[0]\n",
    "dfnodata=Write_df_list3[1]\n",
    "df_new=Write_df_list3[2]\n",
    "dfsales_total_recent_delete=Write_df_list3[3]\n",
    "dfallstorelist=Write_df_list3[4]\n",
    "dfallstorelist=dfallstorelist[dfallstorelist['fiscal_week_nbr']!=0]\n",
    "\n",
    "df_complete.to_csv(outputpath + 'output1.csv',index = False)\n",
    "df_converted.to_csv(outputpath + 'output3_converted'+' '+recentweek+'.csv',index = False)\n",
    "dfnodata.to_csv(outputpath + 'sales_nodata'+' '+recentweek+'.csv',index = False)\n",
    "df_new.to_csv(outputpath + 'output2_new'+' '+recentweek+'.csv',index = False)\n",
    "dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores'+' '+recentweek+'.csv',index = False)\n",
    "dfallstorelist.to_csv(outputpath + 'nobothyeardatastores'+' '+recentweek+'.csv',index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Wide to long\\n\\ndf_output1_17=df_complete[[\\'location_id\\',\\'week_end_dt\\', \\'fiscal_week_nbr\\', \\'Store_Revenue_Rank\\',\\n                           \\'Store_Revenue/Traffic_Rank\\', \\'class_gross_sales_amt\\',\\n                           \\'gross_transaction_cnt\\',\\'traffic_week\\',\\'on_hand\\',\\'location_desc\\', \\'open_dt\\',\\n                           \\'address_line_1\\', \\'address_line_2\\', \\'city_nm\\', \\'state_nm\\', \\'zip_cd\\',\\n                           \\'longitude_meas\\', \\'latitude_meas\\', \\'DMA\\', \\'trade_area_code\\']]\\n# df_output1_17=pd.merge(df_output1_17,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\\n\\n\\n\\ndf_output1_16=df_complete[[\\'location_id\\',\\'week_end_dt\\', \\'fiscal_week_nbr\\', \\'Store_Revenue_Rank\\',\\n                           \\'Store_Revenue/Traffic_Rank\\', \\'class_gross_sales_amt_ly\\',\\n                           \\'gross_transaction_cnt_ly\\',\\'traffic_week_ly\\',\\'on_hand_ly\\',\\'location_desc\\', \\'open_dt\\',\\n                           \\'address_line_1\\', \\'address_line_2\\', \\'city_nm\\', \\'state_nm\\', \\'zip_cd\\',\\n                           \\'longitude_meas\\', \\'latitude_meas\\', \\'DMA\\', \\'trade_area_code\\']]\\n# df_output1_16=pd.merge(df_output1_16,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\\n\\ndf_output1_16[\\'week_end_dt\\']=df_output1_16[\\'week_end_dt\\']+pd.DateOffset(-364)\\n\\ndf_output1_16.columns=[\\'location_id\\',\\'week_end_dt\\', \\'fiscal_week_nbr\\', \\'class_gross_sales_amt\\',\\n                           \\'gross_transaction_cnt\\',\\'traffic_week\\',\\'on_hand\\',\\'location_desc\\', \\'open_dt\\',\\n                           \\'address_line_1\\', \\'address_line_2\\', \\'city_nm\\', \\'state_nm\\', \\'zip_cd\\',\\n                           \\'longitude_meas\\', \\'latitude_meas\\', \\'DMA\\', \\'trade_area_code\\']\\n\\ndf_output1_datorama=pd.concat([df_output1_17,df_output1_16])\\ndf_output1_datorama=df_output1_datorama.reindex_axis(df_output1_17.columns,axis=1)\\ndf_output1_datorama.to_csv(outputpath + \\'output1_datorama.csv\\',index = False)\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Wide to long\n",
    "\n",
    "df_output1_17=df_complete[['location_id','week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank', 'class_gross_sales_amt',\n",
    "                           'gross_transaction_cnt','traffic_week','on_hand','location_desc', 'open_dt',\n",
    "                           'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                           'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']]\n",
    "# df_output1_17=pd.merge(df_output1_17,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "df_output1_16=df_complete[['location_id','week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank', 'class_gross_sales_amt_ly',\n",
    "                           'gross_transaction_cnt_ly','traffic_week_ly','on_hand_ly','location_desc', 'open_dt',\n",
    "                           'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                           'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']]\n",
    "# df_output1_16=pd.merge(df_output1_16,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\n",
    "\n",
    "df_output1_16['week_end_dt']=df_output1_16['week_end_dt']+pd.DateOffset(-364)\n",
    "\n",
    "df_output1_16.columns=['location_id','week_end_dt', 'fiscal_week_nbr', 'class_gross_sales_amt',\n",
    "                           'gross_transaction_cnt','traffic_week','on_hand','location_desc', 'open_dt',\n",
    "                           'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                           'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']\n",
    "\n",
    "df_output1_datorama=pd.concat([df_output1_17,df_output1_16])\n",
    "df_output1_datorama=df_output1_datorama.reindex_axis(df_output1_17.columns,axis=1)\n",
    "df_output1_datorama.to_csv(outputpath + 'output1_datorama.csv',index = False)\n",
    "'''\n",
    "\n",
    "# del df_output1_16\n",
    "# del df_output1_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','trade_area_code']).sum()\n",
    "df_tadata.reset_index(inplace = True)\n",
    "\n",
    "df_tadata['AOV'] = df_tadata['class_gross_sales_amt']/df_tadata['gross_transaction_cnt']\n",
    "df_tadata['AOV_ly'] = df_tadata['class_gross_sales_amt_ly']/df_tadata['gross_transaction_cnt_ly']\n",
    "df_tadata['Trans/Traffic'] = df_tadata['gross_transaction_cnt']/df_tadata['traffic_week']\n",
    "df_tadata['Trans/Traffic_ly'] = df_tadata['gross_transaction_cnt_ly']/df_tadata['traffic_week_ly']\n",
    "\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    df_tadata[b] = df_tadata[i]/df_tadata[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_taclass1 = df_complete[['location_id','trade_area_code','week_end_dt','Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank']]\n",
    "df_taclass1.reset_index(drop = True, inplace = True)\n",
    "df_taclass1['Number_of_HMStores'] = np.where(df_taclass1['Store_Revenue_Rank']!='L',1,0)\n",
    "df_taclass1['Number_of_HMStores_RevTrafRank'] = np.where(df_taclass1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "df_taclass1['Number of Stores'] = 1\n",
    "df_taclass1 = df_taclass1.groupby(['trade_area_code','week_end_dt']).sum()\n",
    "df_taclass1.reset_index(inplace = True)\n",
    "\n",
    "df_taclass2 = df_complete[['trade_area_code','week_end_dt','zip_cd']].drop_duplicates()\n",
    "df_taclass2 = df_taclass2.groupby(['trade_area_code','week_end_dt']).count()\n",
    "df_taclass2.columns = ['NumberofZipcodes']\n",
    "df_taclass2.reset_index(inplace = True)\n",
    "\n",
    "df_taclass3 = df_complete[['trade_area_code','state_nm']].drop_duplicates(['trade_area_code'])\n",
    "\n",
    "df_tadata = pd.merge(df_tadata,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "df_tadata = pd.merge(df_tadata,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "df_tadata = pd.merge(df_tadata,df_taclass3,on =['trade_area_code'])\n",
    "\n",
    "df_tadata = df_tadata.sort_values(['trade_area_code','week_end_dt'],ascending = [1,0])\n",
    "df_tadata.to_csv(outputpath + 'output4.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dmadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','DMA']).sum()\n",
    "df_dmadata.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata['AOV'] = df_dmadata['class_gross_sales_amt']/df_dmadata['gross_transaction_cnt']\n",
    "df_dmadata['AOV_ly'] = df_dmadata['class_gross_sales_amt_ly']/df_dmadata['gross_transaction_cnt_ly']\n",
    "df_dmadata['Trans/Traffic'] = df_dmadata['gross_transaction_cnt']/df_dmadata['traffic_week']\n",
    "df_dmadata['Trans/Traffic_ly'] = df_dmadata['gross_transaction_cnt_ly']/df_dmadata['traffic_week_ly']\n",
    "\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    df_dmadata[b] = df_dmadata[i]/df_dmadata[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_dmadata1 = df_complete[['location_id','DMA','week_end_dt','Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank']]\n",
    "df_dmadata1.reset_index(drop = True, inplace = True)\n",
    "df_dmadata1['Number_of_HMStores'] = np.where(df_dmadata1['Store_Revenue_Rank']!='L',1,0)\n",
    "df_dmadata1['Number_of_HMStores_RevTrafRank'] = np.where(df_dmadata1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "df_dmadata1['Number of Stores'] = 1\n",
    "df_dmadata1 = df_dmadata1.groupby(['DMA','week_end_dt']).sum()\n",
    "df_dmadata1.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata2 = df_complete[['DMA','week_end_dt','zip_cd']].drop_duplicates()\n",
    "df_dmadata2 = df_dmadata2.groupby(['DMA','week_end_dt']).count()\n",
    "df_dmadata2.columns = ['NumberofZipcodes']\n",
    "df_dmadata2.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata4 = df_complete[['DMA','week_end_dt','trade_area_code']].drop_duplicates()\n",
    "df_dmadata4 = df_dmadata4.groupby(['DMA','week_end_dt']).count()\n",
    "df_dmadata4.columns = ['NumberofTAs']\n",
    "df_dmadata4.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata3 = df_complete[['DMA','state_nm']].drop_duplicates(['DMA'])\n",
    "\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata1,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata2,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata4,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata3,on =['DMA'])\n",
    "\n",
    "df_dmadata = df_dmadata.sort_values(['DMA','week_end_dt'],ascending = [1,0])\n",
    "df_dmadata.to_csv(outputpath + 'output5.csv',index = False)\n",
    "del df_dmadata1,df_dmadata2,df_dmadata3,df_dmadata4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_storeweeklist = pd.merge(df_complete[['location_id', 'week_end_dt']],\n",
    "                            dfweeklist[['week_end_dt','weeklastyear']],on ='week_end_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfsales_class1 = pd.merge(dfsales[['location_id', 'week_end_dt','class_code_id','class_gross_sales_amt']],\n",
    "                          df_storeweeklist,\n",
    "                          on = ['location_id', 'week_end_dt'])\n",
    "dfsales_class2 = pd.merge(dfsales[['location_id', 'week_end_dt', 'class_code_id','class_gross_sales_amt']],\n",
    "                          df_storeweeklist,\n",
    "                          left_on = ['location_id', 'week_end_dt'],\n",
    "                          right_on = ['location_id', 'weeklastyear'])\n",
    "\n",
    "del dfsales_class2['week_end_dt_x']\n",
    "dfsales_class2 = dfsales_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "\n",
    "dfsales_class1 = pd.merge(dfsales_class1,dfsales_class2,\n",
    "                          on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "dfsales_class1.fillna(0,inplace = True)\n",
    "del dfsales_class2\n",
    "del dfsales_class1['weeklastyear_x']\n",
    "del dfsales_class1['weeklastyear_y']\n",
    "dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_x':'class_gross_sales_amt'})\n",
    "dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_y':'class_gross_sales_amt_ly'})\n",
    "dfsales_class1['class_gross_sales_amt_yoy'] = dfsales_class1['class_gross_sales_amt']/dfsales_class1['class_gross_sales_amt_ly'] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory_class1 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']], \n",
    "                              df_storeweeklist,\n",
    "                          on = ['location_id', 'week_end_dt'])\n",
    "dfinventory_class2 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']],\n",
    "                          df_storeweeklist,\n",
    "                          left_on = ['location_id', 'week_end_dt'],\n",
    "                          right_on = ['location_id', 'weeklastyear'])\n",
    "\n",
    "del dfinventory_class2['week_end_dt_x']\n",
    "dfinventory_class2 = dfinventory_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "\n",
    "dfinventory_class1 = pd.merge(dfinventory_class1,dfinventory_class2,\n",
    "                          on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "dfinventory_class1.fillna(0,inplace = True)\n",
    "del dfinventory_class2\n",
    "del dfinventory_class1['weeklastyear_x']\n",
    "del dfinventory_class1['weeklastyear_y']\n",
    "dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_x':'on_hand'})\n",
    "dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_y':'on_hand_ly'})\n",
    "dfinventory_class1['on_hand_yoy'] = dfinventory_class1['on_hand']/dfinventory_class1['on_hand_ly'] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tadetail = pd.merge(dfsales_class1,dfinventory_class1,\n",
    "                      on=['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "df_tadetail = pd.merge(df_tadetail,df_tradearea_all,on ='location_id',how = 'left')\n",
    "df_tadetail['trade_area_code'].fillna('NA',inplace = True)\n",
    "df_tadetail.fillna(0,inplace = True)\n",
    "\n",
    "df_tadetail = df_tadetail.groupby(['trade_area_code','week_end_dt','class_code_id']).sum()\n",
    "df_tadetail['class_gross_sales_amt_yoy'] = df_tadetail['class_gross_sales_amt']/df_tadetail['class_gross_sales_amt_ly'] -1\n",
    "df_tadetail['on_hand_yoy'] = df_tadetail['on_hand']/df_tadetail['on_hand_ly'] -1\n",
    "df_tadetail.reset_index(inplace = True)\n",
    "\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass3,on =['trade_area_code'])\n",
    "df_tadetail = pd.merge(dfweeklist[['week_end_dt','fiscal_week_nbr']],df_tadetail,\n",
    "                      on ='week_end_dt')\n",
    "df_tadetail.to_csv(outputpath + 'output4_classdetail.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(outputpath+'BigLots_Weekly_Data_'+recentweek+'.xlsx',\n",
    "                            #engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_transaction_cnt']]\n",
    "test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "test.to_excel(writer,'Transactions', index=False)\n",
    "test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_gross_sales_amt']]\n",
    "test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "test.to_excel(writer,'Revenue', index=False)\n",
    "dftraffic = dftraffic[~dftraffic['location_id'].isin(closed_onlinestorelist)]\n",
    "dftraffic = dftraffic.sort_values(['location_id', 'week_end_dt'])\n",
    "dfinventory_total = dfinventory_total[~dfinventory_total['location_id'].isin(closed_onlinestorelist)]\n",
    "dfinventory_total = dfinventory_total.sort_values(['location_id', 'week_end_dt'])\n",
    "dftraffic.to_excel(writer,'Traffic', index=False)\n",
    "dfinventory_total.to_excel(writer,'Inventory', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_complete[df_complete['week_end_dt']==recentweek_date]\n",
    "df_complete_week.to_csv(outputpath + 'output1_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_highvariance_week = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "df_highvariance_week.to_csv(outputpath + 'highyoy_wowchangestores_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_tadetail_week = df_tadetail[df_tadetail['week_end_dt']==recentweek_date]\n",
    "df_tadetail_week.to_csv(outputpath + 'output4_classdetail_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_dmadata_week = df_dmadata[df_dmadata['week_end_dt']==recentweek_date]\n",
    "df_dmadata_week.to_csv(outputpath + 'output5_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_tadata_week = df_tadata[df_tadata['week_end_dt']==recentweek_date]\n",
    "df_tadata_week.to_csv(outputpath + 'output4_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_converted_week = df_converted[df_converted['week_end_dt']==recentweek_date]\n",
    "df_converted_week.to_csv(outputpath + 'output3_converted_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_new_week = df_new[df_new['week_end_dt']==recentweek_date]\n",
    "df_new_week.to_csv(outputpath + 'output2_new_' + recentweek + '.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_complete_week[['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'location_desc', 'open_dt',\n",
    "                   'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                   'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter(outputpath+'Output1Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_row(0,None, format2)\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format4)\n",
    "worksheet.set_column('W:W', None, format3)\n",
    "worksheet.set_column('Z:Z', None, format3)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('W:W', None, format4)\n",
    "worksheet.set_column('Z:Z', None, format3)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('W:W', None, format3)\n",
    "worksheet.set_column('Z:Z', None, format4)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_tadata_week[['trade_area_code', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'Number_of_HMStores', 'Number_of_HMStores_RevTrafRank',\n",
    "                   'Number of Stores', 'NumberofZipcodes', 'state_nm',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "\n",
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter(outputpath+'Output4Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format4)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format3)\n",
    "worksheet.set_column('Q:Q', None, format4)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format3)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format4)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_dmadata_week[['DMA', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'Number_of_HMStores',\n",
    "                     'Number_of_HMStores_RevTrafRank', 'Number of Stores',\n",
    "                     'NumberofZipcodes', 'NumberofTAs', 'state_nm',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "\n",
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(outputpath+'Output5Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format4)\n",
    "worksheet.set_column('R:R', None, format3)\n",
    "worksheet.set_column('U:U', None, format3)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format3)\n",
    "worksheet.set_column('R:R', None, format4)\n",
    "worksheet.set_column('U:U', None, format3)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format3)\n",
    "worksheet.set_column('R:R', None, format3)\n",
    "worksheet.set_column('U:U', None, format4)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write all historical data into long and wide format by store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_long_df=pd.read_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_\" +lastweeksdate+\".csv\")\n",
    "del sales_long_df['week_indicator']\n",
    "sales_long_df['week_end_date']=sales_long_df['week_end_date'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "start_record_date=min(sales_long_df['week_end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sales_df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "new_sales_df['subclass_gross_sales_amt']=new_sales_df['subclass_gross_sales_amt'].astype(float)\n",
    "new_sales_df=new_sales_df.groupby(['location_id','week_end_dt','fiscal_week_nbr','gross_sales_amt','gross_transaction_cnt','class_code_id'])['subclass_gross_sales_amt'].sum().to_frame().reset_index()\n",
    "new_sales_df=new_sales_df.rename(columns={\"subclass_gross_sales_amt\":\"class_gross_sales_amt\"})\n",
    "\n",
    "new_traffic_df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "new_sales_df=new_sales_df[~new_sales_df['location_id'].isin([\"145\",\"6990\"])]\n",
    "new_traffic_df=new_traffic_df[~new_traffic_df['location_id'].isin([\"145\",\"6990\"])]\n",
    "\n",
    "new_sales_df['location_id']=new_sales_df['location_id'].astype(int)\n",
    "new_sales_df['week_end_dt']=new_sales_df['week_end_dt'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "new_sales_df['class_gross_sales_amt']=new_sales_df['class_gross_sales_amt'].astype(float)\n",
    "new_sales_df['gross_transaction_cnt']=new_sales_df['gross_transaction_cnt'].astype(int)\n",
    "\n",
    "new_traffic_df['location_id']=new_traffic_df['location_id'].astype(int)\n",
    "new_traffic_df['week_end_dt']=new_traffic_df['week_end_dt'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "new_traffic_df['traffic_day_1']=new_traffic_df['traffic_day_1'].astype(int)\n",
    "new_traffic_df['traffic_day_2']=new_traffic_df['traffic_day_2'].astype(int)\n",
    "new_traffic_df['traffic_day_3']=new_traffic_df['traffic_day_3'].astype(int)\n",
    "new_traffic_df['traffic_day_4']=new_traffic_df['traffic_day_4'].astype(int)\n",
    "new_traffic_df['traffic_day_5']=new_traffic_df['traffic_day_5'].astype(int)\n",
    "new_traffic_df['traffic_day_6']=new_traffic_df['traffic_day_6'].astype(int)\n",
    "new_traffic_df['traffic_day_7']=new_traffic_df['traffic_day_7'].astype(int)\n",
    "new_traffic_df['traffic']=new_traffic_df[['traffic_day_1','traffic_day_2','traffic_day_3','traffic_day_4',\n",
    "                                         'traffic_day_5','traffic_day_6','traffic_day_7']].sum(axis=1)\n",
    "new_traffic_df=new_traffic_df[['location_id','week_end_dt','traffic']]\n",
    "new_traffic_df=new_traffic_df.drop_duplicates()\n",
    "new_traffic_df.reset_index(inplace=True)\n",
    "del new_traffic_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sales_df_app=new_sales_df[['location_id','week_end_dt','class_gross_sales_amt']].drop_duplicates()\n",
    "new_sales_df_app=new_sales_df.groupby(['location_id','week_end_dt'])['class_gross_sales_amt'].sum().to_frame()\n",
    "new_sales_df_app.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_transaction_df_app=new_sales_df[['location_id','week_end_dt','gross_transaction_cnt']]\n",
    "new_transaction_df_app=new_transaction_df_app.drop_duplicates().reset_index()\n",
    "del new_transaction_df_app['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek = str(max(new_sales_df_app['week_end_dt']).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "append_to_long=pd.merge(new_sales_df_app,new_transaction_df_app,on=['location_id','week_end_dt'],how='outer')\n",
    "append_to_long=pd.merge(append_to_long,new_traffic_df,on=['location_id','week_end_dt'],how='outer')\n",
    "append_to_long.columns=['location_id','week_end_date','sales','transactions','traffics']\n",
    "append_to_long=append_to_long.fillna(0)\n",
    "append_to_long['traffics']=append_to_long['traffics'].astype(int)\n",
    "sales_long_df=sales_long_df.append(append_to_long)\n",
    "sales_long_df['week_indicator']=sales_long_df['week_end_date'].apply(lambda x: int((x-start_record_date).days/7+1))\n",
    "sales_long_df=sales_long_df.sort_values(['location_id','week_end_date'])\n",
    "sales_long_df['week_indicator']=sales_long_df['week_indicator'].apply(lambda x: x% 52)\n",
    "sales_long_df.to_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_\" +recentweek+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"/home/jian/BiglotsCode/outputs/Output_\"+recentweek+\"/wide_sales_date\"+recentweek+\".xlsx\", engine='xlsxwriter')\n",
    "sales_long_df['week_end_date']=sales_long_df['week_end_date'].apply(lambda x: str(x.date()))\n",
    "sales_wide_df_sales=sales_long_df[['location_id','week_end_date','sales']].pivot(index='location_id',columns='week_end_date',values='sales')\n",
    "sales_wide_df_sales=sales_wide_df_sales.fillna(0)\n",
    "sales_wide_df_sales.to_excel(writer, sheet_name='sales')\n",
    "trans_wide_df_sales=sales_long_df[['location_id','week_end_date','transactions']].pivot(index='location_id',columns='week_end_date',values='transactions')\n",
    "trans_wide_df_sales=trans_wide_df_sales.fillna(0)\n",
    "trans_wide_df_sales.to_excel(writer, sheet_name='transactions')\n",
    "traffics_wide_df_sales=sales_long_df[['location_id','week_end_date','traffics']].pivot(index='location_id',columns='week_end_date',values='traffics')\n",
    "traffics_wide_df_sales=traffics_wide_df_sales.fillna(0)\n",
    "traffics_wide_df_sales.to_excel(writer, sheet_name='traffics')\n",
    "\n",
    "# summary=pd.DataFrame(columns=['location_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_sales_wide_df_sales=sales_wide_df_sales.T\n",
    "T_sales_wide_df_sales['sales']=T_sales_wide_df_sales[T_sales_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "T_sales_wide_df_sales=T_sales_wide_df_sales['sales'].to_frame().T\n",
    "\n",
    "T_trans_wide_df_sales=trans_wide_df_sales.T\n",
    "T_trans_wide_df_sales['trans']=T_trans_wide_df_sales[T_trans_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "T_trans_wide_df_sales=T_trans_wide_df_sales['trans'].to_frame().T\n",
    "\n",
    "T_traffics_wide_df_sales=traffics_wide_df_sales.T\n",
    "T_traffics_wide_df_sales['traffics']=T_traffics_wide_df_sales[T_traffics_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "T_traffics_wide_df_sales=T_traffics_wide_df_sales['traffics'].to_frame().T\n",
    "\n",
    "count_wide_df_sales=sales_wide_df_sales.copy()\n",
    "\n",
    "for col in count_wide_df_sales.columns.tolist():\n",
    "    count_wide_df_sales[col]=np.where(count_wide_df_sales[col]>0,1,0)\n",
    "\n",
    "T_count_wide_df_sales=count_wide_df_sales.T\n",
    "T_count_wide_df_sales['counts']=T_count_wide_df_sales[T_count_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "T_count_wide_df_sales=T_count_wide_df_sales['counts'].to_frame().T\n",
    "\n",
    "T_avg_sales_wide_df_sales=T_sales_wide_df_sales.copy()\n",
    "T_avg_sales_wide_df_sales.index=['avg_sales']\n",
    "for col in T_avg_sales_wide_df_sales.columns.tolist():\n",
    "    T_avg_sales_wide_df_sales[col]['avg_sales']=T_sales_wide_df_sales[col]['sales']/T_count_wide_df_sales[col]['counts']\n",
    "\n",
    "\n",
    "summary=T_sales_wide_df_sales.append(T_trans_wide_df_sales).append(T_traffics_wide_df_sales).append(T_count_wide_df_sales).append(T_avg_sales_wide_df_sales)\n",
    "\n",
    "summary.to_excel(writer,sheet_name=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a sales line chart\n",
    "chartloc = len(summary.index) + 4\n",
    "\n",
    "chart_sales = workbook.add_chart({'type': 'line'})\n",
    "chart_sales.add_series({\n",
    "        'name':       '2018',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "        'values':     ['summary', 1, summary.shape[1]-11, 1, summary.shape[1]]})\n",
    "\n",
    "chart_sales.add_series({\n",
    "        'name':       '2017',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "        'values':     ['summary', 1, summary.shape[1]-11-52, 1, summary.shape[1]-52]})\n",
    "\n",
    "chart_sales.add_series({\n",
    "        'name':       '2016',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "        'values':     ['summary', 1, summary.shape[1]-11-52-52, 1, summary.shape[1]-52-52]})\n",
    "\n",
    "chart_sales.set_x_axis({'name': 'Date'})\n",
    "chart_sales.set_y_axis({'name': 'Revenue', 'major_gridlines': {'visible': True}})\n",
    "\n",
    "chart_sales.set_size({'width': 960, 'height': 432})\n",
    "chart_sales.set_title({'name': 'Recent 12 Weeks Sales'})\n",
    "chart_sales.set_legend({'position': 'bottom'})\n",
    "worksheet.insert_chart('B'+str(chartloc), chart_sales) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a transaction line chart\n",
    "chartloc = len(summary.index) + 4\n",
    "\n",
    "chart_trans = workbook.add_chart({'type': 'line'})\n",
    "chart_trans.add_series({\n",
    "        'name':       '2018',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "        'values':     ['summary', 2, summary.shape[1]-11, 2, summary.shape[1]]})\n",
    "\n",
    "chart_trans.add_series({\n",
    "        'name':       '2017',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "        'values':     ['summary', 2, summary.shape[1]-11-52, 2, summary.shape[1]-52]})\n",
    "\n",
    "chart_trans.add_series({\n",
    "        'name':       '2016',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "        'values':     ['summary', 2, summary.shape[1]-11-52-52, 2, summary.shape[1]-52-52]})\n",
    "\n",
    "chart_trans.set_x_axis({'name': 'Date'})\n",
    "chart_trans.set_y_axis({'name': 'Transaction', 'major_gridlines': {'visible': True}})\n",
    "\n",
    "chart_trans.set_size({'width': 960, 'height': 432})\n",
    "chart_trans.set_title({'name': 'Recent 12 Weeks Transactions'})\n",
    "chart_trans.set_legend({'position': 'bottom'})\n",
    "worksheet.insert_chart('Q'+str(chartloc), chart_trans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a traffics line chart\n",
    "chartloc = len(summary.index) + 4\n",
    "\n",
    "chart_traffics = workbook.add_chart({'type': 'line'})\n",
    "chart_traffics.add_series({\n",
    "        'name':       '2018',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "        'values':     ['summary', 3, summary.shape[1]-11, 3, summary.shape[1]]})\n",
    "\n",
    "chart_traffics.add_series({\n",
    "        'name':       '2017',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "        'values':     ['summary', 3, summary.shape[1]-11-52, 3, summary.shape[1]-52]})\n",
    "\n",
    "chart_traffics.add_series({\n",
    "        'name':       '2016',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "        'values':     ['summary', 3, summary.shape[1]-11-52-52, 3, summary.shape[1]-52-52]})\n",
    "\n",
    "chart_traffics.set_x_axis({'name': 'Date'})\n",
    "chart_traffics.set_y_axis({'name': 'Revenue', \n",
    "                       'major_gridlines': {'visible': True}})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chart_traffics.set_size({'width': 960, 'height': 432})\n",
    "chart_traffics.set_title({'name': 'Recent 12 weeks traffics'})\n",
    "chart_traffics.set_legend({'position': 'bottom'})\n",
    "worksheet.insert_chart('AF'+str(chartloc), chart_traffics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a average sales line chart\n",
    "chartloc = len(summary.index) + 4+25\n",
    "\n",
    "chart_avg_sales = workbook.add_chart({'type': 'line'})\n",
    "chart_avg_sales.add_series({\n",
    "        'name':       '2018',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "        'values':     ['summary', 5, summary.shape[1]-11, 5, summary.shape[1]]})\n",
    "\n",
    "chart_avg_sales.add_series({\n",
    "        'name':       '2017',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "        'values':     ['summary', 5, summary.shape[1]-11-52, 5, summary.shape[1]-52]})\n",
    "\n",
    "chart_avg_sales.add_series({\n",
    "        'name':       '2016',\n",
    "        'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "        'values':     ['summary', 5, summary.shape[1]-11-52-52, 5, summary.shape[1]-52-52]})\n",
    "\n",
    "chart_avg_sales.set_x_axis({'name': 'Date'})\n",
    "chart_avg_sales.set_y_axis({'name': 'Avg_Revenue', \n",
    "                       'major_gridlines': {'visible': True}})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chart_avg_sales.set_size({'width': 960, 'height': 432})\n",
    "chart_avg_sales.set_title({'name': 'Recent 12 weeks sales per open store'})\n",
    "chart_avg_sales.set_legend({'position': 'bottom'})\n",
    "worksheet.insert_chart('B'+str(chartloc), chart_avg_sales) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_long_df_positive_sales=sales_long_df[sales_long_df['sales']>0]\n",
    "store_counts=sales_long_df_positive_sales.groupby(['week_end_date'])['location_id'].count().to_frame()\n",
    "store_counts.to_excel(writer,\"store_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_store_dma=pd.read_excel(\"/home/jian/Big_Lots/OtherInput/all_store_DMA_20180620.xlsx\")\n",
    "all_store_dma=all_store_dma[['location_id','DMA']]\n",
    "store_counts_DMA=pd.merge(sales_long_df_positive_sales,all_store_dma,on='location_id',how='left')\n",
    "store_counts_DMA=store_counts_DMA.groupby(['week_end_date','DMA'])['location_id'].count().to_frame()\n",
    "store_counts_DMA.reset_index(inplace=True)\n",
    "store_counts_DMA.to_excel(writer,\"store_counts_DMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=10))\n",
    "lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=12))\n",
    "\n",
    "# Tuesday_today_str=str(datetime.datetime.now().date())[0:4]+str(datetime.datetime.now().date())[5:7]+str(datetime.datetime.now().date())[8:10]\n",
    "Tuesday_today_str ='20180717'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jzou/biglots/weather/forcast_api_response/2018-07-18: 14.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forecast_date_str=str(datetime.datetime.now().date()) # Tuesday\n",
    "forecast_date_str=\"2018-07-17\"\n",
    "weather_forecast_file=\"/home/jzou/biglots/weather/forcast_api_response/\"+forecast_date_str+\": 14.json\" #2:00 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_forecast=json.load(open(weather_forecast_file,\"r\"))\n",
    "list_k=(5-datetime.datetime.strptime(forecast_date_str,\"%Y-%m-%d\").date().weekday())*8-1\n",
    "inclusion_store_df=pd.read_table(newsalespath,dtype=str,sep=\"|\")\n",
    "inclusion_store_df=inclusion_store_df[~inclusion_store_df['location_id'].isin(['145','6990'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inclusion_store_df['subclass_gross_sales_amt']=inclusion_store_df['subclass_gross_sales_amt'].astype(float)\n",
    "inclusion_store_df['gross_transaction_cnt']=inclusion_store_df['gross_transaction_cnt'].astype(float)\n",
    "\n",
    "inclusion_store_df_trans=inclusion_store_df[['location_id','week_end_dt','gross_transaction_cnt']].drop_duplicates()\n",
    "inclusion_store_df_sales=inclusion_store_df.groupby(['location_id','week_end_dt'])['subclass_gross_sales_amt'].sum().reset_index()\n",
    "\n",
    "inclusion_store_df_trans=inclusion_store_df_trans.rename(columns={\"gross_transaction_cnt\":\"transaction\"})\n",
    "inclusion_store_df_sales=inclusion_store_df_sales.rename(columns={\"subclass_gross_sales_amt\":\"sales\"})\n",
    "df_stores=pd.merge(inclusion_store_df_sales,inclusion_store_df_trans,on=[\"location_id\",\"week_end_dt\"],how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_list_txt=pd.read_table(\"/home/jian/Big_Lots/OtherInput/MediaStormStores_20180703.txt\",sep=\"|\",dtype=str)\n",
    "store_list_txt['zip_cd']=store_list_txt['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "store_list_txt=store_list_txt[['location_id','location_desc','address_line_1','address_line_2','city_nm','state_nm','zip_cd']]\n",
    "\n",
    "df_stores=pd.merge(df_stores,store_list_txt,on=\"location_id\",how=\"left\")\n",
    "df_stores=df_stores.reset_index()\n",
    "del df_stores['index']\n",
    "\n",
    "zip_DMA=pd.read_excel(\"/home/jian/Docs/Geo_mapping/Zips by DMA by County16-17 nielsen.xlsx\",skiprows=1,dtype=str)\n",
    "zip_DMA=zip_DMA.iloc[:,[0,2]].drop_duplicates()\n",
    "zip_DMA=zip_DMA.rename(columns={\"CODE\":\"zip_cd\",\"NAME\":\"DMA\"})\n",
    "zip_DMA=zip_DMA.drop_duplicates(['zip_cd'])\n",
    "df_stores=pd.merge(df_stores,zip_DMA,on=\"zip_cd\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_weight=pd.read_excel(\"/home/jian/Big_Lots/OtherInput/Q1_inclusion_store_all_weather_type_ranked.xlsx\",sheetname=\"all_weather_group_list\")\n",
    "group_weight['Severity']=group_weight['Severity'].astype(int)\n",
    "group_weight=group_weight[['all_type_group','Severity']]\n",
    "group_weight_dict=group_weight[['all_type_group', 'Severity']].set_index('all_type_group').T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_forecast_weather=pd.DataFrame()\n",
    "index_num=0\n",
    "for zip_cd in df_stores['zip_cd'].unique().tolist():\n",
    "    if zip_cd in list(json_forecast.keys()):\n",
    "        weather_list=json_forecast[zip_cd]['list'][list_k]['weather']\n",
    "        forecast_time=datetime.datetime.fromtimestamp(json_forecast[zip_cd]['list'][list_k]['dt'])\n",
    "        all_forecast_group_value_zip=[]\n",
    "        all_forecast_desc_value_zip=[]\n",
    "        for j in range(len(weather_list)):\n",
    "            weather_forecast_group=weather_list[j]['main']\n",
    "            weather_forecast_desc=weather_list[j]['description']\n",
    "            all_forecast_group_value_zip=list(set(all_forecast_group_value_zip+[weather_forecast_group]))\n",
    "            all_forecast_desc_value_zip=list(set(all_forecast_desc_value_zip+[weather_forecast_group]))\n",
    "\n",
    "        all_forecast_group_severity_zip=[]\n",
    "        all_forecast_desc_severity_zip=[]\n",
    "        for k in range(len(all_forecast_group_value_zip)):\n",
    "            forecast_severity_zip=group_weight_dict[all_forecast_group_value_zip[k]]['Severity']\n",
    "            all_forecast_group_severity_zip=list(set(all_forecast_group_severity_zip+[forecast_severity_zip]))\n",
    "            all_forecast_desc_severity_zip\n",
    "            if k==0:\n",
    "                selected_havest_forecast_group_value_zip=all_forecast_group_value_zip[0]\n",
    "                # selected_havest_forecast_desc_value_zip=\n",
    "            else:\n",
    "                if group_weight_dict[all_forecast_group_value_zip[k]]['Severity']>group_weight_dict[selected_havest_forecast_group_value_zip]['Severity']:\n",
    "                    selected_havest_forecast_group_value_zip=all_forecast_group_value_zip[k]\n",
    "                    # selected_havest_forecast_desc_value_zip\n",
    "        forecast_max_severity_zip=max(all_forecast_group_severity_zip)\n",
    "\n",
    "        df_app=pd.DataFrame({\"zip_cd\":zip_cd,\"Predicted_Saturday\":forecast_time,\"Forecast_Tuesday\":forecast_date_str,\"Forecast_Severity\":forecast_max_severity_zip,\n",
    "                            \"Forecast_Weather_Type\":selected_havest_forecast_group_value_zip},index=[index_num])\n",
    "        index_num=index_num+1\n",
    "        output_forecast_weather=output_forecast_weather.append(df_app)\n",
    "    else:\n",
    "        print(zip_cd,\"Not Collected\")\n",
    "        df_app=pd.DataFrame({\"zip_cd\":zip_cd,\"Predicted_Saturday\":\"Not_Collected\",\"Forecast_Tuesday\":\"Not_Collected\",\"Forecast_Severity\":'Not_Collected',\n",
    "                            \"Forecast_Weather_Type\":'Not_Collected'},index=[index_num])\n",
    "        index_num=index_num+1\n",
    "        output_forecast_weather=output_forecast_weather.append(df_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.merge(df_stores,output_forecast_weather,on=\"zip_cd\",how=\"left\")\n",
    "week_end_date=output['week_end_dt'].unique()[0]\n",
    "Saturday_str=str(output[output['Forecast_Time']!='Not_Collected']['Forecast_Time'].apply(lambda x: x.date()).unique()[0])\n",
    "output['location_id']=output['location_id'].astype(int)\n",
    "output=output.sort_values('location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"/home/jian/BiglotsCode/outputs/Output_\"+week_end_date+\"/weather_forecast_for_Saturday_\"+Saturday_str+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "Tuesday_StampDate_Str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1))\n",
    "Tuesday_today_str =Tuesday_StampDate_Str[0:4]+Tuesday_StampDate_Str[5:7]+Tuesday_StampDate_Str[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180717'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuesday_today_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
