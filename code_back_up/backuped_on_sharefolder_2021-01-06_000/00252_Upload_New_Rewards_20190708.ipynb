{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "# Add the new folders of Jian later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2019, 6, 29), datetime.date(2019, 6, 22)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "last_saturday=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()+2) #+7\n",
    "\n",
    "write_folder=\"/home/jian/celery/Append_to_celery_LiveRamp/monthly_update/output_\"+str(last_saturday)+\"/\"\n",
    "try:\n",
    "    os.stat(write_folder)\n",
    "except:\n",
    "    os.mkdir(write_folder)\n",
    "\n",
    "week_beginning=last_saturday-datetime.timedelta(days=14) # To be changed to 14\n",
    "\n",
    "recent_2_weeks=[last_saturday-datetime.timedelta(days=x*7) for x in range(2)]\n",
    "recent_2_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Checking for the recent weeks that haven't moved to the folder\n",
    "possible_recent_folders=[\"/home/jian/BigLots/MediaStorm_\"+str(x)+\"/\" for x in recent_2_weeks]\n",
    "recent_file_list=[]\n",
    "for dirc in possible_recent_folders:\n",
    "    list_recent=[x for x in list(recursive_file_gen(dirc)) if (\"Master\" in x) & (\".txt\" in x) ]\n",
    "    recent_file_list=recent_file_list+list_recent\n",
    "recent_file_df=pd.DataFrame({\"path\":recent_file_list,\"date\":[datetime.datetime.strptime(x.split(\"MasterWeekly\")[1][:8],\"%Y%m%d\").date()-datetime.timedelta(days=3) for x in recent_file_list]},index=[x for x in range(len(recent_file_list))])\n",
    "\n",
    "\n",
    "list_1_after_201806=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"Master\" in x) & (\".txt\" in x) ]\n",
    "folder_date=[datetime.datetime.strptime(x.split(\"/\")[len(x.split(\"/\"))-2].split(\"_\")[1],\"%Y-%m-%d\").date() for x in list_1_after_201806]\n",
    "df_1_after_201806=pd.DataFrame({\"date\":folder_date,\"path\":list_1_after_201806},index=[x for x in range(len(list_1_after_201806))])\n",
    "df_1_after_201806['date'].apply(lambda x: x.weekday()).unique()\n",
    "df_1_after_201806=df_1_after_201806.sort_values(\"date\").reset_index()\n",
    "df_1_after_201806=df_1_after_201806[df_1_after_201806['date']>week_beginning]\n",
    "del df_1_after_201806['index']\n",
    "\n",
    "new_2_biweekly_files=df_1_after_201806.append(recent_file_df)\n",
    "\n",
    "print(len(new_2_biweekly_files))\n",
    "new_2_biweekly_files=new_2_biweekly_files['path'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-06-22/MediaStormMasterWeekly20190625-111845-768.txt',\n",
       " '/home/jian/BigLots/MediaStorm_2019-06-29/MediaStormMasterWeekly20190702-113206-728.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_2_biweekly_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236692\n",
      "236683\n"
     ]
    }
   ],
   "source": [
    "if len(new_2_biweekly_files)==2: # Changed to weekly\n",
    "    df_all_new_master = pd.DataFrame()\n",
    "    for file in new_2_biweekly_files:\n",
    "        df = pd.read_csv(file,nrows = None,sep= '|',usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'],\n",
    "                         dtype =str)\n",
    "        df_all_new_master = df_all_new_master.append(df,ignore_index = True)\n",
    "    print(len(df_all_new_master.index))\n",
    "    df_all_new_master = df_all_new_master.drop_duplicates('email_address_hash')\n",
    "    print(len(df_all_new_master.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# Remove previous week ids\n",
    "\n",
    "previous_id_files_lists=list(recursive_file_gen(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_20190107/by_group_quadrant_24/\"))\n",
    "\n",
    "# previous_id_files_lists=[x for x in previous_id_files_lists if \"Copy of 48 Stores_727.csv\" not in x]\n",
    "previous_id_files_lists=[x for x in previous_id_files_lists if \".csv\" in x]\n",
    "print(len(previous_id_files_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "uploaded_new_seg=list(recursive_file_gen(\"/home/jian/celery/Append_to_celery_LiveRamp/monthly_update/\"))\n",
    "uploaded_new_seg=[x for x in uploaded_new_seg if \".csv\" in x]\n",
    "\n",
    "previous_id_files_lists=previous_id_files_lists+uploaded_new_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-08 13:59:56.127191 236680\n",
      "2019-07-08 13:59:58.182970 236674\n",
      "2019-07-08 13:59:58.826422 236674\n",
      "2019-07-08 13:59:59.573165 236672\n",
      "2019-07-08 14:00:00.181912 236672\n",
      "2019-07-08 14:00:00.331816 236672\n",
      "2019-07-08 14:00:00.456755 236672\n",
      "2019-07-08 14:00:00.526470 236672\n",
      "2019-07-08 14:00:00.583527 236672\n",
      "2019-07-08 14:00:00.640574 236672\n",
      "2019-07-08 14:00:00.738942 236672\n",
      "2019-07-08 14:00:00.847862 236672\n",
      "2019-07-08 14:00:00.908412 236672\n",
      "2019-07-08 14:00:00.974043 236672\n",
      "2019-07-08 14:00:01.035004 236672\n",
      "2019-07-08 14:00:01.087854 236672\n",
      "2019-07-08 14:00:01.200313 236671\n",
      "2019-07-08 14:00:01.298283 236671\n",
      "2019-07-08 14:00:01.349072 236671\n",
      "2019-07-08 14:00:01.416695 236671\n",
      "2019-07-08 14:00:01.467108 236671\n",
      "2019-07-08 14:00:01.711358 236671\n",
      "2019-07-08 14:00:01.923220 236671\n",
      "2019-07-08 14:00:02.012310 236671\n",
      "2019-07-08 14:00:02.098375 236671\n",
      "2019-07-08 14:00:02.151717 236671\n",
      "2019-07-08 14:00:06.369598 236669\n",
      "2019-07-08 14:00:08.260749 236665\n",
      "2019-07-08 14:00:14.419256 236663\n",
      "2019-07-08 14:00:15.613414 236662\n",
      "2019-07-08 14:00:16.011128 236662\n",
      "2019-07-08 14:00:16.753462 236662\n",
      "2019-07-08 14:00:17.173295 236661\n",
      "2019-07-08 14:00:17.590725 236660\n",
      "2019-07-08 14:00:18.602318 236660\n",
      "2019-07-08 14:00:35.316051 236655\n",
      "2019-07-08 14:00:36.896291 236654\n",
      "2019-07-08 14:00:40.116862 236654\n",
      "2019-07-08 14:00:41.072007 236653\n",
      "2019-07-08 14:00:41.343348 236653\n",
      "2019-07-08 14:00:41.757885 236653\n",
      "2019-07-08 14:00:42.014285 236653\n",
      "2019-07-08 14:00:42.117250 236653\n",
      "2019-07-08 14:00:42.316994 236653\n",
      "2019-07-08 14:00:52.889679 236646\n",
      "2019-07-08 14:00:55.828019 236646\n",
      "2019-07-08 14:01:08.302852 236645\n",
      "2019-07-08 14:01:23.827343 236644\n",
      "2019-07-08 14:01:24.516853 236644\n",
      "2019-07-08 14:01:25.965010 236643\n",
      "2019-07-08 14:01:26.503901 236643\n",
      "2019-07-08 14:01:26.735842 236643\n",
      "2019-07-08 14:01:27.163603 236643\n",
      "2019-07-08 14:04:31.904634 236626\n",
      "2019-07-08 14:06:22.390637 236622\n",
      "2019-07-08 14:06:24.032443 236621\n",
      "2019-07-08 14:06:24.342075 236621\n",
      "2019-07-08 14:06:24.455421 236621\n",
      "2019-07-08 14:06:24.514481 236621\n",
      "2019-07-08 14:06:24.570859 236621\n",
      "2019-07-08 14:06:24.634623 236621\n",
      "2019-07-08 14:06:24.698198 236621\n",
      "2019-07-08 14:06:24.741835 236621\n",
      "2019-07-08 14:06:24.791449 236621\n",
      "2019-07-08 14:06:24.984122 236621\n",
      "2019-07-08 14:06:25.111790 236621\n",
      "2019-07-08 14:06:25.192720 236621\n",
      "2019-07-08 14:06:25.240360 236621\n",
      "2019-07-08 14:06:25.345640 236621\n",
      "2019-07-08 14:06:25.423639 236621\n",
      "2019-07-08 14:06:25.480744 236621\n",
      "2019-07-08 14:06:25.555950 236621\n",
      "2019-07-08 14:06:26.407459 236621\n",
      "2019-07-08 14:06:26.696088 236621\n",
      "2019-07-08 14:06:26.894088 236621\n",
      "2019-07-08 14:06:27.581956 236621\n",
      "2019-07-08 14:06:27.776212 236621\n",
      "2019-07-08 14:06:27.904083 236621\n",
      "2019-07-08 14:06:28.528362 236621\n",
      "2019-07-08 14:06:28.722993 236621\n",
      "2019-07-08 14:06:28.842330 236621\n",
      "2019-07-08 14:06:29.561392 236621\n",
      "2019-07-08 14:06:29.782228 236621\n",
      "2019-07-08 14:06:29.899114 236621\n",
      "2019-07-08 14:06:30.341564 236621\n",
      "2019-07-08 14:06:30.487946 236621\n",
      "2019-07-08 14:06:30.580951 236621\n",
      "2019-07-08 14:06:30.921677 236620\n",
      "2019-07-08 14:06:31.022551 236620\n",
      "2019-07-08 14:06:31.095140 236620\n",
      "2019-07-08 14:06:31.438348 236620\n",
      "2019-07-08 14:06:31.578029 236620\n",
      "2019-07-08 14:06:31.661817 236620\n",
      "2019-07-08 14:06:31.995224 236620\n",
      "2019-07-08 14:06:32.158511 236620\n",
      "2019-07-08 14:06:32.242923 236620\n",
      "2019-07-08 14:06:32.610697 236620\n",
      "2019-07-08 14:06:32.747072 236620\n",
      "2019-07-08 14:06:32.841378 236620\n",
      "2019-07-08 14:06:33.222991 236620\n",
      "2019-07-08 14:06:33.360420 236620\n",
      "2019-07-08 14:06:33.593183 236620\n"
     ]
    }
   ],
   "source": [
    "for file_uploaded in previous_id_files_lists:\n",
    "    df = pd.read_csv(file_uploaded,usecols = ['email_address_hash'])\n",
    "    previous_email_set=set(df['email_address_hash'].tolist())\n",
    "    df_all_new_master = df_all_new_master[~df_all_new_master['email_address_hash'].isin(previous_email_set)]\n",
    "    print(datetime.datetime.now(),len(df_all_new_master.index),file_uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236620\n"
     ]
    }
   ],
   "source": [
    "df_all_new_master = df_all_new_master.drop_duplicates('email_address_hash')\n",
    "df_all_new_master = df_all_new_master.drop_duplicates('customer_id_hashed')\n",
    "print(len(df_all_new_master.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'P' nan 'T']\n",
      "['S' 'P' 'T']\n",
      "P 126281\n",
      "S 32116\n",
      "T 78223\n"
     ]
    }
   ],
   "source": [
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "df_all_new_master = pd.merge(df_all_new_master,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(df_all_new_master['zipcodegroup'].unique())\n",
    "\n",
    "df_all_new_master['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(df_all_new_master['zipcodegroup'].unique())\n",
    "\n",
    "last_saturday_str=\"_\"+str(last_saturday).split(\"-\")[1]+str(last_saturday).split(\"-\")[2]+str(last_saturday).split(\"-\")[0][2:]+\"_\"\n",
    "\n",
    "for revenue_flag in ['P','S','T']:\n",
    "    df = df_all_new_master[df_all_new_master['zipcodegroup']==revenue_flag]\n",
    "    df = df[['customer_id_hashed','email_address_hash','customer_zip_code']]\n",
    "    df['segment'] = 'NewReward' + last_saturday_str + revenue_flag\n",
    "    print(revenue_flag,len(df.index))\n",
    "    df.to_csv(write_folder+'NewReward_'+str(last_saturday)+'_'+revenue_flag+'.csv',index = False)\n",
    "# To be uploaded to lr-big-lots /uploads/big_lots_onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcodegroup</th>\n",
       "      <th>email_address_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>126281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>32116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T</td>\n",
       "      <td>78223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zipcodegroup  email_address_hash\n",
       "0            P              126281\n",
       "1            S               32116\n",
       "2            T               78223"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_new_master.groupby(['zipcodegroup'])['email_address_hash'].count().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>email_address_hash</th>\n",
       "      <th>customer_zip_code</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b2f02a0b6ac6d835d570122eac60f9e9c562f58136045d...</td>\n",
       "      <td>42e661f2f7dc149ee14f6ac8aa272843e0c1bec608d796...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NewReward_062919_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1357dd099a168b0f42b698a9a04aca395c9ed039b97d16...</td>\n",
       "      <td>56020b811573e6c045675016bbbc4f1e2c4da7693f3434...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NewReward_062919_T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed  \\\n",
       "4  b2f02a0b6ac6d835d570122eac60f9e9c562f58136045d...   \n",
       "5  1357dd099a168b0f42b698a9a04aca395c9ed039b97d16...   \n",
       "\n",
       "                                  email_address_hash customer_zip_code  \\\n",
       "4  42e661f2f7dc149ee14f6ac8aa272843e0c1bec608d796...               NaN   \n",
       "5  56020b811573e6c045675016bbbc4f1e2c4da7693f3434...               NaN   \n",
       "\n",
       "              segment  \n",
       "4  NewReward_062919_T  \n",
       "5  NewReward_062919_T  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import glob\n",
    "\n",
    "host = \"files.liveramp.com\"\n",
    "port = 22\n",
    "password = \"Biglots2018!\"\n",
    "username = \"lr-big-lots\"\n",
    "\n",
    "transport = paramiko.Transport((host, port))\n",
    "\n",
    "transport.connect(username = username, password = password)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "# local_path defined above before saving the local txt\n",
    "local_list=glob.glob(write_folder+\"*.csv\")\n",
    "local_list=[x for x in local_list if 'NewReward_'+str(last_saturday)+'_' in x]\n",
    "\n",
    "for local_path in local_list:\n",
    "    remote_path=\"/uploads/big_lots_onboarding/\"+os.path.basename(local_path)\n",
    "    sftp.put(local_path,remote_path)\n",
    "sftp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
