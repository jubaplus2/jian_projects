{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapsed Start on: 2017-10-01\n",
      "Active Start on: 2018-04-01\n",
      "Store Allocation Starting on: 2017-10-01\n",
      "Last Saturday: 2019-03-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/Codes_Versions'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V7-4R\n",
    "# upto 20190330\n",
    "# merge email first\n",
    "# then calculate decile\n",
    "# No Remove\n",
    "\n",
    "\n",
    "# In other words: all the same, except for the time range\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='Version7_4R_BL_CRM_LR_Quarterly_upto20190330_MergeEmail_Decile_NoRemove.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "\n",
    "lastdate = datetime.date(2019,3,30) # Recent Saturday\n",
    "active_Sunday = str(lastdate-datetime.timedelta(days=52*7-1))\n",
    "# active_Sunday = \"2017-12-29\"\n",
    "lapsed_Sunday = str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "# lapsed_Sunday = \"2017-06-29\"\n",
    "Beginning_18_months_ago=str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "# Beginning_18_months_ago = \"2017-06-29\"\n",
    "\n",
    "lastdate=str(lastdate)\n",
    "print(\"Lapsed Start on: \"+lapsed_Sunday) #>=\n",
    "print(\"Active Start on: \"+active_Sunday) #>=\n",
    "print(\"Store Allocation Starting on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Last Saturday: \"+lastdate) #<=\n",
    "\n",
    "def recrusive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/output/Version7_4R_upto20190330_MergeEmail_Decile_NoRemove_'+str(datetime.datetime.now().date())+'/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-04-20 19:41:12.728460\n",
      "Earliest Date:2018-01-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize_num = 10**7\n",
    "filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "dftrans_before_20180922=pd.DataFrame()\n",
    "count_i=0\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str,usecols=['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_amt'],nrows=samplerows): #Add back the transaction info\n",
    "    chunk['total_transaction_amt']=chunk['total_transaction_amt'].astype(float)\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    \n",
    "    dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "    count_i+=1\n",
    "    print(count_i,datetime.datetime.now())\n",
    "\n",
    "\n",
    "del chunk\n",
    "print(\"Earliest Date:\" + str(dftrans_before_20180922['transaction_date'].min()))\n",
    "dftrans_before_20180922=dftrans_before_20180922.sort_values(['customer_id_hashed','transaction_date'],ascending=[True,False])\n",
    "dftrans_before_20180922=dftrans_before_20180922.drop_duplicates('customer_id_hashed')\n",
    "\n",
    "logging.info(\"Deduped: \"+str(datetime.datetime.now()))\n",
    "del dftrans_before_20180922['transaction_time']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2018-09-29\n",
      "Max_Date: 2019-02-09\n"
     ]
    }
   ],
   "source": [
    "# Up to 2019-03-30\n",
    "# All item level data, weekly and the 1-time transfered historical data\n",
    "historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "historical_daily_data_list=list(recrusive_file_gen(historical_daily_data_folder))\n",
    "historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1])\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y%m%d\").date())\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']<=datetime.date(2019,2,12)]\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']>datetime.date(2018,9,22)]\n",
    "print(historical_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(historical_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(historical_daily_df['week_end_dt'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n",
      "Min_Date: 2019-02-16\n",
      "Max_Date: 2019-03-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndaily_df_file_after_20180922=historical_daily_df.copy()\\nnew_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_daily_data_folder=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "new_daily_data_list=list(recrusive_file_gen(new_daily_data_folder))\n",
    "new_daily_data_list=[x for x in new_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "new_daily_data_list=[x for x in new_daily_data_list if \"hist\" not in x]\n",
    "\n",
    "new_daily_df=pd.DataFrame({\"file_path\":new_daily_data_list})\n",
    "\n",
    "new_daily_df['week_end_dt']=new_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2019_by_weeks/MediaStorm_\")[1][:10])\n",
    "new_daily_df['week_end_dt']=new_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']<=datetime.date(2019,3,30)]\n",
    "print(new_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(new_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(new_daily_df['week_end_dt'].max()))\n",
    "\n",
    "daily_df_file_after_20180922=historical_daily_df.append(new_daily_df)\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\n",
    "'''\n",
    "daily_df_file_after_20180922=historical_daily_df.copy()\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 27\n",
      "1 done 2019-04-20 19:41:13.253087\n",
      "2 done 2019-04-20 19:41:13.299667\n",
      "3 done 2019-04-20 19:41:13.346428\n",
      "4 done 2019-04-20 19:41:13.394186\n",
      "5 done 2019-04-20 19:41:13.442326\n",
      "6 done 2019-04-20 19:41:13.488776\n",
      "7 done 2019-04-20 19:41:13.536852\n",
      "8 done 2019-04-20 19:41:13.583769\n",
      "9 done 2019-04-20 19:41:13.630434\n",
      "10 done 2019-04-20 19:41:13.679426\n",
      "11 done 2019-04-20 19:41:13.727231\n",
      "12 done 2019-04-20 19:41:13.773338\n",
      "13 done 2019-04-20 19:41:13.821125\n",
      "14 done 2019-04-20 19:41:13.867635\n",
      "15 done 2019-04-20 19:41:13.916147\n",
      "16 done 2019-04-20 19:41:13.966349\n",
      "17 done 2019-04-20 19:41:14.021915\n",
      "18 done 2019-04-20 19:41:14.070691\n",
      "19 done 2019-04-20 19:41:14.131689\n",
      "20 done 2019-04-20 19:41:14.179912\n",
      "21 done 2019-04-20 19:41:14.228524\n",
      "22 done 2019-04-20 19:41:14.284323\n",
      "23 done 2019-04-20 19:41:14.337428\n",
      "24 done 2019-04-20 19:41:14.390775\n",
      "25 done 2019-04-20 19:41:14.442395\n",
      "26 done 2019-04-20 19:41:14.504352\n",
      "27 done 2019-04-20 19:41:14.566752\n"
     ]
    }
   ],
   "source": [
    "combined_rewards_transaction_after_20180922_agg=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.drop_duplicates()\n",
    "\n",
    "    \n",
    "    combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "gc.collect()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"transaction_dt\":\"transaction_date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.groupby(['customer_id_hashed','transaction_date','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"item_transaction_amt\":\"total_transaction_amt\"})\n",
    "\n",
    "all_rewards_since_201606=dftrans_before_20180922.append(combined_rewards_transaction_after_20180922_agg)\n",
    "\n",
    "del dftrans_before_20180922\n",
    "del combined_rewards_transaction_after_20180922_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/output/BL_Rewards_Transactions_20160626_to_20190330.csv\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16833, 4)\n",
      "(16829, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the store for an id\n",
    "\n",
    "frequently_visit_stores_18_months=all_rewards_since_201606[all_rewards_since_201606['transaction_date']>=Beginning_18_months_ago]\n",
    "\n",
    "frequently_visit_stores_2=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['total_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "\n",
    "frequently_visit_stores_18_months=pd.merge(frequently_visit_stores_18_months,frequently_visit_stores_2,on=['customer_id_hashed','location_id'],how=\"outer\")\n",
    "del frequently_visit_stores_2\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.sort_values(['customer_id_hashed','trans','sales'],ascending=[True,False,False])\n",
    "\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months[['customer_id_hashed','location_id']].drop_duplicates(\"customer_id_hashed\")\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months.to_csv(folder_write+\"frequently_visit_stores_18_months.csv\",index=False)\n",
    "del frequently_visit_stores_18_months\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_rewards_since_201606['transaction_id']\n",
    "del all_rewards_since_201606['location_id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-09\n",
      "2019-03-30\n"
     ]
    }
   ],
   "source": [
    "###get recency\n",
    "dfrecency=all_rewards_since_201606[['customer_id_hashed','transaction_date']].sort_values(\"transaction_date\",ascending=False).drop_duplicates()#Allready combined\n",
    "\n",
    "print (min(dfrecency['transaction_date']))\n",
    "print (max(dfrecency['transaction_date']))\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16829, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrecency['transaction_date'] = pd.to_datetime(dfrecency['transaction_date'])\n",
    "dfrecency['recency'] =  datetime.datetime.strptime(str(lastdate), '%Y-%m-%d').date() - dfrecency['transaction_date']\n",
    "dfrecency['recency'] = dfrecency['recency'].apply(lambda x:x.days)\n",
    "dfrecency['recency'] = np.ceil((dfrecency['recency']+1)/30)\n",
    "\n",
    "dfrecency = dfrecency[['customer_id_hashed','recency']]\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency2.csv',index = False)\n",
    "\n",
    "dfrecency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606['transactions'] = 1\n",
    "dftotal = all_rewards_since_201606[['customer_id_hashed','total_transaction_amt','transactions']].groupby(['customer_id_hashed']).sum().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency,on = 'customer_id_hashed',how='outer')\n",
    "del dfrecency\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal = dftotal.sort_values(['transactions','recency','sales'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Transindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['sales','recency','transactions'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Amtindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['recency','transactions','sales'],ascending = [1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'recencyindex'})\n",
    "\n",
    "c_ids = len(dftotal.index)\n",
    "logging.info('total customers from transaction and amt: ')\n",
    "logging.info(c_ids)\n",
    "c_ids = np.ceil(c_ids/5.0)\n",
    "\n",
    "dftotal['Transindex'] = np.ceil((dftotal['Transindex']+1)/c_ids)\n",
    "dftotal['Amtindex'] = np.ceil((dftotal['Amtindex']+1)/c_ids)\n",
    "dftotal['recencyindex'] = np.ceil((dftotal['recencyindex']+1)/c_ids)\n",
    "\n",
    "dftotal['RFM'] = dftotal['recencyindex']*100 + dftotal['Transindex']*10 + dftotal['Amtindex']\n",
    "'''\n",
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm.csv',index = False)\n",
    "\n",
    "dftotal = pd.read_csv(folder_write + 'dfrfm.csv')\n",
    "'''\n",
    "dftotal = dftotal[['customer_id_hashed','RFM','recency','transactions','sales']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-20 19:42:08.820151 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-12/MediaStormMasterBiWeekly20190115-132855-055.txt\n",
      "2019-04-20 19:42:08.840202 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-26/MediaStormMasterBiWeekly20190129-130902-016.txt\n",
      "2019-04-20 19:42:08.862298 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-02/MediaStormMasterWeekly20190205-111610-675.txt\n",
      "2019-04-20 19:42:08.882272 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-09/MediaStormMasterWeekly20190212-122428-267.txt\n",
      "2019-04-20 19:42:08.903923 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-16/MediaStormMasterWeekly20190219-113650-867.txt\n",
      "2019-04-20 19:42:08.927233 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-23/MediaStormMasterWeekly20190226-112921-061.txt\n",
      "2019-04-20 19:42:08.948034 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-02/MediaStormMasterWeekly20190305-112945-302.txt\n",
      "2019-04-20 19:42:08.968491 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-09/MediaStormMasterWeekly20190312-121512-232.txt\n",
      "2019-04-20 19:42:08.997730 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-16/MediaStormMasterWeekly20190319-112932-415.txt\n",
      "2019-04-20 19:42:09.020051 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-23/MediaStormMasterWeekly20190326-113052-887.txt\n",
      "2019-04-20 19:42:09.048761 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-30/MediaStormMasterWeekly20190402-113131-172.txt\n"
     ]
    }
   ],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "#########\n",
    "\n",
    "new_sign_ups_2019_list=list(recrusive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "new_sign_ups_2019_list=sorted([x for x in new_sign_ups_2019_list if \"ster\" in x])\n",
    "\n",
    "new_sign_ups_2019_df=pd.DataFrame({\"file_path\":new_sign_ups_2019_list})\n",
    "new_sign_ups_2019_df['Date']=new_sign_ups_2019_df['file_path'].apply(lambda x: x.split(\"MediaStorm_\")[1][:10])\n",
    "new_sign_ups_2019_df=new_sign_ups_2019_df[new_sign_ups_2019_df['Date']<=lastdate]\n",
    "\n",
    "for file_new_signups in new_sign_ups_2019_df['file_path'].tolist():\n",
    "    df=pd.read_table(file_new_signups,dtype=str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'],sep=\"|\",nrows=samplerows)\n",
    "    dfiddetail=df.append(dfiddetail) # Already sorted and newest kept on the top\n",
    "    print(datetime.datetime.now(),file_new_signups)\n",
    "dfiddetail=dfiddetail.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "######\n",
    "dfiddetail2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail = dfiddetail.append(dfiddetail2,ignore_index = True)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "dfiddetail = dfiddetail.drop_duplicates('email_address_hash')\n",
    "\n",
    "del dfiddetail2\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "logging.info(\"CheckingPoint2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active', 'lapsed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfrecency = pd.read_csv(folder_write + 'dfrecency.csv')\n",
    "dfrecency['active'] = np.where(dfrecency['transaction_date']>=active_Sunday,'active',\n",
    "                               np.where(dfrecency['transaction_date']>=lapsed_Sunday,'lapsed','other')\n",
    "                              )\n",
    "\n",
    "print(dfrecency['active'].unique().tolist())\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency[['customer_id_hashed','active']],on = 'customer_id_hashed')\n",
    "\n",
    "logging.info(str(dfrecency['active'].unique().tolist()))\n",
    "del dfrecency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"CheckingPoint1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n",
      "totalids_trans: 16829\n",
      "totalids_trans_mergewithmaster: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].astype('str')\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].str[0:5]\n",
    "dfiddetail['customer_zip_code'].fillna('00000',inplace = True)\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].apply(lambda x:x.zfill(5))\n",
    "print(len(dfiddetail.index))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "print(\"totalids_trans:\",len(dftotal.index))\n",
    "dftotal = pd.merge(dftotal,dfiddetail,on = 'customer_id_hashed')\n",
    "print(\"totalids_trans_mergewithmaster:\",len(dftotal.index))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "del dfiddetail\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P' 'S' 'T']\n",
      "['P' 'S' 'T']\n",
      "Final wemailcsv: (11, 10)\n"
     ]
    }
   ],
   "source": [
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "\n",
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "dftotal = pd.merge(dftotal,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "dftotal['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_wemail.csv',index = False)\n",
    "print(\"Final wemailcsv:\",dftotal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_other_18_plus=dftotal[dftotal[\\'active\\']==\"other\"][[\\'customer_id_hashed\\',\\'email_address_hash\\']]\\ndf_other_18_plus[\\'segment\\']=\"18_months_plus_back_201606\"\\n\\ndftotal=dftotal[dftotal[\\'active\\']!=\"other\"]\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the ids don't have transactions within 18 months\n",
    "'''\n",
    "df_other_18_plus=dftotal[dftotal['active']==\"other\"][['customer_id_hashed','email_address_hash']]\n",
    "df_other_18_plus['segment']=\"18_months_plus_back_201606\"\n",
    "\n",
    "dftotal=dftotal[dftotal['active']!=\"other\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the primary stores for each member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequently_visit_stores_18_months=pd.read_csv(folder_write+\"frequently_visit_stores_18_months.csv\",dtype=str)\n",
    "register_stores=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/output_sing_up_location/BL_id_by_register_store_JL_2019-04-09.csv\",\n",
    "                            dtype=str,nrows=samplerows)\n",
    "register_stores=register_stores[['customer_id_hashed','sign_up_location']].rename(columns={\"sign_up_location\":\"location_id\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17829, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_for_ids=frequently_visit_stores_18_months.append(register_stores)\n",
    "store_for_ids=store_for_ids.drop_duplicates(\"customer_id_hashed\")\n",
    "store_for_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del frequently_visit_stores_18_months\n",
    "del register_stores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal=pd.merge(dftotal,store_for_ids,on=\"customer_id_hashed\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1361, 2)\n",
      "1361\n"
     ]
    }
   ],
   "source": [
    "# read the quadrant by store for 2018 Q4\n",
    "\n",
    "Q4_store_quadrant=pd.read_excel(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Excel_BL_2018_Q4_post_YoY_small_JL_2019-03-04.xlsx\",\n",
    "                                dtype=str,sheetname=\"Q4_Store_Quadrant_Defination\",usecols=['location_id','Quadrant'])\n",
    "print(Q4_store_quadrant.shape)\n",
    "print(len(Q4_store_quadrant['location_id'].unique()))\n",
    "\n",
    "dftotal=pd.merge(dftotal,Q4_store_quadrant,on=\"location_id\",how=\"left\")\n",
    "dftotal['Quadrant']=dftotal['Quadrant'].fillna(\"Quadrant III\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89172"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "dftotal['customer_zip_code']=dftotal['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x:\"D\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal.to_csv(folder_write + 'dfrfm_final_details_wemail_zip_StoreQuad.csv',index = False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_H=pd.DataFrame({\"frmindex\":['D01','D02','D03','D04']})\n",
    "df_H['HML_Group']=\"H\"\n",
    "\n",
    "df_M=pd.DataFrame({\"frmindex\":['D05','D06','D07']})\n",
    "df_M['HML_Group']=\"M\"\n",
    "\n",
    "df_L=pd.DataFrame({\"frmindex\":['D08','D09','D10']})\n",
    "df_L['HML_Group']=\"L\"\n",
    "\n",
    "df_HML=df_H.append(df_M).append(df_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D01', 'D02', 'D03', 'D04', 'D05', 'D06', 'D07', 'D08', 'D09', 'D10'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "dftotal['frmindex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal=pd.merge(dftotal,df_HML,on='frmindex') # both 10\n",
    "\n",
    "dftotal['segment_2019Q2']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['HML_Group']+\"_2019Q2\"\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "total_rows=len(dftotal)\n",
    "\n",
    "test_all_df=pd.DataFrame()\n",
    "control_all_df=pd.DataFrame()\n",
    "\n",
    "i_counter=0\n",
    "\n",
    "dftotal=dftotal[['segment_2019Q2','customer_id_hashed','email_address_hash']]\n",
    "\n",
    "for seg,group in dftotal.groupby(['segment_2019Q2']):\n",
    "    random_list=random.sample(range(len(group)), int(np.round(len(group)/total_rows*500000)))\n",
    "\n",
    "    group=group.reset_index()\n",
    "    del group['index']\n",
    "    group=group.reset_index()\n",
    "    df_control=group[group['index'].isin(random_list)]\n",
    "    df_test=group[~group['index'].isin(random_list)]\n",
    "    \n",
    "    df_control['segment_2019Q2']=\"C_\"+df_control['segment_2019Q2']\n",
    "    df_test['segment_2019Q2']=\"T_\"+df_test['segment_2019Q2']\n",
    "    test_all_df=test_all_df.append(df_test)\n",
    "    control_all_df=control_all_df.append(df_control)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dftotal\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all_df.to_csv(folder_write+\"all_test.csv\",index=False)\n",
    "control_all_df.to_csv(folder_write+\"all_control.csv\",index=False)\n",
    "\n",
    "folder_write_inner = folder_write+'by_group/'\n",
    "try:\n",
    "    os.stat(folder_write_inner)\n",
    "except:\n",
    "    os.mkdir(folder_write_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_counter=0\n",
    "for seg,group in test_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_counter=0\n",
    "for seg,group in control_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_trans=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed'],dtype=str).drop_duplicates() # Doesn't go to score at all, so no need to read all columns\n",
    "lapsed_trans['lapsed_trans']=True\n",
    "\n",
    "lapsed_master=pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "\n",
    "lapsed_master=lapsed_master.drop_duplicates(\"customer_id_hashed\")\n",
    "print(lapsed_master.shape)\n",
    "\n",
    "lapsed_master=pd.merge(lapsed_master,lapsed_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "print(lapsed_master.shape)\n",
    "lapsed_master=lapsed_master[~pd.isnull(lapsed_master['email_address_hash'])]\n",
    "\n",
    "# remove the non-match email ids at the end and no calculation for the WD here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary_18_plus_back_20160626=df_other_18_plus.groupby('segment')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(test_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(control_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(test_all_df['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(control_all_df['email_address_hash'])]\n",
    "'''\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "'''\n",
    "lapsed_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_master=lapsed_master[['customer_id_hashed','email_address_hash']]\n",
    "lapsed_master['segment']=\"WalkingDead_2019Q2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_master.to_csv(folder_write_inner+\"WalkingDead_Group_before_20160626.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_test=test_all_df.groupby('segment_2019Q2')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_control=control_all_df.groupby('segment_2019Q2')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_WD=lapsed_master.groupby('segment')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "\n",
    "summary_overll=summary_test.append(summary_control).append(summary_WD)\n",
    "# .append(summary_18_plus_back_20160626)\n",
    "\n",
    "\n",
    "summary_overll.to_csv(folder_write_inner+\"test_control_groups_summary_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_overll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
