{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Post_YoY'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2019 Q1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_info=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190501-135143-036.txt\",sep=\"|\",dtype=str)\n",
    "store_info=store_info[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190401-134939-117.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190301-134800-131.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190201-133832-957.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190101-135843-638.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20181201-135231-415.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20181101-134628-331.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20181001-135417-132.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180901-133640-935.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180801-133641-576.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180703.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20171115.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20170913.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "store_info['zip_cd']=store_info['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "\n",
    "\n",
    "DMA_Zip=pd.read_excel(\"/home/jian/Docs/Geo_mapping/Zips by DMA by County16-17 nielsen.xlsx\",skiprows=1,dtype=str)\n",
    "DMA_Zip=DMA_Zip.iloc[:,[0,2]]\n",
    "DMA_Zip.columns=['zip_cd','DMA']\n",
    "DMA_Zip=DMA_Zip.drop_duplicates(['zip_cd'])\n",
    "\n",
    "\n",
    "store_info=pd.merge(store_info,DMA_Zip,on=\"zip_cd\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weeks_2019Q1=[datetime.date(2019,2,9)+datetime.timedelta(days=x*7) for x in range(13)]\n",
    "weeks_2018Q1=[x-datetime.timedelta(days=52*7) for x in weeks_2019Q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2018, 2, 10),\n",
       " datetime.date(2018, 2, 17),\n",
       " datetime.date(2018, 2, 24),\n",
       " datetime.date(2018, 3, 3),\n",
       " datetime.date(2018, 3, 10),\n",
       " datetime.date(2018, 3, 17),\n",
       " datetime.date(2018, 3, 24),\n",
       " datetime.date(2018, 3, 31),\n",
       " datetime.date(2018, 4, 7),\n",
       " datetime.date(2018, 4, 14),\n",
       " datetime.date(2018, 4, 21),\n",
       " datetime.date(2018, 4, 28),\n",
       " datetime.date(2018, 5, 5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks_2018Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_files_2018=\"/home/jian/BigLots/2017_Daily_Sales_Weeks_Quarter_as_Transfer/2018_Q1/*.txt\"\n",
    "Q1_files_2018=glob.glob(Q1_files_2018)\n",
    "\n",
    "len(Q1_files_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "folder_2019_by_week=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "\n",
    "Q1_files_2019_all_types=list(recursive_file_gen(folder_2019_by_week))\n",
    "Q1_files_2019_all_types=[x for x in Q1_files_2019_all_types if \"aily\" in x]\n",
    "Q1_files_2019=[]\n",
    "for week_end_date in weeks_2019Q1:\n",
    "    file_path=[x for x in Q1_files_2019_all_types if str(week_end_date) in x]\n",
    "    Q1_files_2019=Q1_files_2019+file_path\n",
    "    \n",
    "print(len(Q1_files_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def week_end_dt(x):\n",
    "    if x.weekday()==6:\n",
    "        y=x+datetime.timedelta(days=6)\n",
    "    else:\n",
    "        y=x+datetime.timedelta(days=(5-x.weekday()))\n",
    "    return y\n",
    "\n",
    "def count_unique(x):\n",
    "    return len(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_agg_df_subclass(file_list):\n",
    "    \n",
    "    i_counter=0\n",
    "\n",
    "    sales_agg_df=pd.DataFrame()\n",
    "    ids_by_week_store=pd.DataFrame()\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df=pd.read_table(file_path,dtype=str,sep=\"|\",usecols=None,nrows=None)\n",
    "        df=df[df['location_id']!=\"6990\"]\n",
    "        # print(df.shape)\n",
    "        df=df.drop_duplicates()\n",
    "        # print(df.shape)\n",
    "        df['transaction_dt']=df['transaction_dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n",
    "        df['subclass_transaction_amt']=df['subclass_transaction_amt'].astype(float)\n",
    "\n",
    "        df['rewards_label']=np.where(pd.isnull(df['customer_id_hashed']),\"Non_Rewards\",\"Rewards\")\n",
    "\n",
    "        date_max=df['transaction_dt'].max()\n",
    "        date_min=df['transaction_dt'].min()\n",
    "\n",
    "        # print(i_counter,date_max,date_min,datetime.datetime.now())\n",
    "\n",
    "        if ((date_max-date_min).days==6) & (date_max.weekday()==5):\n",
    "            df['week_end_dt']=date_max\n",
    "            df_agg_sales=df.groupby(['location_id','week_end_dt','rewards_label'])['subclass_transaction_amt'].sum().to_frame().reset_index()\n",
    "            df_agg_sales=df_agg_sales.rename(columns={\"subclass_transaction_amt\":\"sales\"})\n",
    "            \n",
    "            df=df[df['subclass_transaction_amt']>0]\n",
    "            df_agg_trans=df[['location_id','transaction_dt','week_end_dt','transaction_id','customer_id_hashed','rewards_label']].drop_duplicates()\n",
    "            df_agg_trans['transactions']=1\n",
    "            df_agg_trans=df_agg_trans.groupby(['location_id','week_end_dt','rewards_label'])['transactions'].sum().to_frame().reset_index()\n",
    "            \n",
    "            df_agg_sales=pd.merge(df_agg_sales,df_agg_trans,on=['location_id','week_end_dt','rewards_label'],how=\"outer\")\n",
    "            \n",
    "            df=df[df['rewards_label']==\"Rewards\"]\n",
    "            df=df[['location_id','week_end_dt','customer_id_hashed']].drop_duplicates()\n",
    "\n",
    "        else:\n",
    "            print(\"Date in the data not 7 days\",file_path)\n",
    "            df=pd.DataFrame()\n",
    "            df_agg_sales=pd.DataFrame()\n",
    "\n",
    "        sales_agg_df=sales_agg_df.append(df_agg_sales)\n",
    "        ids_by_week_store=ids_by_week_store.append(df)\n",
    "        i_counter+=1\n",
    "\n",
    "    return sales_agg_df,ids_by_week_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>transaction_dt</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>class_code_id</th>\n",
       "      <th>subclass_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_transaction_units</th>\n",
       "      <th>item_transaction_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4645</td>\n",
       "      <td>2019-02-16</td>\n",
       "      <td>5694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12006</td>\n",
       "      <td>1</td>\n",
       "      <td>810216162</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000001.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1363</td>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>861</td>\n",
       "      <td>6405a1a5e7c8e7eed6d6d85069660cfa31d4d635403dfd...</td>\n",
       "      <td>11009</td>\n",
       "      <td>2</td>\n",
       "      <td>110106912</td>\n",
       "      <td>2</td>\n",
       "      <td>00000000000001.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1012</td>\n",
       "      <td>2019-02-10</td>\n",
       "      <td>4867</td>\n",
       "      <td>f56a202e1bbdca986d3a2e70cb760ab099740c4561b3c7...</td>\n",
       "      <td>15004</td>\n",
       "      <td>2</td>\n",
       "      <td>810297158</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000001.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4554</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14001</td>\n",
       "      <td>2</td>\n",
       "      <td>810155132</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000004.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1181</td>\n",
       "      <td>2019-02-10</td>\n",
       "      <td>6568</td>\n",
       "      <td>da6a24644efbdd4a773b59f6c15fbc1bc0550d10a4e4a6...</td>\n",
       "      <td>11003</td>\n",
       "      <td>4</td>\n",
       "      <td>810135555</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000000.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>242</td>\n",
       "      <td>2019-02-16</td>\n",
       "      <td>9164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35046</td>\n",
       "      <td>6</td>\n",
       "      <td>810385788</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000005.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4295</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>413</td>\n",
       "      <td>0c1be82404797e3b741177ea0ec45c1e4584b501a40f0d...</td>\n",
       "      <td>12006</td>\n",
       "      <td>14</td>\n",
       "      <td>810333370</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000003.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4551</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>9643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12006</td>\n",
       "      <td>1</td>\n",
       "      <td>810216162</td>\n",
       "      <td>2</td>\n",
       "      <td>00000000000002.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4617</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>7874</td>\n",
       "      <td>58a4f4c1783f49049b2d1ce5b86f7bfdebd8176eb15878...</td>\n",
       "      <td>11007</td>\n",
       "      <td>6</td>\n",
       "      <td>810363610</td>\n",
       "      <td>3</td>\n",
       "      <td>00000000000002.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>461</td>\n",
       "      <td>2019-02-16</td>\n",
       "      <td>6863</td>\n",
       "      <td>6acaacee2b0e2ee082775abd4b94d67646b42877849f86...</td>\n",
       "      <td>48001</td>\n",
       "      <td>1</td>\n",
       "      <td>810348406</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000000005.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id transaction_dt transaction_id  \\\n",
       "0        4645     2019-02-16           5694   \n",
       "1        1363     2019-02-13            861   \n",
       "2        1012     2019-02-10           4867   \n",
       "3        4554     2019-02-14           9977   \n",
       "4        1181     2019-02-10           6568   \n",
       "5         242     2019-02-16           9164   \n",
       "6        4295     2019-02-11            413   \n",
       "7        4551     2019-02-11           9643   \n",
       "8        4617     2019-02-15           7874   \n",
       "9         461     2019-02-16           6863   \n",
       "\n",
       "                                  customer_id_hashed class_code_id  \\\n",
       "0                                                NaN         12006   \n",
       "1  6405a1a5e7c8e7eed6d6d85069660cfa31d4d635403dfd...         11009   \n",
       "2  f56a202e1bbdca986d3a2e70cb760ab099740c4561b3c7...         15004   \n",
       "3                                                NaN         14001   \n",
       "4  da6a24644efbdd4a773b59f6c15fbc1bc0550d10a4e4a6...         11003   \n",
       "5                                                NaN         35046   \n",
       "6  0c1be82404797e3b741177ea0ec45c1e4584b501a40f0d...         12006   \n",
       "7                                                NaN         12006   \n",
       "8  58a4f4c1783f49049b2d1ce5b86f7bfdebd8176eb15878...         11007   \n",
       "9  6acaacee2b0e2ee082775abd4b94d67646b42877849f86...         48001   \n",
       "\n",
       "  subclass_id    item_id item_transaction_units  item_transaction_amt  \n",
       "0           1  810216162                      1   00000000000001.0000  \n",
       "1           2  110106912                      2   00000000000001.6000  \n",
       "2           2  810297158                      1   00000000000001.9000  \n",
       "3           2  810155132                      1   00000000000004.0000  \n",
       "4           4  810135555                      1   00000000000000.8000  \n",
       "5           6  810385788                      1   00000000000005.0000  \n",
       "6          14  810333370                      1   00000000000003.5000  \n",
       "7           1  810216162                      2   00000000000002.0000  \n",
       "8           6  810363610                      3   00000000000002.2500  \n",
       "9           1  810348406                      1   00000000000005.0000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_table(Q1_files_2019[1],nrows=10,dtype=str,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_agg_df(file_list):\n",
    "    \n",
    "    i_counter=0\n",
    "\n",
    "    sales_agg_df=pd.DataFrame()\n",
    "    ids_by_week_store=pd.DataFrame()\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df=pd.read_table(file_path,dtype=str,sep=\"|\",usecols=None,nrows=None)\n",
    "        df=df[df['location_id']!=\"6990\"]\n",
    "        # print(df.shape)\n",
    "        df=df.drop_duplicates()\n",
    "        # print(df.shape)\n",
    "        \n",
    "        \n",
    "        if \"subclass_transaction_amt\" in df.columns.tolist():\n",
    "        \n",
    "            df['transaction_dt']=df['transaction_dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n",
    "            df['subclass_transaction_amt']=df['subclass_transaction_amt'].astype(float)\n",
    "\n",
    "            df['rewards_label']=np.where(pd.isnull(df['customer_id_hashed']),\"Non_Rewards\",\"Rewards\")\n",
    "\n",
    "            date_max=df['transaction_dt'].max()\n",
    "            date_min=df['transaction_dt'].min()\n",
    "\n",
    "            # print(i_counter,date_max,date_min,datetime.datetime.now())\n",
    "\n",
    "            if ((date_max-date_min).days==6) & (date_max.weekday()==5):\n",
    "                df['week_end_dt']=date_max\n",
    "                df_agg_sales=df.groupby(['location_id','week_end_dt','rewards_label'])['subclass_transaction_amt'].sum().to_frame().reset_index()\n",
    "                df_agg_sales=df_agg_sales.rename(columns={\"subclass_transaction_amt\":\"sales\"})\n",
    "\n",
    "                df=df[df['subclass_transaction_amt']>0]\n",
    "                df_agg_trans=df[['location_id','transaction_dt','week_end_dt','transaction_id','customer_id_hashed','rewards_label']].drop_duplicates()\n",
    "                df_agg_trans['transactions']=1\n",
    "                df_agg_trans=df_agg_trans.groupby(['location_id','week_end_dt','rewards_label'])['transactions'].sum().to_frame().reset_index()\n",
    "\n",
    "                df_agg_sales=pd.merge(df_agg_sales,df_agg_trans,on=['location_id','week_end_dt','rewards_label'],how=\"outer\")\n",
    "\n",
    "                df=df[df['rewards_label']==\"Rewards\"]\n",
    "                df=df[['location_id','week_end_dt','customer_id_hashed']].drop_duplicates()\n",
    "\n",
    "            else:\n",
    "                print(\"Date in the data not 7 days\",file_path)\n",
    "                df=pd.DataFrame()\n",
    "                df_agg_sales=pd.DataFrame()\n",
    "\n",
    "            sales_agg_df=sales_agg_df.append(df_agg_sales)\n",
    "            ids_by_week_store=ids_by_week_store.append(df)\n",
    "            i_counter+=1\n",
    "            \n",
    "        elif \"item_transaction_amt\" in df.columns.tolist():\n",
    "        \n",
    "            df['transaction_dt']=df['transaction_dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").date())\n",
    "            df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "\n",
    "            df['rewards_label']=np.where(pd.isnull(df['customer_id_hashed']),\"Non_Rewards\",\"Rewards\")\n",
    "\n",
    "            date_max=df['transaction_dt'].max()\n",
    "            date_min=df['transaction_dt'].min()\n",
    "\n",
    "            # print(i_counter,date_max,date_min,datetime.datetime.now())\n",
    "\n",
    "            if ((date_max-date_min).days==6) & (date_max.weekday()==5):\n",
    "                df['week_end_dt']=date_max\n",
    "                df_agg_sales=df.groupby(['location_id','week_end_dt','rewards_label'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "                df_agg_sales=df_agg_sales.rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "\n",
    "                df=df[df['item_transaction_amt']>0]\n",
    "                df_agg_trans=df[['location_id','transaction_dt','week_end_dt','transaction_id','customer_id_hashed','rewards_label']].drop_duplicates()\n",
    "                df_agg_trans['transactions']=1\n",
    "                df_agg_trans=df_agg_trans.groupby(['location_id','week_end_dt','rewards_label'])['transactions'].sum().to_frame().reset_index()\n",
    "\n",
    "                df_agg_sales=pd.merge(df_agg_sales,df_agg_trans,on=['location_id','week_end_dt','rewards_label'],how=\"outer\")\n",
    "\n",
    "                df=df[df['rewards_label']==\"Rewards\"]\n",
    "                df=df[['location_id','week_end_dt','customer_id_hashed']].drop_duplicates()\n",
    "\n",
    "            else:\n",
    "                print(\"Date in the data not 7 days\",file_path)\n",
    "                df=pd.DataFrame()\n",
    "                df_agg_sales=pd.DataFrame()\n",
    "\n",
    "            sales_agg_df=sales_agg_df.append(df_agg_sales)\n",
    "            ids_by_week_store=ids_by_week_store.append(df)\n",
    "            i_counter+=1\n",
    "        else:\n",
    "            print(\"Check\",file_path)\n",
    "\n",
    "    return sales_agg_df,ids_by_week_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_agg_2018, ids_by_week_store_2018 = load_and_agg_df(Q1_files_2018)\n",
    "\n",
    "sales_agg_2019, ids_by_week_store_2019 = load_and_agg_df(Q1_files_2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_id_2018_by_store=ids_by_week_store_2018.groupby('location_id')['customer_id_hashed'].apply(count_unique).to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"Q1_unique_id\"})\n",
    "unique_id_2019_by_store=ids_by_week_store_2019.groupby('location_id')['customer_id_hashed'].apply(count_unique).to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"Q1_unique_id\"})\n",
    "unique_id_2018_by_store['rewards_label']=\"Rewards\"\n",
    "unique_id_2019_by_store['rewards_label']=\"Rewards\"\n",
    "\n",
    "\n",
    "unique_id_2018_by_store_week=ids_by_week_store_2018.groupby(['location_id','week_end_dt'])['customer_id_hashed'].apply(count_unique).to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"Q1_unique_id\"})\n",
    "unique_id_2019_by_store_week=ids_by_week_store_2019.groupby(['location_id','week_end_dt'])['customer_id_hashed'].apply(count_unique).to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"Q1_unique_id\"})\n",
    "unique_id_2018_by_store_week['week_end_dt']=unique_id_2018_by_store_week['week_end_dt'].apply(lambda x: x.date())\n",
    "unique_id_2019_by_store_week['week_end_dt']=unique_id_2019_by_store_week['week_end_dt'].apply(lambda x: x.date())\n",
    "unique_id_2018_by_store_week['rewards_label']=\"Rewards\"\n",
    "unique_id_2019_by_store_week['rewards_label']=\"Rewards\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_agg_2018_with_id_by_week=pd.merge(sales_agg_2018,unique_id_2018_by_store_week,on=['location_id','week_end_dt','rewards_label'],how=\"outer\")\n",
    "sales_agg_2019_with_id_by_week=pd.merge(sales_agg_2019,unique_id_2019_by_store_week,on=['location_id','week_end_dt','rewards_label'],how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_agg_2018_with_id_by_week['location_id']=sales_agg_2018_with_id_by_week['location_id'].astype(int)\n",
    "sales_agg_2019_with_id_by_week['location_id']=sales_agg_2019_with_id_by_week['location_id'].astype(int)\n",
    "unique_id_2018_by_store['location_id']=unique_id_2018_by_store['location_id'].astype(int)\n",
    "unique_id_2019_by_store['location_id']=unique_id_2019_by_store['location_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(\"/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Post_YoY/BL_2019_Q1_post_data_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "sales_agg_2018_with_id_by_week.to_excel(writer,\"sales_by_week_2018\",index=False)\n",
    "sales_agg_2019_with_id_by_week.to_excel(writer,\"sales_by_week_2019\",index=False)\n",
    "unique_id_2018_by_store.to_excel(writer,\"ids_by_store_quarter_2018\",index=False)\n",
    "unique_id_2019_by_store.to_excel(writer,\"ids_by_store_quarter_2019\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_by_week_store_2018.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Post_YoY/BL_2019_Q1_ids_2018Q1_by_store_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "ids_by_week_store_2019.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Post_YoY/BL_2019_Q1_ids_2019Q1_by_store_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
