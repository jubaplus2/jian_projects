{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import hashlib\n",
    "import logging\n",
    "import gc\n",
    "# logging.basicConfig(filename='QC_BL_Store_Rewards_Stats'+str(datetime.datetime.now().date())+'.log', level=logging.INFO)\n",
    "\n",
    "\n",
    "samplerows=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51424"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_info=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180901-133640-935.txt\",sep=\"|\",dtype=str)\n",
    "store_info=store_info[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180801-133641-576.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180703.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20171115.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20170913.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "store_info['zip_cd']=store_info['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_stores_sales_data=pd.read_excel(\"/home/jian/BiglotsCode/outputs/Output_2018-09-01/wide_sales_date2018-09-01.xlsx\",dtype=str,sheetname=\"sales\")\n",
    "inclusion_stores_trans_data=pd.read_excel(\"/home/jian/BiglotsCode/outputs/Output_2018-09-01/wide_sales_date2018-09-01.xlsx\",dtype=str,sheetname=\"transactions\")\n",
    "\n",
    "inclusions_store_list=inclusion_stores_sales_data[inclusion_stores_sales_data['2018-09-01']!=\"0\"]['location_id'].tolist()\n",
    "\n",
    "inclusion_stores_sales_data=inclusion_stores_sales_data[inclusion_stores_sales_data['location_id'].isin(inclusions_store_list)]\n",
    "inclusion_stores_trans_data=inclusion_stores_trans_data[inclusion_stores_trans_data['location_id'].isin(inclusions_store_list)]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "Q2_Start_week_2018=datetime.date(2018,5,12)\n",
    "Q2_End_week_2018=datetime.date(2018,8,4)\n",
    "Q2_Start_week_2017=datetime.date(2017,5,13)\n",
    "Q2_End_week_2017=datetime.date(2017,8,5)\n",
    "Q2_2017_Weeks=[str(Q2_Start_week_2017+datetime.timedelta(days=7*i)) for i in range(13)]\n",
    "Q2_2018_Weeks=[str(Q2_Start_week_2018+datetime.timedelta(days=7*i)) for i in range(13)]\n",
    "\n",
    "#\n",
    "inclusion_stores_sales_data=inclusion_stores_sales_data[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "inclusion_stores_trans_data=inclusion_stores_trans_data[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "for col in Q2_2017_Weeks+Q2_2018_Weeks:\n",
    "    inclusion_stores_sales_data[col]=inclusion_stores_sales_data[col].astype(float)\n",
    "    inclusion_stores_trans_data[col]=inclusion_stores_trans_data[col].astype(float)\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_stores_trans_data['2017_Q2_Trans']=inclusion_stores_trans_data[Q2_2017_Weeks].sum(axis=1)\n",
    "inclusion_stores_trans_data['2018_Q2_Trans']=inclusion_stores_trans_data[Q2_2018_Weeks].sum(axis=1)\n",
    "inclusion_stores_sales_data['2017_Q2_Sales']=inclusion_stores_sales_data[Q2_2017_Weeks].sum(axis=1)\n",
    "inclusion_stores_sales_data['2018_Q2_Sales']=inclusion_stores_sales_data[Q2_2018_Weeks].sum(axis=1)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "Q2_Sales_Trans=pd.merge(inclusion_stores_trans_data[['location_id','2017_Q2_Trans','2018_Q2_Trans']],\n",
    "                       inclusion_stores_sales_data[['location_id','2017_Q2_Sales','2018_Q2_Sales']],\n",
    "                       on=\"location_id\",how=\"outer\")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "DMA_Zip=pd.read_excel(\"/home/jian/Docs/Geo_mapping/Zips by DMA by County16-17 nielsen.xlsx\",skiprows=1,dtype=str)\n",
    "DMA_Zip=DMA_Zip.iloc[:,[0,2]]\n",
    "DMA_Zip.columns=['zip_cd','DMA']\n",
    "DMA_Zip=DMA_Zip.drop_duplicates(['zip_cd'])\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "store_info=pd.merge(store_info,DMA_Zip,on=\"zip_cd\",how=\"left\")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_store_list_set=set(Q2_Sales_Trans['location_id'].unique().tolist())\n",
    "\n",
    "#\n",
    "\n",
    "df_1=store_info.copy()\n",
    "df_1=df_1[df_1['location_id'].isin(inclusion_store_list_set)]\n",
    "df_4=Q2_Sales_Trans.copy()\n",
    "del store_info\n",
    "df_4['Q2_Sales_YoY']=(df_4['2018_Q2_Sales']-df_4['2017_Q2_Sales'])/df_4['2017_Q2_Sales']\n",
    "df_4['Q2_Trans_YoY']=(df_4['2018_Q2_Trans']-df_4['2017_Q2_Trans'])/df_4['2017_Q2_Trans']\n",
    "df_4=df_4[['location_id','Q2_Sales_YoY','Q2_Trans_YoY','2018_Q2_Sales','2017_Q2_Sales','2018_Q2_Trans','2017_Q2_Trans']]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards_sales_from_SP_2017Q2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d852c3025b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrewards_sales_from_SP_2017Q2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transaction_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_qc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards_sales_from_SP_2017Q2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrewards_sales_from_SP_2017Q2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"1553\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_qc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_qc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewards_sales_from_SP_2017Q2' is not defined"
     ]
    }
   ],
   "source": [
    "rewards_sales_from_SP=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/From_Sp/combinedtransactions_0811.csv\",\n",
    "                                 dtype=str,nrows=samplerows,usecols=['customer_id_hashed','transaction_date','transaction_id','location_id','total_transaction_amt'])\n",
    "rewards_sales_from_SP=rewards_sales_from_SP[rewards_sales_from_SP['location_id'].isin(inclusion_store_list_set)]\n",
    "rewards_sales_from_SP['transaction_date']=rewards_sales_from_SP['transaction_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards_sales_from_SP_2017Q2=rewards_sales_from_SP[(rewards_sales_from_SP['transaction_date']>=Q2_Start_week_2017) & (rewards_sales_from_SP['transaction_date']<=Q2_End_week_2017)]\n",
    "rewards_sales_from_SP=rewards_sales_from_SP[(rewards_sales_from_SP['transaction_date']>=Q2_Start_week_2018) & (rewards_sales_from_SP['transaction_date']<=Q2_End_week_2018)]\n",
    "\n",
    "rewards_sales_from_SP=rewards_sales_from_SP.drop_duplicates()\n",
    "rewards_sales_from_SP_2017Q2=rewards_sales_from_SP_2017Q2.drop_duplicates()\n",
    "\n",
    "rewards_sales_from_SP['total_transaction_amt']=rewards_sales_from_SP['total_transaction_amt'].astype(float)\n",
    "rewards_sales_from_SP_2017Q2['total_transaction_amt']=rewards_sales_from_SP_2017Q2['total_transaction_amt'].astype(float)\n",
    "\n",
    "del rewards_sales_from_SP['transaction_id']\n",
    "del rewards_sales_from_SP_2017Q2['transaction_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QC 1553 rewards 2017 sales: 132591.0\n",
      "QC 1553 rewards 2017 days: 85\n",
      "QC 1553 rewards 2018 sales: 295542.96\n",
      "QC 1553 rewards 2018 days: 85\n",
      "QC 3 rewards 2017 sales: 265620.73\n",
      "QC 3 rewards 2017 days: 85\n",
      "QC 3 rewards 2018 sales: 413950.94\n",
      "QC 3 rewards 2018 days: 85\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'store_level_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ca685310d796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# In[19]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mfinal_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_level_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_zip_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"location_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Merge1 | \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSep_12_new_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"location_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'store_level_output' is not defined"
     ]
    }
   ],
   "source": [
    "rewards_sales_from_SP_2017Q2['transaction_date'].unique()\n",
    "df_qc=rewards_sales_from_SP_2017Q2[rewards_sales_from_SP_2017Q2['location_id']==\"1553\"]\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "print(\"QC 1553 rewards 2017 sales: \"+str(sales_1553))\n",
    "print(\"QC 1553 rewards 2017 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "df_qc=rewards_sales_from_SP[rewards_sales_from_SP['location_id']==\"1553\"]\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "print(\"QC 1553 rewards 2018 sales: \"+str(sales_1553))\n",
    "print(\"QC 1553 rewards 2018 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "\n",
    "df_qc=rewards_sales_from_SP_2017Q2[rewards_sales_from_SP_2017Q2['location_id']==\"3\"]\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "print(\"QC 3 rewards 2017 sales: \"+str(sales_1553))\n",
    "print(\"QC 3 rewards 2017 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "\n",
    "df_qc=rewards_sales_from_SP[rewards_sales_from_SP['location_id']==\"3\"]\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "print(\"QC 3 rewards 2018 sales: \"+str(sales_1553))\n",
    "print(\"QC 3 rewards 2018 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Final_Output\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "final_output=pd.merge(store_level_output,output_zip_level,on=\"location_id\",how=\"left\")\n",
    "print(\"Merge1 | \"+str(datetime.datetime.now()))\n",
    "final_output=pd.merge(final_output,Sep_12_new_request,on=\"location_id\",how=\"left\")\n",
    "print(\"Merge2 | \"+str(datetime.datetime.now()))\n",
    "final_output['Q2_rewards_AOV_2018']=final_output['Q2_rewards_sales_2018']/final_output['Q2_rewards_trans_2018']\n",
    "print(\"3 | \"+str(datetime.datetime.now()))\n",
    "final_output['Q2_rewards_trans_per_id']=final_output['Q2_rewards_trans_2018']/final_output['2018_Q2_rewards_id_shopped']\n",
    "print(\"4 | \"+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To add\n",
    "\n",
    "# 1. zip+4 rolled up to zip then ta, total HH 25-54 #Done\n",
    "# 2. Q2 rewards members count up to end of Q2 2017 #Done\n",
    "# 3. 2017 Q2 rewards all variables #Done\n",
    "# 4. rewards week by week Q2, yoy #Done\n",
    "# 5. Split the compariable stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EASI_C=pd.read_csv(\"/home/jian/Docs/EASI/2018-07-19/ZIP4_18_DATA_C2_CSV/ZIP4_18_DATA_C2.CSV\",\n",
    "                   dtype=str,skiprows=1,nrows=samplerows,usecols=['ZIP_CODE','ZIP4','HH18','PCTHH25_34','PCTHH35_44','PCTHH45_54'])\n",
    "EASI_C['HH18']=EASI_C['HH18'].astype(float)\n",
    "\n",
    "EASI_C['PCTHH25_34']=EASI_C['PCTHH25_34'].astype(float)\n",
    "EASI_C['PCTHH35_44']=EASI_C['PCTHH35_44'].astype(float)\n",
    "EASI_C['PCTHH45_54']=EASI_C['PCTHH45_54'].astype(float)\n",
    "\n",
    "EASI_C['PCTHH25_54']=EASI_C['PCTHH25_34']+EASI_C['PCTHH35_44']+EASI_C['PCTHH45_54']\n",
    "\n",
    "del EASI_C['PCTHH25_34']\n",
    "del EASI_C['PCTHH35_44']\n",
    "del EASI_C['PCTHH45_54']\n",
    "\n",
    "EASI_C['HH_25_54']=EASI_C['HH18']*EASI_C['PCTHH25_54']/100\n",
    "check_x=EASI_C['HH18'].sum()\n",
    "EASI_C=EASI_C.groupby(['ZIP_CODE'])['HH_25_54'].sum().to_frame().reset_index()\n",
    "EASI_C=EASI_C.rename(columns={\"ZIP_CODE\":\"zip_cd\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51424"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_info=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180901-133640-935.txt\",sep=\"|\",dtype=str)\n",
    "store_info=store_info[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180801-133641-576.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20180703.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20171115.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "\n",
    "store_info_2=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20170913.txt\",sep=\"|\",dtype=str)\n",
    "store_info_2=store_info_2[['location_id','city_nm','state_nm','zip_cd','open_dt']]\n",
    "store_info_2=store_info_2[~store_info_2['location_id'].isin(store_info['location_id'].tolist())]\n",
    "store_info=store_info.append(store_info_2)\n",
    "store_info['zip_cd']=store_info['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_stores_sales_data=pd.read_excel(\"/home/jian/BiglotsCode/outputs/Output_2018-09-01/wide_sales_date2018-09-01.xlsx\",dtype=str,sheetname=\"sales\")\n",
    "inclusion_stores_trans_data=pd.read_excel(\"/home/jian/BiglotsCode/outputs/Output_2018-09-01/wide_sales_date2018-09-01.xlsx\",dtype=str,sheetname=\"transactions\")\n",
    "\n",
    "inclusions_store_list=inclusion_stores_sales_data[inclusion_stores_sales_data['2018-09-01']!=\"0\"]['location_id'].tolist()\n",
    "\n",
    "inclusion_stores_sales_data=inclusion_stores_sales_data[inclusion_stores_sales_data['location_id'].isin(inclusions_store_list)]\n",
    "inclusion_stores_trans_data=inclusion_stores_trans_data[inclusion_stores_trans_data['location_id'].isin(inclusions_store_list)]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "Q2_Start_week_2018=datetime.date(2018,5,12)\n",
    "Q2_End_week_2018=datetime.date(2018,8,4)\n",
    "Q2_Start_week_2017=datetime.date(2017,5,13)\n",
    "Q2_End_week_2017=datetime.date(2017,8,5)\n",
    "Q2_2017_Weeks=[str(Q2_Start_week_2017+datetime.timedelta(days=7*i)) for i in range(13)]\n",
    "Q2_2018_Weeks=[str(Q2_Start_week_2018+datetime.timedelta(days=7*i)) for i in range(13)]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_stores_sales_data=inclusion_stores_sales_data[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "inclusion_stores_trans_data=inclusion_stores_trans_data[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "for col in Q2_2017_Weeks+Q2_2018_Weeks:\n",
    "    inclusion_stores_sales_data[col]=inclusion_stores_sales_data[col].astype(float)\n",
    "    inclusion_stores_trans_data[col]=inclusion_stores_trans_data[col].astype(float)\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_stores_trans_data['2017_Q2_Trans']=inclusion_stores_trans_data[Q2_2017_Weeks].sum(axis=1)\n",
    "inclusion_stores_trans_data['2018_Q2_Trans']=inclusion_stores_trans_data[Q2_2018_Weeks].sum(axis=1)\n",
    "inclusion_stores_sales_data['2017_Q2_Sales']=inclusion_stores_sales_data[Q2_2017_Weeks].sum(axis=1)\n",
    "inclusion_stores_sales_data['2018_Q2_Sales']=inclusion_stores_sales_data[Q2_2018_Weeks].sum(axis=1)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "Q2_Sales_Trans=pd.merge(inclusion_stores_trans_data[['location_id','2017_Q2_Trans','2018_Q2_Trans']],\n",
    "                       inclusion_stores_sales_data[['location_id','2017_Q2_Sales','2018_Q2_Sales']],\n",
    "                       on=\"location_id\",how=\"outer\")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "DMA_Zip=pd.read_excel(\"/home/jian/Docs/Geo_mapping/Zips by DMA by County16-17 nielsen.xlsx\",skiprows=1,dtype=str)\n",
    "DMA_Zip=DMA_Zip.iloc[:,[0,2]]\n",
    "DMA_Zip.columns=['zip_cd','DMA']\n",
    "DMA_Zip=DMA_Zip.drop_duplicates(['zip_cd'])\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "store_info=pd.merge(store_info,DMA_Zip,on=\"zip_cd\",how=\"left\")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "inclusion_store_list_set=set(Q2_Sales_Trans['location_id'].unique().tolist())\n",
    "\n",
    "#\n",
    "\n",
    "df_1=store_info.copy()\n",
    "df_1=df_1[df_1['location_id'].isin(inclusion_store_list_set)]\n",
    "df_4=Q2_Sales_Trans.copy()\n",
    "del store_info\n",
    "df_4['Q2_Sales_YoY']=(df_4['2018_Q2_Sales']-df_4['2017_Q2_Sales'])/df_4['2017_Q2_Sales']\n",
    "df_4['Q2_Trans_YoY']=(df_4['2018_Q2_Trans']-df_4['2017_Q2_Trans'])/df_4['2017_Q2_Trans']\n",
    "df_4=df_4[['location_id','Q2_Sales_YoY','Q2_Trans_YoY','2018_Q2_Sales','2017_Q2_Sales','2018_Q2_Trans','2017_Q2_Trans']]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>Q2_Sales_YoY</th>\n",
       "      <th>Q2_Trans_YoY</th>\n",
       "      <th>2018_Q2_Sales</th>\n",
       "      <th>2017_Q2_Sales</th>\n",
       "      <th>2018_Q2_Trans</th>\n",
       "      <th>2017_Q2_Trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.078347</td>\n",
       "      <td>-0.074313</td>\n",
       "      <td>1125645.06</td>\n",
       "      <td>1043861.37</td>\n",
       "      <td>32051.0</td>\n",
       "      <td>34624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.109054</td>\n",
       "      <td>-0.106154</td>\n",
       "      <td>781138.33</td>\n",
       "      <td>876751.35</td>\n",
       "      <td>24865.0</td>\n",
       "      <td>27818.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id  Q2_Sales_YoY  Q2_Trans_YoY  2018_Q2_Sales  2017_Q2_Sales  \\\n",
       "0           1      0.078347     -0.074313     1125645.06     1043861.37   \n",
       "1           3     -0.109054     -0.106154      781138.33      876751.35   \n",
       "\n",
       "   2018_Q2_Trans  2017_Q2_Trans  \n",
       "0        32051.0        34624.0  \n",
       "1        24865.0        27818.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folderpath = '/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/' # usecols=[0,1,5],\n",
    "selected_columns = ['customer_id_hashed','sign_up_location','customer_zip_code','sign_up_date']\n",
    "dfiddetail = pd.read_table(folderpath+\"/MediaStormCustTot-hashed-email.txt\",\n",
    "                       header=None,nrows = samplerows,sep = ',',dtype = str,usecols=[0,2,4,5])\n",
    "dfiddetail.columns=['customer_id','sign_up_date','sign_up_location','customer_zip_code']\n",
    "dfiddetail['customer_id_hashed'] = dfiddetail['customer_id'].apply(lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest())\n",
    "dfiddetail=dfiddetail[selected_columns]\n",
    "dfiddetail=dfiddetail[dfiddetail['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'MediaStormCustomerTransactionTotals_2018-01-09_2018-03-31.txt',\n",
    "                          nrows = samplerows,sep = ',',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'Existing Reward Member Master as of 2018-06-05.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'New Reward Member Master as of 2018-06-05.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'New Reward Member Master as of 2018-07-03.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'MediaStormMasterBiWeekly20180717-132337-377.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'MediaStormMasterBiWeekly20180731-130714-098.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "\n",
    "#\n",
    "dfiddetail2 = pd.read_csv(folderpath+'MediaStormMasterBiWeekly20180814-130703-491.txt',\n",
    "                          nrows = samplerows,sep = '|',usecols=[0,2,4,5],dtype = str)\n",
    "dfiddetail2=dfiddetail2[selected_columns]\n",
    "dfiddetail2['sign_up_location']=dfiddetail2['sign_up_location'].fillna(\"nan\")\n",
    "dfiddetail2=dfiddetail2[dfiddetail2['sign_up_location'].isin(inclusion_store_list_set)]\n",
    "dfiddetail = dfiddetail.append(dfiddetail2)\n",
    "del dfiddetail2\n",
    "#\n",
    "dfiddetail['customer_zip_code']=dfiddetail['customer_zip_code'].astype(str)\n",
    "dfiddetail['customer_zip_code']=dfiddetail['customer_zip_code'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "dfiddetail['customer_zip_code']=dfiddetail['customer_zip_code'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "dfiddetail['customer_zip_code']=dfiddetail['customer_zip_code'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "dfiddetail['zip_cd']=dfiddetail['customer_zip_code'].apply(lambda x: str(x).split(\"-\")[0].zfill(5))\n",
    "del dfiddetail['customer_zip_code']\n",
    "\n",
    "dfiddetail=dfiddetail[dfiddetail['sign_up_date'].apply(lambda x: len(x))==10]\n",
    "\n",
    "dfiddetail['sign_up_date']=dfiddetail['sign_up_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "dfiddetail_2017_Q2=dfiddetail[dfiddetail['sign_up_date']<=Q2_End_week_2017]\n",
    "\n",
    "\n",
    "dfiddetail=dfiddetail.sort_values(['sign_up_date'],ascending=False)\n",
    "dfiddetail_2017_Q2=dfiddetail_2017_Q2.sort_values(['sign_up_date'],ascending=False)\n",
    "\n",
    "\n",
    "dfiddetail=dfiddetail[['customer_id_hashed','sign_up_location','zip_cd','sign_up_date']].drop_duplicates('customer_id_hashed')\n",
    "dfiddetail_2017_Q2=dfiddetail_2017_Q2[['customer_id_hashed','sign_up_location','zip_cd','sign_up_date']].drop_duplicates('customer_id_hashed')\n",
    "\n",
    "\n",
    "logging.info(\"Finished read of singing up data, with rows of \"+str(dfiddetail.shape[0]))\n",
    "\n",
    "dfiddetail=dfiddetail[dfiddetail['zip_cd'].apply(lambda x: len(x)).isin([5,9])]\n",
    "dfiddetail['zip_cd']=dfiddetail['zip_cd'].apply(lambda x: x[:5])\n",
    "\n",
    "dfiddetail_2017_Q2=dfiddetail_2017_Q2[dfiddetail_2017_Q2['zip_cd'].apply(lambda x: len(x)).isin([5,9])]\n",
    "dfiddetail_2017_Q2['zip_cd']=dfiddetail_2017_Q2['zip_cd'].apply(lambda x: x[:5])\n",
    "\n",
    "#\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>id_counts_signed_up_Q2End_2018</th>\n",
       "      <th>id_counts_signed_up_Q2End_2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11656</td>\n",
       "      <td>9174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>7400</td>\n",
       "      <td>5566.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id  id_counts_signed_up_Q2End_2018  id_counts_signed_up_Q2End_2017\n",
       "0           1                           11656                          9174.0\n",
       "1        1001                            7400                          5566.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sign_up_id_by_zip=dfiddetail.groupby(['zip_cd'])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "sign_up_id_by_store=dfiddetail.groupby(['sign_up_location'])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "sign_up_id_by_store_2017_Q2=dfiddetail_2017_Q2.groupby(['sign_up_location'])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "sign_up_id_by_store_2017_Q2=sign_up_id_by_store_2017_Q2.rename(columns={\"sign_up_location\":\"location_id\",\"customer_id_hashed\":\"id_counts_signed_up_Q2End_2017\"})\n",
    "df_2=sign_up_id_by_store.copy()\n",
    "\n",
    "df_2=df_2.rename(columns={\"sign_up_location\":\"location_id\",\"customer_id_hashed\":\"id_counts_signed_up_Q2End_2018\"})\n",
    "\n",
    "dfiddetail_2017_Q2=dfiddetail_2017_Q2.sort_values(['sign_up_date'],ascending=False)\n",
    "dfiddetail_2017_Q2=dfiddetail_2017_Q2[['customer_id_hashed','sign_up_location','zip_cd','sign_up_date']].drop_duplicates('customer_id_hashed')\n",
    "sign_up_id_by_store_2017_Q2=dfiddetail_2017_Q2.groupby(['sign_up_location'])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "sign_up_id_by_store_2017_Q2=sign_up_id_by_store_2017_Q2.rename(columns={\"sign_up_location\":\"location_id\",\"customer_id_hashed\":\"id_counts_signed_up_Q2End_2017\"})\n",
    "\n",
    "df_2=pd.merge(df_2,sign_up_id_by_store_2017_Q2,on=\"location_id\",how=\"left\")\n",
    "del dfiddetail_2017_Q2\n",
    "del sign_up_id_by_store\n",
    "del sign_up_id_by_store_2017_Q2\n",
    "\n",
    "gc.collect()\n",
    "df_2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15259"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta_by_zip=pd.read_excel(\"/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/BL_Zips in new TA (TA level)_JL_20180330.xlsx\",\n",
    "                       dtype=str,usecols=['location_id','zip_cd','revenue_flag','TA_of_zip','TA_of_store'])\n",
    "TA_Zips=ta_by_zip[['zip_cd','TA_of_zip']].drop_duplicates()\n",
    "TA_Store=ta_by_zip[['location_id','TA_of_store']].drop_duplicates()\n",
    "TA_Store=TA_Store[TA_Store['location_id'].isin(inclusion_store_list_set)]\n",
    "\n",
    "stores_not_in_TA=[x for x in inclusion_store_list_set if x not in ta_by_zip['location_id'].tolist()]\n",
    "zips_for_new_store=df_1[df_1['location_id'].isin(stores_not_in_TA)]['zip_cd'].unique().tolist()\n",
    "# 8 new stores not in TA, 7 of the 8 zips are found in TA and allocated to the TA\n",
    "# 4675|02719 missing TA info, so no population\n",
    "TA_new_store=TA_Zips[TA_Zips['zip_cd'].isin(zips_for_new_store)]\n",
    "\n",
    "given_TA_7_New_store=df_1[df_1['location_id'].isin(stores_not_in_TA)]\n",
    "given_TA_7_New_store=pd.merge(given_TA_7_New_store,TA_Zips,on=\"zip_cd\",how=\"left\")\n",
    "given_TA_7_New_store=given_TA_7_New_store[['location_id','TA_of_zip']].rename(columns={\"TA_of_zip\":\"TA_of_store\"})\n",
    "TA_Store=TA_Store.append(given_TA_7_New_store)\n",
    "\n",
    "demo_F_25_54=pd.read_csv(\"/home/jian/Docs/Household_and_Population/2016/Demo_Dataset_2018EASI.csv\",\n",
    "                         dtype=str,usecols=['ZIP_CODE','Estimate; Female: - 25 to 29 years','Estimate; Female: - 30 to 34 years',\n",
    "                                                    'Estimate; Female: - 35 to 39 years','Estimate; Female: - 40 to 44 years',\n",
    "                                                     'Estimate; Female: - 45 to 49 years','Estimate; Female: - 50 to 54 years'])\n",
    "for col in demo_F_25_54.columns.tolist()[1:]:\n",
    "    demo_F_25_54[col]=demo_F_25_54[col].astype(float)\n",
    "demo_F_25_54['ZIP_CODE']=demo_F_25_54['ZIP_CODE'].apply(lambda x: x.zfill(5))\n",
    "demo_F_25_54['Pop_F_25_54']=demo_F_25_54[demo_F_25_54.columns.tolist()[1:]].sum(axis=1)\n",
    "demo_F_25_54=demo_F_25_54.rename(columns={\"ZIP_CODE\":\"zip_cd\"})\n",
    "demo_F_25_54=demo_F_25_54[['zip_cd','Pop_F_25_54']]\n",
    "\n",
    "TA_Zips_POP=pd.merge(TA_Zips,demo_F_25_54,on=\"zip_cd\",how=\"left\")\n",
    "TA_POP=TA_Zips_POP.groupby(['TA_of_zip'])['Pop_F_25_54'].sum().to_frame().reset_index()\n",
    "TA_POP=TA_POP.rename(columns={\"TA_of_zip\":\"TA\"})\n",
    "TA_Store=TA_Store.rename(columns={\"TA_of_store\":\"TA\"})\n",
    "df_3=pd.merge(TA_Store,TA_POP,on=\"TA\",how=\"left\")\n",
    "\n",
    "TA_Zips_HH_EASI=pd.merge(TA_Zips,EASI_C,on=\"zip_cd\",how=\"left\")\n",
    "TA_HH_EASI=TA_Zips_HH_EASI.groupby(['TA_of_zip'])['HH_25_54'].sum().to_frame().reset_index()\n",
    "TA_HH_EASI=TA_HH_EASI.rename(columns={\"TA_of_zip\":\"TA\"})\n",
    "\n",
    "df_3=pd.merge(df_3,TA_HH_EASI,on=\"TA\",how=\"left\")\n",
    "\n",
    "del EASI_C\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards_sales_from_SP=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/From_Sp/combinedtransactions_0811.csv\",\n",
    "                                 dtype=str,nrows=samplerows,usecols=['customer_id_hashed','transaction_date','transaction_id','location_id','total_transaction_amt'])\n",
    "rewards_sales_from_SP=rewards_sales_from_SP[rewards_sales_from_SP['location_id'].isin(inclusion_store_list_set)]\n",
    "rewards_sales_from_SP['transaction_date']=rewards_sales_from_SP['transaction_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "rewards_sales_from_SP_2017Q2=rewards_sales_from_SP[(rewards_sales_from_SP['transaction_date']>=Q2_Start_week_2017) & (rewards_sales_from_SP['transaction_date']<=Q2_End_week_2017)]\n",
    "rewards_sales_from_SP=rewards_sales_from_SP[(rewards_sales_from_SP['transaction_date']>=Q2_Start_week_2018) & (rewards_sales_from_SP['transaction_date']<=Q2_End_week_2018)]\n",
    "\n",
    "rewards_sales_from_SP=rewards_sales_from_SP.drop_duplicates()\n",
    "rewards_sales_from_SP_2017Q2=rewards_sales_from_SP_2017Q2.drop_duplicates()\n",
    "\n",
    "rewards_sales_from_SP['total_transaction_amt']=rewards_sales_from_SP['total_transaction_amt'].astype(float)\n",
    "rewards_sales_from_SP_2017Q2['total_transaction_amt']=rewards_sales_from_SP_2017Q2['total_transaction_amt'].astype(float)\n",
    "\n",
    "del rewards_sales_from_SP['transaction_id']\n",
    "del rewards_sales_from_SP_2017Q2['transaction_id']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def add_week_end_date(x):\n",
    "    weekday_num=x.weekday()\n",
    "    if weekday_num==6:\n",
    "        y=x+datetime.timedelta(days=6)\n",
    "    else:\n",
    "        y=x+datetime.timedelta(days=5-weekday_num)\n",
    "    return y\n",
    "\n",
    "rewards_by_store_Q2_sales=rewards_sales_from_SP.groupby(['location_id'])['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "rewards_by_store_Q2_sales=rewards_by_store_Q2_sales.rename(columns={\"total_transaction_amt\":\"Q2_rewards_sales_2018\"})\n",
    "rewards_by_store_Q2_trans=rewards_sales_from_SP.groupby(['location_id'])['total_transaction_amt'].count().to_frame().reset_index()\n",
    "rewards_by_store_Q2_trans=rewards_by_store_Q2_trans.rename(columns={\"total_transaction_amt\":\"Q2_rewards_trans_2018\"})\n",
    "\n",
    "rewards_by_store_Q2_sales_2017Q2=rewards_sales_from_SP_2017Q2.groupby(['location_id'])['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "rewards_by_store_Q2_sales_2017Q2=rewards_by_store_Q2_sales_2017Q2.rename(columns={\"total_transaction_amt\":\"Q2_rewards_sales_2017\"})\n",
    "rewards_by_store_Q2_trans_2017Q2=rewards_sales_from_SP_2017Q2.groupby(['location_id'])['total_transaction_amt'].count().to_frame().reset_index()\n",
    "rewards_by_store_Q2_trans_2017Q2=rewards_by_store_Q2_trans_2017Q2.rename(columns={\"total_transaction_amt\":\"Q2_rewards_trans_2017\"})\n",
    "\n",
    "df_5=pd.merge(rewards_by_store_Q2_sales,rewards_by_store_Q2_trans,on=\"location_id\",how=\"outer\")\n",
    "df_5['Q2_rewards_sales_share_2018']=np.nan\n",
    "df_5['Q2_rewards_trans_share_2018']=np.nan\n",
    "df_5['Rewards_AOV_2018']=df_5['Q2_rewards_sales_2018']/df_5['Q2_rewards_trans_2018']\n",
    "\n",
    "df_5_2017=pd.merge(rewards_by_store_Q2_sales_2017Q2,rewards_by_store_Q2_trans_2017Q2,on=\"location_id\",how=\"outer\")\n",
    "df_5_2017['Q2_rewards_sales_share_2017']=np.nan\n",
    "df_5_2017['Q2_rewards_trans_share_2017']=np.nan\n",
    "df_5_2017['Rewards_AOV_2017']=df_5_2017['Q2_rewards_sales_2017']/df_5_2017['Q2_rewards_trans_2017']\n",
    "\n",
    "df_5=pd.merge(df_5,df_5_2017,on=\"location_id\",how=\"left\")\n",
    "df_5.head(2)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_7=inclusion_stores_sales_data[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "for col in Q2_2018_Weeks:\n",
    "    week_2018=col\n",
    "    week_2017=str(datetime.datetime.strptime(col,\"%Y-%m-%d\").date()-datetime.timedelta(days=52*7))\n",
    "    df_7[\"YoY_\"+col]=np.round((df_7[week_2018]-df_7[week_2017])/df_7[week_2017],6)\n",
    "df_7=df_7[[\"location_id\"]+[\"YoY_\"+x for x in Q2_2018_Weeks]]\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "store_level_output=pd.merge(df_1,df_2,on=\"location_id\",how=\"left\")\n",
    "store_level_output=pd.merge(store_level_output,df_3,on=\"location_id\",how=\"left\")\n",
    "store_level_output=pd.merge(store_level_output,df_4,on=\"location_id\",how=\"left\")\n",
    "store_level_output=pd.merge(store_level_output,df_5,on=\"location_id\",how=\"left\")\n",
    "store_level_output=pd.merge(store_level_output,df_7,on=\"location_id\",how=\"left\")\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "store_level_output['Q2_rewards_sales_share_2018']=store_level_output['Q2_rewards_sales_2018']/store_level_output['2018_Q2_Sales']\n",
    "store_level_output['Q2_rewards_trans_share_2018']=store_level_output['Q2_rewards_trans_2018']/store_level_output['2018_Q2_Trans']\n",
    "\n",
    "store_level_output['Q2_rewards_sales_share_2017']=store_level_output['Q2_rewards_sales_2017']/store_level_output['2017_Q2_Sales']\n",
    "store_level_output['Q2_rewards_trans_share_2017']=store_level_output['Q2_rewards_trans_2017']/store_level_output['2017_Q2_Trans']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New from YG request\n",
    "# Week by week of rewards members YoY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_k=0\n",
    "rewards_week_by_week=store_level_output[['location_id']]\n",
    "for week in Q2_2017_Weeks:\n",
    "    week_end=datetime.datetime.strptime(week,\"%Y-%m-%d\").date()\n",
    "    week_start=week_end-datetime.timedelta(days=6)\n",
    "    df=rewards_sales_from_SP_2017Q2[(rewards_sales_from_SP_2017Q2['transaction_date']>=week_start) &\\\n",
    "                                   (rewards_sales_from_SP_2017Q2['transaction_date']<=week_end)]\n",
    "    df=df.groupby(['location_id'])['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.rename(columns={\"total_transaction_amt\":\"Rewards_Sales_\"+week})\n",
    "    rewards_week_by_week=pd.merge(rewards_week_by_week,df,on=\"location_id\",how=\"left\")\n",
    "    \n",
    "    count_k+=1\n",
    "    print(count_k,week,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_k=0\n",
    "for week in Q2_2018_Weeks:\n",
    "    week_end=datetime.datetime.strptime(week,\"%Y-%m-%d\").date()\n",
    "    week_start=week_end-datetime.timedelta(days=6)\n",
    "    df=rewards_sales_from_SP[(rewards_sales_from_SP['transaction_date']>=week_start) &\\\n",
    "                                   (rewards_sales_from_SP['transaction_date']<=week_end)]\n",
    "    df=df.groupby(['location_id'])['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.rename(columns={\"total_transaction_amt\":\"Rewards_Sales_\"+week})\n",
    "    rewards_week_by_week=pd.merge(rewards_week_by_week,df,on=\"location_id\",how=\"left\")\n",
    "    \n",
    "    count_k+=1\n",
    "    print(count_k,week,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_YoY=[\"location_id\"]\n",
    "for week in Q2_2018_Weeks:\n",
    "    col_2018=\"Rewards_Sales_\"+week\n",
    "    col_2017=\"Rewards_Sales_\"+str(datetime.datetime.strptime(week,\"%Y-%m-%d\").date()-datetime.timedelta(days=52*7))\n",
    "    new_col=\"Rewards_Sales_YoY_\"+week\n",
    "    rewards_week_by_week[new_col]=(rewards_week_by_week[col_2018]-rewards_week_by_week[col_2017])/rewards_week_by_week[col_2017]\n",
    "    selected_YoY=selected_YoY+[new_col]\n",
    "    \n",
    "df_8=rewards_week_by_week[selected_YoY] #df_8 not used, using the whole rewards_week_by_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_level_output=pd.merge(store_level_output,rewards_week_by_week,on=\"location_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip level\n",
    "del dfiddetail['sign_up_date']\n",
    "rewards_sales_from_SP=pd.merge(rewards_sales_from_SP,dfiddetail,on=\"customer_id_hashed\",how=\"left\")\n",
    "rewards_sales_from_SP['zip_cd']=rewards_sales_from_SP['zip_cd'].fillna(\"zip_missing\")\n",
    "\n",
    "Store_level_PST=pd.read_csv(\"/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/sales_by_zip (Store level).csv\",dtype=str)\n",
    "Store_level_PST=Store_level_PST[Store_level_PST['location_id'].isin(inclusion_store_list_set)]\n",
    "\n",
    "Store_level_P_Zips=Store_level_PST[Store_level_PST['revenue_flag']==\"P\"].groupby(['location_id'])['zip'].apply(set).to_frame().reset_index()\n",
    "Store_level_S_Zips=Store_level_PST[Store_level_PST['revenue_flag']==\"S\"].groupby(['location_id'])['zip'].apply(set).to_frame().reset_index()\n",
    "Store_level_T_Zips=Store_level_PST[Store_level_PST['revenue_flag']==\"T\"].groupby(['location_id'])['zip'].apply(set).to_frame().reset_index()\n",
    "\n",
    "Store_level_P_Zips_Dict=Store_level_P_Zips.set_index(['location_id']).to_dict()['zip']\n",
    "Store_level_S_Zips_Dict=Store_level_S_Zips.set_index(['location_id']).to_dict()['zip']\n",
    "Store_level_T_Zips_Dict=Store_level_T_Zips.set_index(['location_id']).to_dict()['zip']\n",
    "\n",
    "store_list_with_PST=Store_level_PST['location_id'].unique().tolist()\n",
    "\n",
    "\n",
    "df_SP_July_Decile=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/Segment_Movement_analysis/Data_From_Sp/df_crm_finalscore_0714data.csv\",\n",
    "                              dtype=str,usecols=['customer_id_hashed','frmindex','zipcodegroup','customer_zip_code'])\n",
    "df_SP_July_Decile=df_SP_July_Decile.drop_duplicates(['customer_id_hashed'])\n",
    "decile_list=df_SP_July_Decile['frmindex'].unique().tolist()\n",
    "len(df_SP_July_Decile['customer_id_hashed'].unique())\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "dfiddetail=pd.merge(dfiddetail,df_SP_July_Decile[['customer_id_hashed','frmindex']],on=\"customer_id_hashed\",how=\"left\")\n",
    "dfiddetail_id_count_by_signup_zip_decile=dfiddetail.groupby(['sign_up_location','zip_cd','frmindex'])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "dfiddetail_id_count_by_signup_zip_decile=dfiddetail_id_count_by_signup_zip_decile.rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "rewards_sales_by_store=rewards_sales_from_SP.groupby('location_id')['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "rewards_sales_by_store_dict=rewards_sales_by_store.set_index(['location_id']).to_dict()['total_transaction_amt']\n",
    "\n",
    "rewards_sales_by_customer_zip_store=rewards_sales_from_SP.groupby(['location_id','zip_cd'])['total_transaction_amt'].sum().to_frame().reset_index()\n",
    "rewards_trans_by_customer_zip_store=rewards_sales_from_SP.groupby(['location_id','zip_cd'])['total_transaction_amt'].count().to_frame().reset_index()\n",
    "\n",
    "sign_up_id_by_store_zip=dfiddetail.groupby(['sign_up_location','zip_cd'])['customer_id_hashed'].count().to_frame().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_zip_level=pd.DataFrame()\n",
    "k=0\n",
    "for store in store_list_with_PST:\n",
    "    total_rewards_sales_store=rewards_sales_by_store_dict[store]\n",
    "    store_rewards_sales_by_customer_zip_store=rewards_sales_by_customer_zip_store[rewards_sales_by_customer_zip_store['location_id']==store]\n",
    "    store_rewards_trans_by_customer_zip_store=rewards_trans_by_customer_zip_store[rewards_trans_by_customer_zip_store['location_id']==store]\n",
    "    store_dfiddetail_id_count_by_signup_zip_decile=dfiddetail_id_count_by_signup_zip_decile[dfiddetail_id_count_by_signup_zip_decile['sign_up_location']==store]\n",
    "    store_sign_up_id_by_store_zip=sign_up_id_by_store_zip[sign_up_id_by_store_zip['sign_up_location']==store]\n",
    "    \n",
    "    for label in [\"P\",\"S\",\"T\"]:\n",
    "        zips_store_label=locals()[\"Store_level_\"+label+\"_Zips_Dict\"][store]\n",
    "        \n",
    "        id_count_store_label_all_zips=sign_up_id_by_zip[sign_up_id_by_zip['zip_cd'].isin(zips_store_label)]['customer_id_hashed'].sum()\n",
    "        id_count_store_label_store_only=store_sign_up_id_by_store_zip[store_sign_up_id_by_store_zip['zip_cd'].isin(zips_store_label)]['customer_id_hashed'].sum()\n",
    "        \n",
    "        sales_store_label=store_rewards_sales_by_customer_zip_store[store_rewards_sales_by_customer_zip_store['zip_cd'].isin(zips_store_label)]['total_transaction_amt'].sum()\n",
    "        trans_store_label=store_rewards_trans_by_customer_zip_store[store_rewards_trans_by_customer_zip_store['zip_cd'].isin(zips_store_label)]['total_transaction_amt'].sum()\n",
    "        \n",
    "        AOV_store_label=sales_store_label/trans_store_label\n",
    "        sales_share_label=sales_store_label/total_rewards_sales_store\n",
    "        \n",
    "        trans_to_id=trans_store_label/id_count_store_label_store_only\n",
    "        \n",
    "        for D_decile in decile_list:\n",
    "            locals()[D_decile+\"_store_label\"]=store_dfiddetail_id_count_by_signup_zip_decile[(store_dfiddetail_id_count_by_signup_zip_decile['frmindex']==D_decile) &                                                                                             (store_dfiddetail_id_count_by_signup_zip_decile['zip_cd'].isin(zips_store_label))]['id_count'].sum()\n",
    "            \n",
    "        locals()['df_app_'+label]=pd.DataFrame({\"location_id\":store,label+\"_id_counts\":id_count_store_label_all_zips,\n",
    "                                                label+\"_id_counts_singed_store\":id_count_store_label_store_only,label+\"_sales_share\":sales_share_label,\n",
    "                                                label+\"_AOV\":AOV_store_label,label+\"_trans_per_id_avg\":trans_to_id,\n",
    "                                                label+\"_D1\":locals()[\"D01_store_label\"],label+\"_D2\":locals()[\"D02_store_label\"],\n",
    "                                                label+\"_D3\":locals()[\"D03_store_label\"],label+\"_D4\":locals()[\"D04_store_label\"],\n",
    "                                                label+\"_D5\":locals()[\"D05_store_label\"],label+\"_D6\":locals()[\"D06_store_label\"],\n",
    "                                                label+\"_D7\":locals()[\"D07_store_label\"],label+\"_D8\":locals()[\"D08_store_label\"],\n",
    "                                                label+\"_D9\":locals()[\"D09_store_label\"],label+\"_D10\":locals()[\"D10_store_label\"],\n",
    "                                               },index=[store])\n",
    "    df_app_3_lable=pd.merge(df_app_P,df_app_S,on=\"location_id\",how=\"left\")\n",
    "    df_app_3_lable=pd.merge(df_app_3_lable,df_app_T,on=\"location_id\",how=\"left\")\n",
    "    df_app_3_lable=df_app_3_lable[['location_id','P_id_counts','P_id_counts_singed_store','P_sales_share','P_AOV','P_trans_per_id_avg','P_D1','P_D2','P_D3','P_D4','P_D5','P_D6','P_D7','P_D8','P_D9','P_D10',\n",
    "                                   'S_id_counts','S_id_counts_singed_store','S_sales_share','S_AOV','S_trans_per_id_avg','S_D1','S_D2','S_D3','S_D4','S_D5','S_D6','S_D7','S_D8','S_D9','S_D10',\n",
    "                                   'T_id_counts','T_id_counts_singed_store','T_sales_share','T_AOV','T_trans_per_id_avg','T_D1','T_D2','T_D3','T_D4','T_D5','T_D6','T_D7','T_D8','T_D9','T_D10']]\n",
    "    output_zip_level=output_zip_level.append(df_app_3_lable)\n",
    "    k+=1\n",
    "    if k %100==1:\n",
    "        print(store,datetime.datetime.now())\n",
    "        logging.info(str(k)+\"|\"+store+\"|\"+str(datetime.datetime.now()))\n",
    "\n",
    "    if k %100==23:\n",
    "        print(store,datetime.datetime.now())\n",
    "        logging.info(str(k)+\"|\"+store+\"|\"+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All_Rewards_regardless_of_Zips_New_Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_id_count(x):\n",
    "    y=len(set(x))\n",
    "    return y\n",
    "    \n",
    "Sep_12_new_request=rewards_sales_from_SP.groupby(['location_id'])['customer_id_hashed'].apply(unique_id_count).to_frame().reset_index()\n",
    "Sep_12_new_request=Sep_12_new_request.rename(columns={\"customer_id_hashed\":\"2018_Q2_rewards_id_shopped\"})\n",
    "\n",
    "Sep_12_new_request_2017=rewards_sales_from_SP_2017Q2.groupby(['location_id'])['customer_id_hashed'].apply(unique_id_count).to_frame().reset_index()\n",
    "Sep_12_new_request_2017=Sep_12_new_request_2017.rename(columns={\"customer_id_hashed\":\"2017_Q2_rewards_id_shopped\"})\n",
    "\n",
    "Sep_12_new_request=pd.merge(Sep_12_new_request,Sep_12_new_request_2017,on=\"location_id\",how=\"left\")\n",
    "Sep_12_new_request['Q2_rewards_trans_per_id']=np.nan\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "rewards_sales_from_SP_Decile=rewards_sales_from_SP[['customer_id_hashed','location_id']].drop_duplicates()\n",
    "sp_decile=df_SP_July_Decile[['customer_id_hashed','frmindex']]\n",
    "rewards_sales_from_SP_Decile=pd.merge(rewards_sales_from_SP_Decile,sp_decile,on=\"customer_id_hashed\",how=\"left\")\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "count_k=0\n",
    "Sep_12_new_request_Decile=pd.DataFrame()\n",
    "for store,group in rewards_sales_from_SP_Decile.groupby(['location_id']):\n",
    "    for i in range(10):\n",
    "        frmindex_str=\"D\"+str(i+1).zfill(2)\n",
    "        locals()[frmindex_str]=len(group[group['frmindex']==frmindex_str])\n",
    "    df_app=pd.DataFrame({\"location_id\":store,\"D01_July\":D01,\"D02_July\":D02,\"D03_July\":D03,\"D04_July\":D04,\"D05_July\":D05,\n",
    "                        \"D06_July\":D06,\"D07_July\":D07,\"D08_July\":D08,\"D09_July\":D09,\"D10_July\":D10},index=[store])\n",
    "    Sep_12_new_request_Decile=Sep_12_new_request_Decile.append(df_app)\n",
    "    count_k+=1\n",
    "    if count_k %100==1:\n",
    "        print(count_k,store,datetime.datetime.now())\n",
    "        logging.info(str(count_k)+\"|\"+store+\"|\"+str(datetime.datetime.now()))\n",
    "\n",
    "    if count_k %100==23:\n",
    "        print(count_k,store,datetime.datetime.now())\n",
    "        logging.info(str(count_k)+\"|\"+store+\"|\"+str(datetime.datetime.now()))\n",
    "        \n",
    "    \n",
    "Sep_12_new_request=pd.merge(Sep_12_new_request,Sep_12_new_request_Decile,on=\"location_id\",how=\"left\")\n",
    "\n",
    "logging.info(\"Done of the new request | \"+str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# QC\n",
    "rewards_sales_from_SP=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/From_Sp/combinedtransactions_0811.csv\",\n",
    "                                 dtype=str,nrows=samplerows,usecols=['customer_id_hashed','transaction_date','transaction_id','location_id','total_transaction_amt'])\n",
    "rewards_sales_from_SP=rewards_sales_from_SP[rewards_sales_from_SP['location_id'].isin(inclusion_store_list_set)]\n",
    "rewards_sales_from_SP['transaction_date']=rewards_sales_from_SP['transaction_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards_sales_from_SP_2017Q2['transaction_date'].unique()\n",
    "df_qc=rewards_sales_from_SP_2017Q2[rewards_sales_from_SP_2017Q2['location_id']==\"1553\"]\n",
    "df_qc=df_qc.drop_duplicates()\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "logging.info(\"QC 1553 rewards 2017 sales: \"+str(sales_1553))\n",
    "logging.info(\"QC 1553 rewards 2017 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "df_qc=rewards_sales_from_SP[rewards_sales_from_SP['location_id']==\"1553\"]\n",
    "df_qc=df_qc.drop_duplicates()\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "logging.info(\"QC 1553 rewards 2018 sales: \"+str(sales_1553))\n",
    "logging.info(\"QC 1553 rewards 2018 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "\n",
    "df_qc=rewards_sales_from_SP_2017Q2[rewards_sales_from_SP_2017Q2['location_id']==\"3\"]\n",
    "df_qc=df_qc.drop_duplicates()\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "logging.info(\"QC 3 rewards 2017 sales: \"+str(sales_1553))\n",
    "logging.info(\"QC 3 rewards 2017 days: \"+str(len(df_qc['transaction_date'].unique())))\n",
    "\n",
    "\n",
    "df_qc=rewards_sales_from_SP[rewards_sales_from_SP['location_id']==\"3\"]\n",
    "df_qc=df_qc.drop_duplicates()\n",
    "sales_1553=df_qc['total_transaction_amt'].astype(float).sum()\n",
    "logging.info(\"QC 3 rewards 2018 sales: \"+str(sales_1553))\n",
    "logging.info(\"QC 3 rewards 2018 days: \"+str(len(df_qc['transaction_date'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_output=pd.merge(store_level_output,output_zip_level,on=\"location_id\",how=\"left\")\n",
    "logging.info(\"Merge1 | \"+str(datetime.datetime.now()))\n",
    "final_output=pd.merge(final_output,Sep_12_new_request,on=\"location_id\",how=\"left\")\n",
    "logging.info(\"Merge2 | \"+str(datetime.datetime.now()))\n",
    "final_output['Q2_rewards_AOV_2018']=final_output['Q2_rewards_sales_2018']/final_output['Q2_rewards_trans_2018']\n",
    "logging.info(\"3 | \"+str(datetime.datetime.now()))\n",
    "final_output['Q2_rewards_trans_per_id']=final_output['Q2_rewards_trans_2018']/final_output['2018_Q2_rewards_id_shopped']\n",
    "logging.info(\"4 | \"+str(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_weekly=pd.read_excel(\"/home/jian/BiglotsCode/outputs/Output_2018-09-08/wide_sales_date2018-09-08.xlsx\",\n",
    "                          dtype=str,sheetname=\"sales\")\n",
    "sales_weekly=sales_weekly[['location_id']+Q2_2017_Weeks+Q2_2018_Weeks]\n",
    "for col in sales_weekly.columns.tolist()[1:]:\n",
    "    sales_weekly[col]=sales_weekly[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_weekly['inclusion_index']=np.where(((sales_weekly['2017-05-13']>0) &\\\n",
    "                                          (sales_weekly['2017-05-20']>0) &\\\n",
    "                                          (sales_weekly['2017-05-27']>0) &\\\n",
    "                                          (sales_weekly['2017-06-03']>0) &\\\n",
    "                                          (sales_weekly['2017-06-10']>0) &\\\n",
    "                                          (sales_weekly['2017-06-17']>0) &\\\n",
    "                                          (sales_weekly['2017-06-24']>0) &\\\n",
    "                                          (sales_weekly['2017-07-01']>0) &\\\n",
    "                                          (sales_weekly['2017-07-08']>0) &\\\n",
    "                                          (sales_weekly['2017-07-15']>0) &\\\n",
    "                                          (sales_weekly['2017-07-22']>0) &\\\n",
    "                                          (sales_weekly['2017-07-29']>0) &\\\n",
    "                                          (sales_weekly['2017-08-05']>0) &\\\n",
    "                                          (sales_weekly['2018-05-12']>0) &\\\n",
    "                                          (sales_weekly['2018-05-19']>0) &\\\n",
    "                                          (sales_weekly['2018-05-26']>0) &\\\n",
    "                                          (sales_weekly['2018-06-02']>0) &\\\n",
    "                                          (sales_weekly['2018-06-09']>0) &\\\n",
    "                                          (sales_weekly['2018-06-16']>0) &\\\n",
    "                                          (sales_weekly['2018-06-23']>0) &\\\n",
    "                                          (sales_weekly['2018-06-30']>0) &\\\n",
    "                                          (sales_weekly['2018-07-07']>0) &\\\n",
    "                                          (sales_weekly['2018-07-14']>0) &\\\n",
    "                                          (sales_weekly['2018-07-21']>0) &\\\n",
    "                                          (sales_weekly['2018-07-28']>0) &\\\n",
    "                                          (sales_weekly['2018-08-04']>0)),1,0)\n",
    "sales_weekly_inclusion_df=sales_weekly[['location_id','inclusion_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_output=pd.merge(final_output,sales_weekly_inclusion_df,on=\"location_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_output.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/Store_Stats_201809/BL_Store_Rewards_Stats_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(\"/home/jian/Projects/Big_Lots/Analysis/Store_Stats_201809/BL_Store_Rewards_Stats_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inclusion_df=final_output[final_output['inclusion_index']==1]\n",
    "exclusion_df=final_output[final_output['inclusion_index']==0]\n",
    "\n",
    "inclusion_df.to_excel(writer,\"inclusion\",index=False)\n",
    "exclusion_df.to_excel(writer,\"exclusion\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
