{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick your Day Test vs Control\n",
    " - Measure Incremental Sales and Activation\n",
    " - (6/25 -7/17 20% Off) \n",
    " - FB Target Audience groups (Test & hold out controls) to be:\n",
    "1.  12 month active\n",
    "2.  over 12 month Lapsed\n",
    "3.  Recent enrolls not yet activated:\\\n",
    "\n",
    "KPIs:\n",
    "Perspectives:\n",
    "- whole period: 6-25 to 7-17\n",
    "- weekly\n",
    "\n",
    "Metrics:\n",
    "- Total unique ids\n",
    "- Total unique shoppers\n",
    "- Total sales\n",
    "- Total transactions\n",
    "\n",
    "Once the above extracted from the data, others can be easily edited in Excel or you are free to add the lines in your code:\n",
    "\n",
    "- Conversion rate (cvr): shopper/total_id\n",
    "- Trans per id: total trans / total unique ids\n",
    "- Sales per id: total sales / total unique ids\n",
    "- AOV: tot sales / tot trxns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlalchemy\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "DATA_PATH = Path('../data')\n",
    "DATA_PATH_RAW = DATA_PATH / 'raw'\n",
    "DATA_PATH_INTERIM = DATA_PATH / 'interim'\n",
    "DATA_PATH_PROCESSED = DATA_PATH / 'processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_date(date, offset):\n",
    "    \"\"\"Adjust the date by offset.\n",
    "    \n",
    "    Args:\n",
    "        date (str or date object):\n",
    "            The date to be adjusted.\n",
    "        offset (int): how many days to adjust\n",
    "    \n",
    "    Returns: date str of 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(date, infer_datetime_format=True)\n",
    "    new_date = d + pd.to_timedelta(offset, unit='D')\n",
    "    return new_date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def is_in_date_range(x, start, end, include_start=False, include_end=True):\n",
    "    \"\"\"Helper func to extract the date string in path \n",
    "    and determine if it's in the given range.\n",
    "    \n",
    "    Returns: True or False\n",
    "    \"\"\"\n",
    "    d = str(x).split('_by_weeks/MediaStorm_')[1][:10]\n",
    "    flag_start = d >= start if include_start else d > start\n",
    "    flag_end = d <= end if include_end else d < end\n",
    "    return flag_start and flag_end\n",
    "\n",
    "\n",
    "def list_files_by_date_range(start, end, file_type, \n",
    "                             include_start=False, include_end=True):\n",
    "    \"\"\"List all available weekly data files under path:\n",
    "        /home/jian/BigLots/YYYY_by_weeks/\n",
    "    on 192 server that match given date range.\n",
    "    \n",
    "    Args:\n",
    "        start: str start date in format 'YYYY-MM-DD'\n",
    "        end: str\n",
    "        file_type: str one of the following: ['DailySales', 'MasterWeekly']\n",
    "    \n",
    "    Returns:\n",
    "        list of absolute file paths(pathlib.Path)\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for p in list(Path('/home/jian/BigLots/').glob('*_by_weeks/')):\n",
    "        files += (list(p.rglob('MediaStorm'+file_type+'*.[Tt][Xx][Tt]')))\n",
    "    files = [x for x in files \n",
    "             if is_in_date_range(x, start, end, include_start, include_end)]\n",
    "    return files\n",
    "\n",
    "\n",
    "def collect_data_by_date_range(start, end, file_type, use_cols=None,\n",
    "                               include_start=True, include_end=True,\n",
    "                               output_path=None):\n",
    "    \"\"\"Read weekly data files under path:\n",
    "        /home/jian/BigLots/YYYY_by_weeks/\n",
    "    on 192 server that match given date range.\n",
    "    \n",
    "    Args:\n",
    "        start (str): start date in format 'YYYY-MM-DD'\n",
    "        end (str): end date in format 'YYYY-MM-DD'\n",
    "        file_type (str): one of the following: ['DailySales', 'MasterWeekly']\n",
    "        use_cols (iterable): columns to use when reading files\n",
    "        include_start (bool): if start date is inclusive\n",
    "        include_end (bool): if end date is inclusive\n",
    "        output_path (str or pathlib.Path): if specified, file will be saved to path\n",
    "            in parquet format.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "            \n",
    "    files = list_files_by_date_range(\n",
    "        start=start, \n",
    "        end=end, \n",
    "        file_type=file_type, \n",
    "        include_start=include_start, \n",
    "        include_end=include_end)\n",
    "    df = pd.concat(\n",
    "        [pd.read_csv(f, sep='|', usecols=use_cols) for f in files]\n",
    "    ).dropna().drop_duplicates()\n",
    "    \n",
    "    if output_path:\n",
    "        df.to_parquet(Path(output_path), index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_days_since_sign_up(df, inspect_date):\n",
    "    t = pd.to_datetime(inspect_date) - pd.to_datetime(df['sign_up_date'])\n",
    "    return t.apply(lambda x:x.days)\n",
    "\n",
    "\n",
    "def filter_ids_by_window(df, window):\n",
    "    df = df.loc[(df['days_since_sign_up'] >= window[0])\n",
    "                & (df['days_since_sign_up'] <= window[1])]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_customer_data_from_db(start, end):\n",
    "    # TO BE EXTENDED\n",
    "    BL_SQL_CONNECTION= 'mysql+pymysql://nick:Nick-2020@localhost/BigLots' \n",
    "    BL_engine = sqlalchemy.create_engine(\n",
    "        BL_SQL_CONNECTION, \n",
    "        pool_recycle=1800)\n",
    "\n",
    "    all_ids = pd.read_sql(\n",
    "        \"\"\"SELECT customer_id_hashed, email_address_hash\n",
    "           FROM BL_Rewards_Master\n",
    "           WHERE sign_up_date >= %s AND sign_up_date <= %s;\"\"\",\n",
    "        con = BL_engine,\n",
    "        params=(start, end))\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def generate_weekly_intervals(start, end, week_start_on='SUN'):\n",
    "    week_starts = pd.date_range(start, end, freq='W-' + week_start_on)\n",
    "    week_ends = week_starts  - pd.Timedelta('1D')\n",
    "    week_starts = [start] + week_starts.strftime('%Y-%m-%d').tolist()\n",
    "    week_ends = week_ends.strftime('%Y-%m-%d').tolist() + [end]\n",
    "    return week_starts, week_ends\n",
    "\n",
    "\n",
    "def calc_kpis(group, transactions, name, start=None, end=None):\n",
    "    \"\"\"Calculate KPIs.\"\"\"\n",
    "    if start:\n",
    "        transactions = transactions.loc[transactions['transaction_dt'] >= start]\n",
    "    else:\n",
    "        start = transactions['transaction_dt'].min()\n",
    "    if end:\n",
    "        transactions = transactions.loc[transactions['transaction_dt'] <= end]\n",
    "    else:\n",
    "        end = transactions['transaction_dt'].max()\n",
    "    \n",
    "    \n",
    "    group = group[['customer_id_hashed']].drop_duplicates().dropna()\n",
    "    group = group.merge(transactions, how='left', on='customer_id_hashed')\n",
    "\n",
    "    tot_ids = group['customer_id_hashed'].drop_duplicates().shape[0]\n",
    "    tot_shprs = (group.loc[~group['transaction_id'].isna(), 'customer_id_hashed']\n",
    "                 .drop_duplicates().shape[0])\n",
    "    tot_sales = group['item_transaction_amt'].sum()\n",
    "    tot_trxns = group.loc[~group['transaction_id'].isna(), :].shape[0]\n",
    "    cvr = tot_shprs / tot_ids # conversion rate\n",
    "    tpi = tot_trxns / tot_ids # trxns per id\n",
    "    spi = tot_sales / tot_ids # sales per id\n",
    "    tps = tot_trxns / tot_shprs # trxns per shopper\n",
    "    sps = tot_sales / tot_shprs # sales per shopper\n",
    "    aov = tot_sales / tot_trxns # avg order value\n",
    "\n",
    "    res = pd.DataFrame({'start': [start],\n",
    "                        'end': [end],\n",
    "                        'group': [name],\n",
    "                        'tot_ids': [tot_ids],\n",
    "                        'tot_shprs': [tot_shprs],\n",
    "                        'tot_sales': [tot_sales],\n",
    "                        'tot_trxns': [tot_trxns],\n",
    "                        'convertion_rate': [cvr],\n",
    "                        'trxns_per_id': [tpi],\n",
    "                        'sales_per_id': [spi],\n",
    "                        'trxns_per_shpr': [tps],\n",
    "                        'sales_per_shpr': [sps],\n",
    "                        'avg_order_value': [aov]},)\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_kpis(kpis, notional_count=100000, by='tot_ids'):\n",
    "    norm_kpis = kpis.copy()\n",
    "    factor = notional_count / kpis['tot_ids']\n",
    "    cols_to_normalize = ['tot_ids', 'tot_shprs',\n",
    "                         'tot_sales', 'tot_trxns']\n",
    "    norm_kpis[cols_to_normalize] = norm_kpis[cols_to_normalize] * factor\n",
    "    return norm_kpis\n",
    "\n",
    "\n",
    "def analyze(test, control, transactions, start=None, end=None,\n",
    "            by_week=False, name=None, add_norm_res=False):\n",
    "    \"\"\"Analyze transactions and sales for test & control group.\"\"\"\n",
    "    if not start:\n",
    "        start = transactions['transaction_dt'].min()\n",
    "    if not end:\n",
    "        end = transactions['transaction_dt'].max()\n",
    "        \n",
    "    if by_week:\n",
    "        kpis_by_week = []\n",
    "        week_starts, week_ends = generate_weekly_intervals(start, end)\n",
    "        for st, en in zip(week_starts, week_ends):\n",
    "            kpis_test = calc_kpis(test, transactions, 'test', st, en)\n",
    "            kpis_ctrl = calc_kpis(control, transactions, 'control', st, en)\n",
    "            kpis_by_week.append(pd.concat([kpis_test, kpis_ctrl]))\n",
    "        kpis = pd.concat(kpis_by_week)\n",
    "            \n",
    "    else:\n",
    "        kpis_test = calc_kpis(test, transactions, 'test', start, end)\n",
    "        kpis_ctrl = calc_kpis(control, transactions, 'control', start, end)\n",
    "        kpis = pd.concat([kpis_test, kpis_ctrl])\n",
    "\n",
    "    if name:\n",
    "        kpis.insert(0, 'name', name)\n",
    "        \n",
    "    if add_norm_res and not by_week:\n",
    "        norm_kpis = kpis.apply(normalize_kpis, axis=1)\n",
    "        norm_kpis['name'] = norm_kpis['name'] + '_normalized'\n",
    "        kpis = pd.concat([kpis, norm_kpis])\n",
    "    return kpis\n",
    "\n",
    "\n",
    "def read_agg_trxns(start, end):\n",
    "    \"\"\"Wrapper for reading trxn data\"\"\"\n",
    "    print(\"Collecting transactions data...\")\n",
    "    agg_trxn_filename = ('agg_trxns_'\n",
    "                         + start.replace('-', '')\n",
    "                         + '_'\n",
    "                         + end.replace('-', '')\n",
    "                         + '.parquet')    \n",
    "    agg_trxn_path = DATA_PATH_INTERIM / agg_trxn_filename\n",
    "\n",
    "    if agg_trxn_path.exists():\n",
    "        print(\"Loading existing agg trxns data...\")\n",
    "        agg_trxns = pd.read_parquet(agg_trxn_path)\n",
    "        print(\"Agg trxns read from path: \" + str(agg_trxn_path))\n",
    "    else:\n",
    "        print(\"No existing agg trxn data...\")\n",
    "        trxn_start = adjust_date(start, -6)\n",
    "        trxn_end = adjust_date(end, +6)\n",
    "        trxn_filename = ('trxns_'\n",
    "                         + trxn_start.replace('-', '')\n",
    "                         + '_'\n",
    "                         + trxn_end.replace('-', '')\n",
    "                         + '.parquet')\n",
    "        trxn_path = DATA_PATH_RAW / trxn_filename\n",
    "        if trxn_path.exists():\n",
    "            print(\"Found existing transactions data. Will generate agg trxns data from this dataset.\")\n",
    "            print(\"Loading transaction data...\")\n",
    "            transactions = pd.read_parquet(trxn_path)\n",
    "            print(\"Transactions data read from path: \" + str(trxn_path))\n",
    "        else:\n",
    "            print(\"No existing trxn data...collecting from server...\")\n",
    "            transactions = collect_data_by_date_range(\n",
    "                start=trxn_start,\n",
    "                end=trxn_end,\n",
    "                file_type='DailySales', \n",
    "                use_cols=['location_id', 'transaction_dt', 'transaction_id',\n",
    "                          'customer_id_hashed', 'item_transaction_amt'],\n",
    "                output_path=trxn_path)\n",
    "            print(\"Trxns data has been gathered and saved to: \" + str(trxn_path))\n",
    "\n",
    "\n",
    "        print(\"Transactions data shape: \" + str(transactions.shape))\n",
    "\n",
    "        print(\"\\nAggregating trxns data...\")\n",
    "        agg_trxns = (transactions\n",
    "                     .groupby(['location_id',\n",
    "                               'transaction_dt',\n",
    "                               'transaction_id',\n",
    "                               'customer_id_hashed'])\n",
    "                     ['item_transaction_amt'].sum().reset_index())\n",
    "\n",
    "        # filter transactions to date range\n",
    "        msk = ((agg_trxns['transaction_dt'] >= start)\n",
    "               & (agg_trxns['transaction_dt'] <= end))\n",
    "        agg_trxns = agg_trxns.loc[msk, :]\n",
    "        \n",
    "        # save to file\n",
    "        agg_trxns.to_parquet(agg_trxn_path, index=False)                 \n",
    "        print(\"Data has been aggregated and saved to: \" + str(agg_trxn_path))\n",
    "\n",
    "    print(\"Agg trxns data shape: \" + str(agg_trxns.shape) + \"\\n\")\n",
    "    return agg_trxns\n",
    "\n",
    "def read_groups(p):\n",
    "    return (pd.read_parquet(p)\n",
    "              .rename(columns={'hashed_customer_num': 'customer_id_hashed'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transactions data...\n",
      "Loading existing agg trxns data...\n",
      "Agg trxns read from path: ../data/interim/agg_trxns_20200625_20200717.parquet\n",
      "Agg trxns data shape: (5685942, 5)\n",
      "\n",
      "Calculating KPIs for group: lapsed\n",
      "Calculating KPIs for group: active\n",
      "Calculating KPIs for group: inactive\n",
      "Merging results...\n",
      "Done.\n",
      "CPU times: user 22min 12s, sys: 10min 55s, total: 33min 8s\n",
      "Wall time: 33min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = '2020-06-25'\n",
    "end = '2020-07-17'\n",
    "\n",
    "# read agg trxns\n",
    "agg_trxns = read_agg_trxns(start, end)\n",
    "\n",
    "# read audiences and calculate kpis\n",
    "lapsed = {'test': DATA_PATH_RAW / 'PYD_Lapsed.parquet',\n",
    "          'control': DATA_PATH_RAW / 'PYD_Lapsed_Control.parquet',\n",
    "          'name': 'lapsed'}\n",
    "active = {'test': DATA_PATH_RAW / 'PYD_Active.parquet',\n",
    "          'control': DATA_PATH_RAW / 'PYD_Active_Control.parquet',\n",
    "          'name': 'active'}\n",
    "inactive = {'test': DATA_PATH_RAW / 'PYD_Web_Not_Active.parquet',\n",
    "            'control': DATA_PATH_RAW / 'PYD_Web_Not_Active_Control.parquet',\n",
    "            'name': 'inactive'}\n",
    "experiments = [lapsed, active, inactive]\n",
    "\n",
    "kpis_alltime_list = []\n",
    "kpis_weekly_list = []\n",
    "for exp in experiments:\n",
    "    print(\"Calculating KPIs for group: \" + exp['name'])\n",
    "    test = read_groups(exp['test'])\n",
    "    control = read_groups(exp['control'])\n",
    "    name = exp['name']\n",
    "    kpis_alltime = analyze(test, control, agg_trxns, start, end,\n",
    "                           by_week=False, name=name, add_norm_res=True)\n",
    "    kpis_weekly = analyze(test, control, agg_trxns, start, end,\n",
    "                          by_week=True, name=name)\n",
    "    kpis_alltime.to_csv(DATA_PATH_PROCESSED / (name + '_alltime.csv'), index=False)\n",
    "    kpis_weekly.to_csv(DATA_PATH_PROCESSED / (name + '_weekly.csv'), index=False)\n",
    "    kpis_alltime_list.append(kpis_alltime)\n",
    "    kpis_weekly_list.append(kpis_weekly)\n",
    "\n",
    "print(\"Merging results...\")    \n",
    "kpis_alltime_all = pd.concat(kpis_alltime_list)\n",
    "kpis_weekly_all = pd.concat(kpis_weekly_list)\n",
    "kpis_alltime_all.to_csv(DATA_PATH_PROCESSED / ('all_groups_alltime.csv'), index=False)\n",
    "kpis_weekly_all.to_csv(DATA_PATH_PROCESSED / ('all_groups_weekly.csv'), index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
