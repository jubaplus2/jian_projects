{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-27 13:51:57.005811\n",
      "/home/jian/Projects/Big_Lots/Analysis/2019_Q4/Planner_Request/JT_non_rewards_sales_so_far_2019Q4_4_weeks\n",
      "ty_start_date: 2019-11-03\n",
      "ty_end_date: 2020-02-01\n",
      "ly_start_date: 2018-11-04\n",
      "ly_end_date: 2019-02-02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "print(datetime.datetime.now())\n",
    "print(os.getcwd())\n",
    "\n",
    "ty_start_date=datetime.date(2019,11,3)\n",
    "ty_end_date=datetime.date(2020,2,1)\n",
    "\n",
    "ly_start_date=ty_start_date-datetime.timedelta(days=52*7)\n",
    "ly_end_date=ty_end_date-datetime.timedelta(days=52*7)\n",
    "\n",
    "\n",
    "print(\"ty_start_date:\",ty_start_date)\n",
    "print(\"ty_end_date:\",ty_end_date)\n",
    "\n",
    "print(\"ly_start_date:\",ly_start_date)\n",
    "print(\"ly_end_date:\",ly_end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def recursive_file_gen(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root,file)\n",
    "list_files_ly=glob.glob(\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/*.txt\")\n",
    "list_files_ly=[x for x in list_files_ly if x.split(\"/MediaStormDailySalesHistory\")[1][:8]>=str(ly_start_date).replace(\"-\",\"\")]\n",
    "list_files_ly=[x for x in list_files_ly if x.split(\"/MediaStormDailySalesHistory\")[1][:8]<=str(ly_end_date).replace(\"-\",\"\")]\n",
    "list_files_ly.sort()\n",
    "print(len(list_files_ly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "list_files_ty=recursive_file_gen(\"/home/jian/BigLots/\")\n",
    "list_files_ty=[x for x in list_files_ty if \"dailysales\" in x.lower()]\n",
    "list_files_ty=[x for x in list_files_ty if \"/MediaStorm_\" in x]\n",
    "list_files_ty=[x for x in list_files_ty if x.split(\"/MediaStorm_\")[1][:10]>=str(ty_start_date)]\n",
    "list_files_ty=[x for x in list_files_ty if x.split(\"/MediaStorm_\")[1][:10]<=str(ty_end_date)]\n",
    "list_files_ty.sort()\n",
    "print(len(list_files_ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-09/MediaStormDailySales20191112-115210-002.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-16/MediaStormDailySales20191119-112232-478.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-23/MediaStormDailySales20191126-112901-552.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-30/MediaStormDailySales20191203.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-07/MediaStormDailySales.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-14/MediaStormDailySales20191217-195625-000.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-21/MediaStormDailySales20191226-122746-000.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-28/MediaStormDailySales20191231-112945-515.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-04/MediaStormDailySales20200107-112859-015.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-11/MediaStormDailySales20200114-115009-140.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-18/MediaStormDailySales20200121-111749-649.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files_ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>division_id</th>\n",
       "      <th>department_id</th>\n",
       "      <th>class_code_id</th>\n",
       "      <th>subclass_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>11001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>11001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   division_id  department_id class_code_id subclass_id\n",
       "0            1            108         11001           2\n",
       "1            1            108         11001           4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_taxo=pd.read_table(\"/home/jian/BigLots/static_files/ProductTaxonomy/MediaStormProductTaxonomy20200101-135600-916.txt\",\n",
    "                              dtype=str,sep=\"|\")\n",
    "df_product_taxo=df_product_taxo[['division_id','department_id','class_code_id','subclass_id']].drop_duplicates()\n",
    "df_product_taxo['division_id']=df_product_taxo['division_id'].astype(int)\n",
    "df_product_taxo['department_id']=df_product_taxo['department_id'].astype(int)\n",
    "df_division_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Division Names.txt\",sep=\"|\")\n",
    "df_department_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",sep=\"|\")\n",
    "\n",
    "df_product_taxo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-27 14:50:53.507057 1 done\n",
      "2020-01-27 14:53:04.006411 2 done\n",
      "2020-01-27 14:56:06.515859 3 done\n",
      "2020-01-27 14:59:20.751780 4 done\n",
      "2020-01-27 15:02:57.401403 5 done\n",
      "2020-01-27 15:06:52.126354 6 done\n",
      "2020-01-27 15:11:13.578111 7 done\n",
      "2020-01-27 15:14:14.079532 8 done\n",
      "2020-01-27 15:16:23.140376 9 done\n",
      "2020-01-27 15:18:32.325163 10 done\n",
      "2020-01-27 15:21:15.867267 11 done\n",
      "2020-01-27 15:23:50.643051 12 done\n",
      "2020-01-27 15:26:01.871916 13 done\n"
     ]
    }
   ],
   "source": [
    "df_summary_InWeek_ly=pd.DataFrame()\n",
    "df_summary_CumQ_ly=pd.DataFrame()\n",
    "\n",
    "df_rewards_ids_ly=pd.DataFrame()\n",
    "df_division_output_ly=pd.DataFrame()\n",
    "df_department_output_ly=pd.DataFrame()\n",
    "\n",
    "cum_R_sales=0\n",
    "cum_R_trans=0\n",
    "\n",
    "cum_N_sales=0\n",
    "cum_N_trans=0\n",
    "\n",
    "week_num=0\n",
    "for file in list_files_ly:\n",
    "    week_num+=1\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",\n",
    "                    usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','class_code_id','subclass_id','item_transaction_amt'])\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=pd.merge(df,df_product_taxo,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "    df['division_id']=df['division_id'].fillna(\"nan\")\n",
    "    df['department_id']=df['department_id'].fillna(\"nan\")\n",
    "    \n",
    "    date_min=df['transaction_dt'].min()\n",
    "    date_max=df['transaction_dt'].max()\n",
    "    \n",
    "    df['rewards_label']=np.where(pd.notnull(df['customer_id_hashed']),\"Rewards\",\"Non_Rewards\")\n",
    "    \n",
    "    \n",
    "    ######\n",
    "    # division view from department, no change of the order below\n",
    "    df_department_sales=df.groupby(['rewards_label','division_id','department_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_division_sales=df_department_sales.groupby(['rewards_label','division_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_division_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','rewards_label']].drop_duplicates()\n",
    "    df_division_trans['trans']=1\n",
    "    df_division_trans=df_division_trans.groupby(['rewards_label','division_id'])['trans'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_department_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','department_id','rewards_label']].drop_duplicates()\n",
    "    df_department_trans['trans']=1\n",
    "    df_department_trans=df_department_trans.groupby(['rewards_label','division_id','department_id'])['trans'].sum().to_frame().reset_index()\n",
    "\n",
    "    df_division=pd.merge(df_division_sales,df_division_trans,on=['rewards_label','division_id'],how=\"outer\")\n",
    "    df_department=pd.merge(df_department_sales,df_department_trans,on=['rewards_label','division_id','department_id'],how=\"outer\")\n",
    "    \n",
    "    df_division['week_num']=week_num\n",
    "    df_division['week_start']=date_min\n",
    "    df_division['week_end']=date_max\n",
    "\n",
    "    df_department['week_num']=week_num\n",
    "    df_department['week_start']=date_min\n",
    "    df_department['week_end']=date_max    \n",
    "    ######\n",
    "    df_division_output_ly=df_division_output_ly.append(df_division)\n",
    "    df_department_output_ly=df_department_output_ly.append(df_department)\n",
    "    \n",
    "    \n",
    "    df_rewards=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df_nonrewards=df[pd.isnull(df['customer_id_hashed'])]\n",
    "    \n",
    "    num_R_sales=df_rewards['item_transaction_amt'].sum()\n",
    "    num_R_trans=df_rewards[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates().shape[0]\n",
    "    num_R_shoppers=df_rewards['customer_id_hashed'].nunique()\n",
    "    \n",
    "    num_N_sales=df_nonrewards['item_transaction_amt'].sum()\n",
    "    num_N_trans=df_nonrewards[['location_id','transaction_dt','transaction_id']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_rewards_shoppers=df_rewards[['customer_id_hashed']].drop_duplicates()\n",
    "    df_rewards_shoppers['week_start']=date_min\n",
    "    df_rewards_shoppers['week_end']=date_max\n",
    "    df_rewards_ids_ly=df_rewards_ids_ly.append(df_rewards_shoppers)\n",
    "    \n",
    "    cum_R_sales+=num_R_sales\n",
    "    cum_R_trans+=num_R_trans\n",
    "    cum_N_sales+=num_N_sales\n",
    "    cum_N_trans+=num_N_trans\n",
    "    cum_R_shoppers=df_rewards_ids_ly['customer_id_hashed'].nunique()\n",
    "    \n",
    "    df_in_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":num_R_sales,\"Rewards_trans\":num_R_trans,\"Rewards_shoppers\":num_R_shoppers,\n",
    "                             'NonRewards_sales':num_N_sales,\"NonRewards_trans\":num_N_trans,\n",
    "                            },index=[0])\n",
    "    \n",
    "    df_cum_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":cum_R_sales,\"Rewards_trans\":cum_R_trans,\"Rewards_shoppers\":cum_R_shoppers,\n",
    "                             'NonRewards_sales':cum_N_sales,\"NonRewards_trans\":cum_N_trans,\n",
    "                            },index=[0])\n",
    "\n",
    "    df_summary_InWeek_ly=df_summary_InWeek_ly.append(df_in_week)\n",
    "    df_summary_CumQ_ly=df_summary_CumQ_ly.append(df_cum_week)\n",
    "    print(datetime.datetime.now(),week_num,\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-27 15:29:13.446679 1 done\n",
      "2020-01-27 15:31:23.344415 2 done\n",
      "2020-01-27 15:33:58.416819 3 done\n",
      "2020-01-27 15:37:46.518751 4 done\n",
      "2020-01-27 15:41:32.857479 5 done\n",
      "2020-01-27 15:45:37.511323 6 done\n",
      "2020-01-27 15:50:13.126638 7 done\n",
      "2020-01-27 15:53:47.667324 8 done\n",
      "2020-01-27 15:56:22.699252 9 done\n",
      "2020-01-27 15:58:42.727723 10 done\n",
      "2020-01-27 16:01:10.653397 11 done\n"
     ]
    }
   ],
   "source": [
    "df_summary_InWeek_ty=pd.DataFrame()\n",
    "df_summary_CumQ_ty=pd.DataFrame()\n",
    "\n",
    "df_rewards_ids_ty=pd.DataFrame()\n",
    "df_division_output_ty=pd.DataFrame()\n",
    "df_department_output_ty=pd.DataFrame()\n",
    "\n",
    "cum_R_sales=0\n",
    "cum_R_trans=0\n",
    "\n",
    "cum_N_sales=0\n",
    "cum_N_trans=0\n",
    "\n",
    "week_num=0\n",
    "for file in list_files_ty:\n",
    "    week_num+=1\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",\n",
    "                    usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','class_code_id','subclass_id','item_transaction_amt'])\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=pd.merge(df,df_product_taxo,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "    df['division_id']=df['division_id'].fillna(\"nan\")\n",
    "    df['department_id']=df['department_id'].fillna(\"nan\")\n",
    "    \n",
    "    date_min=df['transaction_dt'].min()\n",
    "    date_max=df['transaction_dt'].max()\n",
    "    \n",
    "    df['rewards_label']=np.where(pd.notnull(df['customer_id_hashed']),\"Rewards\",\"Non_Rewards\")\n",
    "    \n",
    "    \n",
    "    ######\n",
    "    # division view from department, no change of the order below\n",
    "    df_department_sales=df.groupby(['rewards_label','division_id','department_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_division_sales=df_department_sales.groupby(['rewards_label','division_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_division_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','rewards_label']].drop_duplicates()\n",
    "    df_division_trans['trans']=1\n",
    "    df_division_trans=df_division_trans.groupby(['rewards_label','division_id'])['trans'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_department_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','department_id','rewards_label']].drop_duplicates()\n",
    "    df_department_trans['trans']=1\n",
    "    df_department_trans=df_department_trans.groupby(['rewards_label','division_id','department_id'])['trans'].sum().to_frame().reset_index()\n",
    "\n",
    "    df_division=pd.merge(df_division_sales,df_division_trans,on=['rewards_label','division_id'],how=\"outer\")\n",
    "    df_department=pd.merge(df_department_sales,df_department_trans,on=['rewards_label','division_id','department_id'],how=\"outer\")\n",
    "    \n",
    "    df_division['week_num']=week_num\n",
    "    df_division['week_start']=date_min\n",
    "    df_division['week_end']=date_max\n",
    "\n",
    "    df_department['week_num']=week_num\n",
    "    df_department['week_start']=date_min\n",
    "    df_department['week_end']=date_max    \n",
    "    ######\n",
    "    df_division_output_ty=df_division_output_ty.append(df_division)\n",
    "    df_department_output_ty=df_department_output_ty.append(df_department)\n",
    "    \n",
    "    \n",
    "    df_rewards=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df_nonrewards=df[pd.isnull(df['customer_id_hashed'])]\n",
    "    \n",
    "    num_R_sales=df_rewards['item_transaction_amt'].sum()\n",
    "    num_R_trans=df_rewards[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates().shape[0]\n",
    "    num_R_shoppers=df_rewards['customer_id_hashed'].nunique()\n",
    "    \n",
    "    num_N_sales=df_nonrewards['item_transaction_amt'].sum()\n",
    "    num_N_trans=df_nonrewards[['location_id','transaction_dt','transaction_id']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_rewards_shoppers=df_rewards[['customer_id_hashed']].drop_duplicates()\n",
    "    df_rewards_shoppers['week_start']=date_min\n",
    "    df_rewards_shoppers['week_end']=date_max\n",
    "    df_rewards_ids_ty=df_rewards_ids_ty.append(df_rewards_shoppers)\n",
    "    \n",
    "    cum_R_sales+=num_R_sales\n",
    "    cum_R_trans+=num_R_trans\n",
    "    cum_N_sales+=num_N_sales\n",
    "    cum_N_trans+=num_N_trans\n",
    "    cum_R_shoppers=df_rewards_ids_ty['customer_id_hashed'].nunique()\n",
    "    \n",
    "    df_in_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":num_R_sales,\"Rewards_trans\":num_R_trans,\"Rewards_shoppers\":num_R_shoppers,\n",
    "                             'NonRewards_sales':num_N_sales,\"NonRewards_trans\":num_N_trans,\n",
    "                            },index=[0])\n",
    "    \n",
    "    df_cum_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":cum_R_sales,\"Rewards_trans\":cum_R_trans,\"Rewards_shoppers\":cum_R_shoppers,\n",
    "                             'NonRewards_sales':cum_N_sales,\"NonRewards_trans\":cum_N_trans,\n",
    "                            },index=[0])\n",
    "\n",
    "    \n",
    "    df_summary_InWeek_ty=df_summary_InWeek_ty.append(df_in_week)\n",
    "    df_summary_CumQ_ty=df_summary_CumQ_ty.append(df_cum_week)\n",
    "    print(datetime.datetime.now(),week_num,\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_InWeek_ty=df_summary_InWeek_ty[df_summary_InWeek_ly.columns.tolist()]\n",
    "\n",
    "writer=pd.ExcelWriter(\"./BL_sales_summary_in_quarter_11weeks_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "df_summary_InWeek_ty.to_excel(writer,\"df_summary_InWeek_ty\",index=False)\n",
    "df_summary_CumQ_ty.to_excel(writer,\"df_summary_CumQ_ty\",index=False)\n",
    "df_division_output_ty.to_excel(writer,\"division_output_ty\",index=False)\n",
    "df_department_output_ty.to_excel(writer,\"department_output_ty\",index=False)\n",
    "\n",
    "df_summary_InWeek_ly.to_excel(writer,\"df_summary_InWeek_ly\",index=False)\n",
    "df_summary_CumQ_ly.to_excel(writer,\"df_summary_CumQ_ly\",index=False)\n",
    "df_division_output_ly.to_excel(writer,\"division_output_ly\",index=False)\n",
    "df_department_output_ly.to_excel(writer,\"department_output_ly\",index=False)\n",
    "\n",
    "df_division_name.to_excel(writer,\"df_division_name\",index=False)\n",
    "df_department_name.to_excel(writer,\"df_department_name\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards_ids_ty.to_csv(\"./df_rewards_ids_ty_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "df_rewards_ids_ly.to_csv(\"./df_rewards_ids_ly_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
