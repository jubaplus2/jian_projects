{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n",
      "Job start: model building 2020-08-25 13:48:36.356931\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(sklearn.__version__)\n",
    "print(\"Job start: model building\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'table_df_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c967d45c01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdict_tables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/table_names_%s.json\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtable_df_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'table_df_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtable_2_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'table_2_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtable_2_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'table_2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'table_df_1'"
     ]
    }
   ],
   "source": [
    "dict_config=json.load(open(\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/config.json\"))\n",
    "high_date=dict_config['pos_end_date']\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "recent_n_month=dict_config['recent_n_month']\n",
    "\n",
    "dict_tables=json.load(open(\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/table_names_%s.json\"%str(high_date).replace(\"-\",\"\")))\n",
    "table_df_1=dict_tables['table_df_1']\n",
    "table_2_1=dict_tables['table_2_1']\n",
    "table_2_2=dict_tables['table_2_2']\n",
    "table_0_train=dict_tables['table_crm_id_list_train']\n",
    "table_0_test=dict_tables['table_crm_id_list_test']\n",
    "\n",
    "BL_engine=sqlalchemy.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_dcm_id': '',\n",
       " 'table_id_from_crm': 'NEall_id_from_crm_pos_until_20200425',\n",
       " 'table_filtered_id': 'NEall_id_pos_until_20200425',\n",
       " 'table_filtered_pos': 'pred_pos_dept_NEall_id_pos_until_20200425',\n",
       " 'table_filtered_crm': 'crm_NEall_id_pos_until_20200425',\n",
       " 'table_max_trans_order': 'max_trans_NEall_id_pos_until_20200425',\n",
       " 'table_2_1': 'all_NEall_id_pred_pos_2_1_pos_until_20200425',\n",
       " 'table_2_2': 'all_NEall_id_pred_pos_2_2_pos_until_20200425'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count(*)    2939161\n",
      "Name: 0, dtype: int64 table_pred_1_crm_up_to_20200607\n",
      "count(*)    2939161\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_1_pos_until_20200607\n",
      "count(*)    2939161\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_2_pos_until_20200607\n",
      "count(*)    1000000\n",
      "Name: 0, dtype: int64 crm_table_id_list_train_20200607\n"
     ]
    }
   ],
   "source": [
    "for t in [table_df_1,table_2_1,table_2_2,table_0_train]:\n",
    "    print(pd.read_sql(\"select count(*) from %s\"%t,con=BL_engine).iloc[0],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_df0_train=pd.read_sql(\"desc %s\"%table_0_train,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_df_1=pd.read_sql(\"desc %s\"%table_df_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_1=pd.read_sql(\"desc %s\"%table_2_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_2=pd.read_sql(\"desc %s\"%table_2_2,con=BL_engine)['Field'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_no_need=['sign_up_location','customer_zip_code','nearest_BL_store','distc_to_sign_up',\n",
    "              'week_1st_trans','week_recent_one_trans','week_recent_two_trans']\n",
    "for col_remove in cols_no_need:\n",
    "    col_list_df_1=[x for x in col_list_df_1 if x != col_remove and (x not in [\"customer_id_hashed\", \"sign_up_date\"])]\n",
    "    col_list_2_1=[x for x in col_list_2_1 if x != col_remove and x!=\"id\"]\n",
    "    col_list_2_2=[x for x in col_list_2_2 if x != col_remove and x!=\"id\"]\n",
    "    \n",
    "sql_str_cols_df0_train=str([\"t0.\"+x for x in col_list_df0_train]).replace(\"'\",\"\")[1:-1]  \n",
    "sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_df_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_2_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_2_2]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_all=\", \".join([sql_str_cols_df0_train,sql_str_cols_df_1,sql_str_cols_2_1,sql_str_cols_2_2])\n",
    "# sql_str_cols_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cols_in_table={\n",
    "    \"t0_train\":{\"table_name\":table_0_train,\"cols\":['customer_id_hashed']},\n",
    "    \"t0_test\":{\"table_name\":table_0_test,\"cols\":['customer_id_hashed']},\n",
    "    \"t1\":{\"table_name\":table_df_1,\"cols\":col_list_df_1},\n",
    "    \"t2_1\":{\"table_name\":table_2_1,\"cols\":col_list_2_1},\n",
    "    \"t2_2\":{\"table_name\":table_2_2,\"cols\":col_list_2_2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queary=\"SELECT %s from %s as t0 \\\n",
    "left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed \\\n",
    "left join %s as t2_1 on t0.customer_id_hashed=t2_1.id \\\n",
    "left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%(sql_str_cols_all,table_0_train,table_df_1,table_2_1,table_2_2)\n",
    "# queary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-19 16:51:47.132024\n",
      "(1000000, 247)\n",
      "2020-08-19 17:04:50.187653\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "df_train=pd.read_sql(queary,con=BL_engine)\n",
    "print(df_train.shape)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "df_train_trans_1_only=df_train[pd.isnull(df_train['total_sales_recent_two_trans'])]\n",
    "df_train_trans_2_plus=df_train[pd.notnull(df_train['total_sales_recent_two_trans'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_trans_1_only, col_nunique<=1 dropped: total_trans_since_registration\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: trans_in_store\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: unique_stores\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: trans_online\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_361_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_361_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_361_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: week_counts_to_now_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: recent_two_trans_also_1st\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: purchase_channel_1st_trans_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: total_sales_recent_two_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: total_units_recent_two_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_minus_one_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_108_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_109_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_110_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_111_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_114_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_120_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_130_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_140_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_150_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_160_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_170_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_210_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_230_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_250_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_270_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_310_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_320_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_330_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_340_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_350_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_351_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_352_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_353_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_354_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_355_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_360_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_361_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_362_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_363_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_364_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_367_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_370_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_410_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_425_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_470_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_480_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_510_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_520_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_521_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_526_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_530_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_550_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_560_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_608_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_610_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_612_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_615_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_710_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_800_recent_two\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_1st_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_365_1st_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_540_1st_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_526_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_540_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_recent_two\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_366_recent_two\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_540_recent_two\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cols=df_train.columns.tolist()\n",
    "del df_train\n",
    "\n",
    "for col in list_cols:\n",
    "    if df_train_trans_1_only[col].nunique()<=1:\n",
    "        del df_train_trans_1_only[col]\n",
    "        print(\"df_train_trans_1_only, col_nunique<=1 dropped: %s\"%col)\n",
    "        \n",
    "for col in list_cols:\n",
    "    if df_train_trans_2_plus[col].nunique()<=1:\n",
    "        del df_train_trans_2_plus[col]\n",
    "        print(\"df_train_trans_2_plus, col_nunique<=1 dropped: %s\"%col)\n",
    "        \n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_trans_1_only 2020-08-19 17:05:14.630520\n",
      "df_train_trans_1_only (205878, 168)\n",
      "df_train_trans_1_only 2020-08-19 17:08:25.535534\n",
      "(205878, 114)\n",
      "df_train_trans_2_plus 2020-08-19 17:08:25.537856\n",
      "df_train_trans_2_plus_copy (300000, 236)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train_trans_1_only\",datetime.datetime.now())\n",
    "print(\"df_train_trans_1_only\",df_train_trans_1_only.shape)\n",
    "df_train_trans_1_only=df_train_trans_1_only.T.drop_duplicates().T\n",
    "print(\"df_train_trans_1_only\",datetime.datetime.now())\n",
    "print(df_train_trans_1_only.shape)\n",
    "\n",
    "## \n",
    "print(\"df_train_trans_2_plus\",datetime.datetime.now())\n",
    "df_train_trans_2_plus_copy=df_train_trans_2_plus.head(3*10**5)\n",
    "print(\"df_train_trans_2_plus_copy\",df_train_trans_2_plus_copy.shape)\n",
    "df_train_trans_2_plus_copy=df_train_trans_2_plus_copy.T.drop_duplicates().T\n",
    "print(\"df_train_trans_2_plus_copy\",datetime.datetime.now())\n",
    "print(df_train_trans_2_plus_copy.shape)\n",
    "\n",
    "list_cols_keep=df_train_trans_2_plus_copy.columns.tolist()\n",
    "df_train_trans_2_plus=df_train_trans_2_plus[list_cols_keep]\n",
    "print(\"df_train_trans_2_plus\",df_train_trans_2_plus.shape)\n",
    "\n",
    "del df_train_trans_2_plus_copy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols_remove_na_rows=['nearest_BL_dist']\n",
    "\n",
    "for col in df_train_trans_2_plus.columns.tolist():\n",
    "    df_na=df_train_trans_2_plus[pd.isnull(df_train_trans_2_plus[col])]\n",
    "    if df_na.shape[0]>0:\n",
    "        if col in list_cols_remove_na_rows:\n",
    "            df_train_trans_2_plus=df_train_trans_2_plus[pd.notnull(df_train_trans_2_plus[col])]\n",
    "            print(col,\"with na to delete\", df_na.shape[0])\n",
    "        else:\n",
    "            print(\"Warning: nan detected in the df_train_trans_2_plus col -- %s\"%col)\n",
    "   \n",
    "\n",
    "for col in df_train_trans_1_only.columns.tolist():\n",
    "    df_na=df_train_trans_1_only[pd.isnull(df_train_trans_1_only[col])]\n",
    "    if df_na.shape[0]>0:\n",
    "        if col in list_cols_remove_na_rows:\n",
    "            df_train_trans_1_only=df_train_trans_1_only[pd.notnull(df_train_trans_1_only[col])]\n",
    "            print(col,\"with na to delete\", df_na.shape[0])\n",
    "        else:\n",
    "            print(\"Warning: nan detected in the df_train_trans_1_only col -- %s\"%col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_trans_2_plus_id=df_train_trans_2_plus.iloc[:,0].values.tolist()\n",
    "df_train_trans_2_plus_Y=df_train_trans_2_plus.iloc[:,1:5]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus.iloc[:,5:]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus_X.astype(float)\n",
    "del df_train_trans_2_plus\n",
    "\n",
    "list_train_trans_1_only_id=df_train_trans_1_only.iloc[:,0].values.tolist()\n",
    "df_train_trans_1_only_Y=df_train_trans_1_only.iloc[:,1:5]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only.iloc[:,5:]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only_X.astype(float)\n",
    "\n",
    "del df_train_trans_1_only\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_low_variance columns\n",
    "\n",
    "r_variance=0.98\n",
    "threshold_variance_iv=r_variance*(1-r_variance)\n",
    "# df_train_trans_2_plus_X\n",
    "selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "df_redused_X=selector.fit_transform(df_train_trans_2_plus_X)\n",
    "print(\"df_train_trans_2_plus_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus_X.iloc[:,indices]\n",
    "\n",
    "# df_train_trans_1_only_X\n",
    "selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "df_redused_X=selector.fit_transform(df_train_trans_1_only_X)\n",
    "print(\"df_train_trans_1_only_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only_X.iloc[:,indices]\n",
    "\n",
    "del df_redused_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high correlated cols\n",
    "\n",
    "def remove_cols_with_high_coor(df_X,coorelation_threshold):\n",
    "    df_coor_X=df_X.corr().abs()\n",
    "    df_coor_X=df_coor_X.unstack().reset_index()\n",
    "    df_coor_X.columns=['iv_1','iv_2','coor']\n",
    "\n",
    "    df_coor_X_high=df_coor_X[df_coor_X['iv_1']!=df_coor_X['iv_2']]\n",
    "    df_coor_X_high=df_coor_X_high[df_coor_X_high['coor']>coorelation_threshold]\n",
    "    df_coor_X_high['high_coor_pairs']=df_coor_X_high[['iv_1','iv_2']].values.tolist()\n",
    "\n",
    "    list_highly_pairs=df_coor_X_high['high_coor_pairs'].tolist()\n",
    "    list_highly_pairs=[sorted(x) for x in list_highly_pairs]\n",
    "\n",
    "    list_highly_pairs=[str(x) for x in list_highly_pairs]\n",
    "    list_highly_pairs=list(set(list_highly_pairs))\n",
    "    list_highly_pairs=[eval(x) for x in list_highly_pairs]\n",
    "    \n",
    "    list_col_keep_in_priority=['trans_in_store','total_items','total_trans_since_registration']\n",
    "    list_cols_to_remove=[]\n",
    "    list_cols_to_keep=[]\n",
    "\n",
    "    def remove_p_with_v(list_all_pairs,v_remove):\n",
    "        for p in list_all_pairs:\n",
    "            if v_remove in p:\n",
    "                list_all_pairs.remove(p)\n",
    "        return list_all_pairs\n",
    "    def remaining_unique_list(list_all_pairs):\n",
    "        res=[]\n",
    "        for x in list_all_pairs:\n",
    "            res.extend(x)\n",
    "        res=list(set(res))\n",
    "        return res\n",
    "    def update_paired_list_in_priority(l1_to_keep_priority,l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_unique=remaining_unique_list(l2_all_for_now)\n",
    "        for v_keep in l1_to_keep_priority:\n",
    "            if v_keep in list_unique:\n",
    "                l4_keep_for_now.append(v_keep)\n",
    "                list_removed_due_to_vkeep=[]\n",
    "                for p in l2_all_for_now:\n",
    "                    if v_keep in p:\n",
    "                        v_remove=[x for x in p if x!=v_keep][0]\n",
    "                        list_removed_due_to_vkeep.append(v_remove)\n",
    "                list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "                if len(list_removed_due_to_vkeep)>0:\n",
    "                    for v_remove in list_removed_due_to_vkeep:\n",
    "                        l3_remove_for_now.append(v_remove)\n",
    "                        l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                        if v_remove in l1_to_keep_priority:\n",
    "                            l1_to_keep_priority.remove(v_remove)\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now=list(set(l4_keep_for_now))\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "\n",
    "    def update_paired_list_v_total(l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_keep_unique_total=[]\n",
    "        for p in l2_all_for_now:\n",
    "            for v in p:\n",
    "                if \"total\" in v:\n",
    "                    list_keep_unique_total.append(v)\n",
    "        list_keep_unique_total=list(set(list_keep_unique_total))\n",
    "        for v_keep in list_keep_unique_total:\n",
    "            list_removed_due_to_vkeep=[]\n",
    "            for p in l2_all_for_now:\n",
    "                if v_keep in p:\n",
    "                    v_remove=[x for x in p if x!=v_keep][0]\n",
    "                    list_removed_due_to_vkeep.append(v_remove)\n",
    "            list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "            if len(list_removed_due_to_vkeep)>0:\n",
    "                for v_remove in list_removed_due_to_vkeep:\n",
    "                    l3_remove_for_now.append(v_remove)\n",
    "                    l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                    if v_remove in list_keep_unique_total:\n",
    "                        list_keep_unique_total.remove(v_remove)\n",
    "\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now.extend(list_keep_unique_total)\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "    def remove_remaining_arbitrary(l2_all_for_now,l3_remove_for_now):\n",
    "        list_remove_arbitrary=[]\n",
    "        if len(l2_all_for_now)==0:\n",
    "            return l2_all_for_now,l3_remove_for_now\n",
    "        while len(l2_all_for_now)>0:\n",
    "            for p in l2_all_for_now:\n",
    "                v_remove=p[0]\n",
    "                list_remove_arbitrary.append(v_remove)\n",
    "                l2_all_for_now.remove(p)\n",
    "                for p2 in l2_all_for_now:\n",
    "                    if v_remove in p2:\n",
    "                        l2_all_for_now.remove(p2)\n",
    "        l3_remove_for_now.extend(list_remove_arbitrary)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        return l2_all_for_now,l3_remove_for_now \n",
    "    \n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_in_priority(l1_to_keep_priority=list_col_keep_in_priority,\n",
    "                                                 l2_all_for_now=list_highly_pairs,\n",
    "                                                 l3_remove_for_now=list_cols_to_remove,\n",
    "                                                 l4_keep_for_now=list_cols_to_keep\n",
    "                                                )\n",
    "\n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_v_total(\n",
    "                                                     l2_all_for_now=list_highly_pairs,\n",
    "                                                     l3_remove_for_now=list_cols_to_remove,\n",
    "                                                     l4_keep_for_now=list_cols_to_keep\n",
    "                                                    )\n",
    "\n",
    "    list_highly_pairs,list_cols_to_remove=remove_remaining_arbitrary(l2_all_for_now=list_highly_pairs,\n",
    "                                                                     l3_remove_for_now=list_cols_to_remove)\n",
    "    \n",
    "    for col in list_cols_to_remove:\n",
    "        del df_X[col]\n",
    "        print(col, \"removed due to high coor with others\")\n",
    "    return df_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coorelation_threshold=0.8\n",
    "\n",
    "print(datetime.datetime.now(),\"Start remove correlated cols: df_train_trans_2_plus_X\",df_train_trans_2_plus_X.shape)\n",
    "df_train_trans_2_plus_X=remove_cols_with_high_coor(df_X=df_train_trans_2_plus_X,coorelation_threshold=coorelation_threshold)\n",
    "print(datetime.datetime.now(),\"Done remove correlated cols: df_train_trans_2_plus_X\",df_train_trans_2_plus_X.shape)\n",
    "###### \n",
    "print(datetime.datetime.now(),\"Start remove correlated cols: df_train_trans_1_only_X\",df_train_trans_1_only_X.shape)\n",
    "df_train_trans_1_only_X=remove_cols_with_high_coor(df_X=df_train_trans_1_only_X,coorelation_threshold=coorelation_threshold)\n",
    "print(datetime.datetime.now(),\"Done remove correlated cols: df_train_trans_1_only_X\",df_train_trans_1_only_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_type={\n",
    "    \"trans_1_only\":{\n",
    "        \"df_X\":df_train_trans_1_only_X,\n",
    "        \"df_Y\":df_train_trans_1_only_Y,\n",
    "        \"list_id\":list_train_trans_1_only_id\n",
    "    },\n",
    "    \"trans_2_plus\":{\n",
    "        \"df_X\":df_train_trans_2_plus_X,\n",
    "        \"df_Y\":df_train_trans_2_plus_Y,\n",
    "        \"list_id\":list_train_trans_2_plus_id\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv_start_date\n",
    "iv_end_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()\n",
    "iv_start_date=iv_end_date-datetime.timedelta(days=int(np.ceil(365*recent_n_month/12)))\n",
    "\n",
    "dv_end_date=iv_end_date+datetime.timedelta(days=28)\n",
    "dv_start_date=iv_end_date+datetime.timedelta(days=1)  \n",
    "df_date_range=pd.DataFrame({\"start\":[iv_start_date,dv_start_date],\"end\":[iv_end_date,dv_end_date]},index=['IVs',\"DVs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_above_5pctg_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "    \n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "    f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "    \n",
    "    score=(9*tp-8*fn*(1-f1_score)-fp)*accuracy\n",
    "    # the score ignored the f1 and overall accuracy due to low pctg\n",
    "    \n",
    "    \n",
    "    \n",
    "    # consider the profit vs lost 10:1 (30%*$30) vs (cpc*frequecy or click)\n",
    "    # which means 1 missed (fn) is 10 times of 1 wrong targeted (fp)\n",
    "    # very aribitury\n",
    "    return score\n",
    "\n",
    "def scoring_below_5pctg_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "    \n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "    \n",
    "    r_1=tn/(tn+fp)\n",
    "    r_2=tp/(tp+fn)\n",
    "    \n",
    "    # consider the 2 pctgs that matter\n",
    "    score=(1+r_1)*(1+r_2*(1+pctg))\n",
    "    '''\n",
    "    score=tp*4*(1-pctg*3)-fn*pctg*4-fp*0.1*(1-pctg)+tn*pctg*0.025 # just work for this\n",
    "    # 4=8*0.5 as the benefit * the posible inherit purchase rate\n",
    "    # the false negative is the missed should be getting benefit, but the pctg is the one that most will be 0s\n",
    "    # the false positive is only the one that spend money wrong, ~0.25 cost per reach on FB - 3 weeks, 4 times adjust\n",
    "    # true negative ignored\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return score\n",
    "\n",
    "def write_aggregate_func_gain_chart(list_selected_features,df_pred_table_detail):\n",
    "    func_dict={\"customer_id_hashed\":\"count\"}\n",
    "    list_cols_for_ratios=['y_true','y_hat']\n",
    "    for col in list_selected_features:\n",
    "        if len(df_pred_table_detail[col].unique())==2:\n",
    "            func_dict.update({col:'sum'})\n",
    "            list_cols_for_ratios.append(col)\n",
    "        else:\n",
    "            func_dict.update({col:\"mean\"})\n",
    "    func_dict.update({\"y_true\":\"sum\"})\n",
    "    func_dict.update({\"y_hat\":\"sum\"})\n",
    "    # func_dict.update({\"pred_prob\":['max','min']})\n",
    "    return func_dict,list_cols_for_ratios\n",
    "\n",
    "\n",
    "def generate_gain_chart_function(df_X,list_y,list_ids,result_sm_model,threshold,list_selected_features):\n",
    "    list_pred_prob=result_sm_model.predict(sm.add_constant(df_X)).values\n",
    "    df_pred_by_id=pd.DataFrame({\"customer_id_hashed\":list_ids,\"pred_prob\":list_pred_prob},index=range(len(list_pred_prob)))\n",
    "    copy_xtrain=df_X.copy().reset_index()\n",
    "    del copy_xtrain['index']\n",
    "    df_pred_by_id=pd.concat([df_pred_by_id,copy_xtrain],axis=1,ignore_index=False)\n",
    "\n",
    "    df_pred_by_id['decile']=pd.qcut(df_pred_by_id['pred_prob'], 10, labels=False)\n",
    "    df_pred_by_id['decile']=df_pred_by_id['decile'].apply(lambda x: \"D\"+str(x+1).zfill(2))\n",
    "    df_pred_by_id['y_true']=list_y\n",
    "    df_pred_by_id['y_hat']=np.where(df_pred_by_id['pred_prob']>threshold,1,0)\n",
    "\n",
    "    agg_func,list_cols_to_get_ratio=write_aggregate_func_gain_chart(list_selected_features,df_pred_by_id)\n",
    "    df_gainchart=df_pred_by_id.groupby(\"decile\")[['customer_id_hashed']+list_selected_features].agg(agg_func).reset_index()\n",
    "\n",
    "    df_prob_max=df_pred_by_id.groupby(\"decile\")['pred_prob'].max().to_frame().reset_index().rename(columns={\"pred_prob\":\"max_prob\"})\n",
    "    df_prob_min=df_pred_by_id.groupby(\"decile\")['pred_prob'].min().to_frame().reset_index().rename(columns={\"pred_prob\":\"min_prob\"})\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_max,on=\"decile\")\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_min,on=\"decile\")\n",
    "    df_gainchart.insert(2,\"actual_ratio\",df_gainchart['y_true']/df_gainchart['customer_id_hashed'])\n",
    "    df_gainchart.insert(3,\"pred_ratio\",df_gainchart['y_hat']/df_gainchart['customer_id_hashed'])\n",
    "\n",
    "\n",
    "    df_gainchart.insert(4,\"max_pred_prob\",df_gainchart['max_prob'])\n",
    "    df_gainchart.insert(5,\"min_pred_prob\",df_gainchart['min_prob'])\n",
    "    del df_gainchart['max_prob']\n",
    "    del df_gainchart['min_prob']\n",
    "\n",
    "    return df_gainchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SM_Logistic_Model_dvN:\n",
    "    # 1\n",
    "    def __init__(self,n_week_DV,key_df_type,df_date_range,sql_engine=BL_engine,dict_cols_in_table=dict_cols_in_table):\n",
    "\n",
    "# n_week_DV: 1-4\n",
    "# key_df_type: \"trans_1_only\" or \"trans_2_plus\" in the keys of dict_df_type\n",
    "# df_date_range: defined global df -- df_date_range\n",
    "# sql_engine: mysql engine to BigLots database\n",
    "\n",
    "        self.n_week_DV=n_week_DV\n",
    "        self.dict_cols_in_table=dict_cols_in_table\n",
    "        self.key_df_type=key_df_type\n",
    "        self.sql_engine=BL_engine\n",
    "        self.df_train_X=dict_df_type[key_df_type]['df_X']\n",
    "        self.X_train_scaled=scale(self.df_train_X) # will be wroten later in #2, 3 & 4\n",
    "\n",
    "        self.df_train_Y=dict_df_type[key_df_type]['df_Y']\n",
    "        self.input_y_train_list=self.df_train_Y[\"DV_cumulative_week_updated_%i\"%n_week_DV].values.tolist()\n",
    "        \n",
    "        self.list_ids_y_train=dict_df_type[key_df_type]['list_id']\n",
    "        \n",
    "        self.X_features=self.df_train_X.columns.tolist()\n",
    "\n",
    "        self.df_test_X=pd.DataFrame()\n",
    "        self.df_test_Y=pd.DataFrame()\n",
    "        self.df_test_id=pd.DataFrame()\n",
    "        self.input_y_test_list=[]\n",
    "        \n",
    "        self.df_date_range=df_date_range\n",
    "        \n",
    "        self.db_row_counts=pd.DataFrame({\"records\":self.df_train_X.shape[0],\"IVs\":self.df_train_X.shape[1]},index=[\"X_train\"])        \n",
    "        self.df_y_train_count=pd.DataFrame()\n",
    "        self.df_y_test_count=pd.DataFrame()\n",
    "        self.pctg=None\n",
    "        self.threshold_max_selfdefinedscore=None\n",
    "        self.df_step_table=pd.DataFrame()\n",
    "        self.df_confusion_table=pd.DataFrame()\n",
    "        self.df_gainchart_train=pd.DataFrame()\n",
    "        self.df_gainchart_test=pd.DataFrame()\n",
    "        \n",
    "        self.df_train_ids_labeled_summary=pd.DataFrame()\n",
    "        self.df_test_ids_labeled_summary=pd.DataFrame()\n",
    "        \n",
    "        self.df_train_ids_labeled=pd.DataFrame()\n",
    "        self.df_test_ids_labeled=pd.DataFrame()\n",
    "        \n",
    "        \n",
    "        self.output_folder=\"./output_No_DCM_%s_%s/\"%(str(self.df_date_range.iloc[0,1]),str(datetime.datetime.now().date()))\n",
    "        \n",
    "        try:\n",
    "            os.stat(self.output_folder)\n",
    "        except:\n",
    "            os.mkdir(self.output_folder)\n",
    "            \n",
    "        self.output_path=self.output_folder+\"BL_LRModeling_NoDCM_%s_DV%s_%s_JL_%s.xlsx\"%(key_df_type,str(n_week_DV),str(self.df_date_range.iloc[0,1]),str(datetime.datetime.now()))\n",
    "        self.df_department_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",\n",
    "                                              sep=\"|\").drop_duplicates()\n",
    "    # 2\n",
    "    def select_from_model_n_features(self, N_feature_select_from_models):\n",
    "        print(\"Starting select_from_model_n_features: \",datetime.datetime.now())\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(random_state=0,\n",
    "                                                                solver=\"saga\",\n",
    "                                                                max_iter=2000,\n",
    "                                                                n_jobs=24,\n",
    "                                                                tol=0.0001),\n",
    "                                   max_features=N_feature_select_from_models,\n",
    "                                   threshold=-np.inf).fit(self.X_train_scaled, self.input_y_train_list)\n",
    "\n",
    "        print(\"selector.threshold_\",selector.threshold_)\n",
    "        selector_support_FROMMODEL=selector.get_support()\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_FROMMODEL) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        print(\"Done select_from_model_n_features %d: \"%N_feature_select_from_models,datetime.datetime.now())\n",
    "        \n",
    "    #3\n",
    "    def select_REF(self,n_features_to_select):\n",
    "        print(\"Starting select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        estimator = LogisticRegression(fit_intercept=True,solver='saga',max_iter=2000,n_jobs=24,tol=0.001)\n",
    "        selector = RFE(estimator,step=1,n_features_to_select=n_features_to_select)\n",
    "        selector = selector.fit(self.X_train_scaled, self.input_y_train_list)\n",
    "        selector_support_REF=selector.support_\n",
    "        print(\"Done select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_REF) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "    \n",
    "    #4\n",
    "    def forwards_feature_elimination_based_on_p_and_vif(self,niter=50,method=\"lbfgs\",p_tol=0.1,vif_tol=5):\n",
    "        len_x_features=self.df_train_X.shape[1]\n",
    "        len_x_features_new=0\n",
    "        df_x_dropped=self.df_train_X.copy()\n",
    "        i_iter=0\n",
    "        while len_x_features_new<len_x_features and i_iter<=100:\n",
    "            i_iter+=1\n",
    "            len_x_features=df_x_dropped.shape[1]\n",
    "            mod=sm.Logit(self.input_y_train_list,sm.add_constant(df_x_dropped),niter=niter,method=method)\n",
    "            res=mod.fit()\n",
    "            table=res.summary2().tables[1]   \n",
    "            X=add_constant(scale(df_x_dropped))\n",
    "            list_cols=table.index.tolist()\n",
    "            table[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "\n",
    "            max_vif=table['VIF Factor'].max()\n",
    "            max_p=table['P>|z|'].max()\n",
    "            \n",
    "            if max_vif>vif_tol:\n",
    "                col_name_to_drop=table.index[table['VIF Factor']==max_vif][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to high vif\"%col_name_to_drop)\n",
    "                \n",
    "            elif max_p>p_tol:\n",
    "                col_name_to_drop=table.index[table['P>|z|']==max_p][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to p value\"%col_name_to_drop)\n",
    "            else:\n",
    "                i_iter+=100\n",
    "                \n",
    "        self.df_train_X=df_x_dropped\n",
    "        self.X_features=df_x_dropped.columns.tolist()\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "\n",
    "        \n",
    "    # 5\n",
    "    def run_sm_logR_model(self):\n",
    "        self.sm_model=sm.Logit(self.input_y_train_list,sm.add_constant(self.df_train_X),niter=50,method=\"lbfgs\")\n",
    "        self.res_of_model=self.sm_model.fit()\n",
    "        self.summary_table_over=self.res_of_model.summary2().tables[0].reset_index()\n",
    "        self.summary_table_output=self.res_of_model.summary2().tables[1].reset_index()\n",
    "        \n",
    "        std=self.sm_model.exog.std(axis=0)\n",
    "        std[0] = 1\n",
    "        tt = self.res_of_model.t_test(np.diag(std))\n",
    "        df_std_coef=tt.summary_frame()\n",
    "        list_std_coefficients=df_std_coef['coef'].tolist()\n",
    "        self.summary_table_output['std_coef']=list_std_coefficients\n",
    "        \n",
    "        self.list_train_pred=self.res_of_model.predict()\n",
    "        # \n",
    "        \n",
    "        coefficient_of_dermination = r2_score(self.input_y_train_list, self.list_train_pred)\n",
    "        self.summary_table_over=self.summary_table_over.append(pd.DataFrame({\"index\":[8],0:\"calculated_r_squared\",1:coefficient_of_dermination},index=[8]))\n",
    "    \n",
    "        #VIF\n",
    "        X=add_constant(self.X_train_scaled)\n",
    "        list_cols=self.summary_table_output['index'].tolist()\n",
    "        self.summary_table_output[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        self.summary_table_output=self.summary_table_output.sort_values(\"std_coef\")\n",
    "\n",
    "    # 6\n",
    "    def select_test_df_from_mysql(self):\n",
    "        cols_in_X=self.summary_table_output.iloc[:,0].values.tolist()\n",
    "        cols_in_X.remove(\"const\")\n",
    "        self.X_features=cols_in_X\n",
    "        \n",
    "        table_name_t0=self.dict_cols_in_table['t0_test']['table_name']\n",
    "        col_list_t0=self.dict_cols_in_table['t0_test']['cols']\n",
    "        \n",
    "        table_name_t1=self.dict_cols_in_table['t1']['table_name']\n",
    "        col_list_t1=self.dict_cols_in_table['t1']['cols']\n",
    "        \n",
    "        table_name_t2_1=self.dict_cols_in_table['t2_1']['table_name']\n",
    "        col_list_t2_1=self.dict_cols_in_table['t2_1']['cols']\n",
    "        \n",
    "        table_name_t2_2=self.dict_cols_in_table['t2_2']['table_name']\n",
    "        col_list_t2_2=self.dict_cols_in_table['t2_2']['cols']\n",
    "        \n",
    "        col_list_t1=[x for x in col_list_t1 if x in cols_in_X]\n",
    "        col_list_t2_1=[x for x in col_list_t2_1 if x in cols_in_X]\n",
    "        col_list_t2_2=[x for x in col_list_t2_2 if x in cols_in_X]\n",
    "        \n",
    "        \n",
    "        sql_str_cols_df0_test=str([\"t0.\"+x for x in col_list_t0]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list=[sql_str_cols_df0_test]\n",
    "        list_tables=[\"t0\"]\n",
    "        col_list_dv=[x for x in self.df_train_Y.columns.tolist()]\n",
    "        sql_str_cols_dv=str([\"t1.\"+x for x in col_list_dv]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list.append(sql_str_cols_dv)\n",
    "        if len(col_list_t1)>0:\n",
    "            sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_t1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_df_1)\n",
    "            list_tables.append(\"t1\")\n",
    "        if len(col_list_t2_1)>0:\n",
    "            sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_t2_1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_1)\n",
    "            list_tables.append(\"t2_1\")\n",
    "        if len(col_list_t2_2)>0:\n",
    "            sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_t2_2]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_2)\n",
    "            list_tables.append(\"t2_2\")\n",
    "        str_cols_test=\", \".join(list_query_col_list)\n",
    "        select_clause=\"SELECT %s from %s as t0\"%(str_cols_test,table_name_t0)\n",
    "        \n",
    "        list_of_join_clause=[]\n",
    "        if \"t1\" in list_tables:\n",
    "            str_join_clause_t1=\"left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed\"%table_name_t1\n",
    "            list_of_join_clause.append(str_join_clause_t1)\n",
    "        if \"t2_1\" in list_tables:\n",
    "            str_join_clause_t2_1=\"left join %s as t2_1 on t0.customer_id_hashed=t2_1.id\"%table_name_t2_1\n",
    "            list_of_join_clause.append(str_join_clause_t2_1)\n",
    "        if \"t2_2\" in list_tables:\n",
    "            str_join_clause_t2_2=\"left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%table_name_t2_2\n",
    "            list_of_join_clause.append(str_join_clause_t2_2)\n",
    "            \n",
    "        if self.key_df_type==\"trans_1_only\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is null\"\n",
    "        elif self.key_df_type==\"trans_2_plus\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is not null\"\n",
    "        else:\n",
    "            where_clause=\"\"\n",
    "            print(\"key_df_type not specified, please choose either trans_1_only or trans_2_plus\")\n",
    "        query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]).strip() \n",
    "        print(query_full)\n",
    "        df=pd.read_sql(query_full,con=self.sql_engine)\n",
    "        if \"nearest_BL_dist\" in df.columns.tolist():\n",
    "            df=df[pd.notnull(df['nearest_BL_dist'])]\n",
    "        for col in df.columns.tolist():\n",
    "            df_nan=df[pd.isnull(df[col])]\n",
    "            if df_nan.shape[0]>0:\n",
    "                raise ValueError(\"%s in the selected test df is null\"%col)\n",
    "                \n",
    "        cols_Y=[x for x in df.columns.tolist() if \"cumulative\" in x]\n",
    "        self.df_test_Y=df[cols_Y]\n",
    "        for col in cols_Y:\n",
    "            del df[col]\n",
    "        self.list_ids_y_test=df['customer_id_hashed'].values.tolist()\n",
    "        del df['customer_id_hashed']\n",
    "        self.input_y_test_list=self.df_test_Y[\"DV_cumulative_week_updated_%i\"%self.n_week_DV].values.tolist()     \n",
    "        self.df_test_X=df\n",
    "        self.X_teest_scaled=scale(self.df_test_X)\n",
    "    # 7\n",
    "    def run_updating_df_count(self):\n",
    "        df_test_X_count=pd.DataFrame({\"records\":self.df_test_X.shape[0],\"IVs\":self.df_test_X.shape[1]},index=[\"X_test\"])\n",
    "        self.db_row_counts=self.db_row_counts.append(df_test_X_count)\n",
    "\n",
    "    # 8\n",
    "    def generate_DV_distribution(self):\n",
    "        df_y_train_count=pd.DataFrame()\n",
    "        for col in self.df_train_Y.columns.tolist():\n",
    "            count_1=self.df_train_Y[self.df_train_Y[col]==1].shape[0]\n",
    "            count_0=self.df_train_Y[self.df_train_Y[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_train_count=df_y_train_count.append(df)\n",
    "        df_y_train_count.insert(0,\"set\",\"y_train\")\n",
    "\n",
    "\n",
    "        df_y_test_count=pd.DataFrame()\n",
    "        for col in self.df_test_Y.columns.tolist():\n",
    "            count_1=self.df_test_Y[self.df_test_Y[col]==1].shape[0]\n",
    "            count_0=self.df_test_Y[self.df_test_Y[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_test_count=df_y_test_count.append(df)\n",
    "        df_y_test_count.insert(0,\"set\",\"y_test\")\n",
    "\n",
    "        self.df_y_train_count=df_y_train_count\n",
    "        self.df_y_test_count=df_y_test_count\n",
    "        self.pctg=(sum(self.input_y_train_list)+sum(self.input_y_test_list))/(len(self.input_y_train_list)+len(self.input_y_test_list))\n",
    "        \n",
    "    # 9\n",
    "    def pred_test_Y(self):\n",
    "        self.list_test_pred=self.res_of_model.predict(sm.add_constant(self.df_test_X)).tolist()\n",
    "        \n",
    "    # 10\n",
    "    def generate_step_table_of_test_SM(self,):\n",
    "        if self.pctg>=0.05:\n",
    "            threshold_list = [(x+1)/100 for x in range(0,100)] \n",
    "        else:\n",
    "            start_prob_pctg=max(0.001,int(np.floor((self.pctg-0.02)*100))/100)\n",
    "            end_prob_pctg=int(np.floor((self.pctg+0.02)*100))/100\n",
    "            threshold_list = [(x+1)/1000 for x in range(int(start_prob_pctg*1000),int(end_prob_pctg*1000))]\n",
    "            \n",
    "        list_prob_test=self.list_test_pred\n",
    "        df_output=pd.DataFrame()\n",
    "        for i in threshold_list:\n",
    "            y_test_pred=[1 if x>i else 0 for x in list_prob_test]\n",
    "\n",
    "            accuracy_score = metrics.accuracy_score(self.input_y_test_list,y_test_pred)    \n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(self.input_y_test_list, y_test_pred).ravel()\n",
    "            # \n",
    "            TPR=tp/(tp+fn) #recall\n",
    "            FNR=fn/(tp+fn)\n",
    "            FPR=fp/(fp+tn)\n",
    "            TNR=tn/(fp+tn)\n",
    "\n",
    "            PPV=tp/(tp+fp) #precission\n",
    "            f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "            df=pd.DataFrame({\"predicted_positive\":len([x for x in y_test_pred if x==1]),\n",
    "                             \"predicted_negative\":len([x for x in y_test_pred if x==0]),\n",
    "                             \"accuracy_score\":accuracy_score,\n",
    "                             'true_negative':tn,\n",
    "                             'false_positive':fp,\n",
    "                             'false_negative':fn,\n",
    "                             'true_positive':tp,\n",
    "                             'true_positive_rate':TPR,\n",
    "                             'false_negative_rate':FNR,\n",
    "                             'false_positive_rate':FPR,\n",
    "                             'true_negative_rate':TNR,\n",
    "                             'precission_(Positive predictive value)':PPV,\n",
    "                             'f1_score':f1_score\n",
    "                            },index=[i])\n",
    "            df_output=df_output.append(df)\n",
    "\n",
    "        self.df_step_table=df_output\n",
    "        \n",
    "    # 11\n",
    "    def select_best_scored_pred_prob(self):\n",
    "        \n",
    "        if self.pctg>=0.05:\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_above_5pctg_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)\n",
    "        else:\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_below_5pctg_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)\n",
    "            \n",
    "        threshold_max_selfdefinedscore=self.df_step_table[self.df_step_table['self_defined_score']==self.df_step_table['self_defined_score'].max()].index[0]\n",
    "        self.threshold_max_selfdefinedscore=threshold_max_selfdefinedscore\n",
    "        print(\"threshold_max_selfdefinedscore\",threshold_max_selfdefinedscore)\n",
    "        self.df_step_table=self.df_step_table.reset_index()\n",
    "        self.df_confusion_table=self.df_step_table.loc[self.df_step_table['index']==threshold_max_selfdefinedscore,:]\n",
    "\n",
    "    # 12\n",
    "    def generate_gain_chart(self):\n",
    "        self.df_gainchart_train=generate_gain_chart_function(df_X=self.df_train_X,\n",
    "                                                             list_y=self.input_y_train_list,\n",
    "                                                             list_ids=self.list_ids_y_train,\n",
    "                                                             result_sm_model=self.res_of_model,\n",
    "                                                             threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                             list_selected_features=self.X_features)\n",
    "        \n",
    "        self.df_gainchart_test=generate_gain_chart_function(df_X=self.df_test_X,\n",
    "                                                            list_y=self.input_y_test_list,\n",
    "                                                            list_ids=self.list_ids_y_test,\n",
    "                                                            result_sm_model=self.res_of_model,\n",
    "                                                            threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                            list_selected_features=self.X_features)\n",
    "        \n",
    "    # 13\n",
    "    def check_shopper_type(self):\n",
    "        recent_4_week_sign_up_end_dt=self.df_date_range.iloc[0,1]\n",
    "        recent_4_week_sign_up_start_dt=recent_4_week_sign_up_end_dt-datetime.timedelta(days=27)\n",
    "        str_start_sign_up=\"'\"+str(recent_4_week_sign_up_start_dt)+\"'\"\n",
    "        str_end_sign_up=\"'\"+str(recent_4_week_sign_up_end_dt)+\"'\"\n",
    "        print(\"new sign up date range below: \\n\",recent_4_week_sign_up_start_dt,recent_4_week_sign_up_end_dt)\n",
    "\n",
    "        df_recent_4_week_new_sings=pd.read_sql(\"select customer_id_hashed from BL_Rewards_Master where sign_up_date between %s and %s\"%(str_start_sign_up,str_end_sign_up),con=BL_engine)\n",
    "        df_recent_4_week_new_sings=df_recent_4_week_new_sings.drop_duplicates()\n",
    "        df_recent_4_week_new_sings['sign_up_label']=\"new_signs\"\n",
    "        # \n",
    "        df_train_ids_labeled=pd.DataFrame({\"y_hat\":self.list_train_pred},index=self.list_ids_y_train).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_train_ids_labeled['selection_label']=np.where(df_train_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_train_ids_labeled=pd.merge(df_train_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_train_ids_labeled['sign_up_label']=df_train_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_train_ids_labeled['actual_shopping_label']=self.input_y_train_list\n",
    "        df_train_ids_labeled['actual_shopping_label']=df_train_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "        \n",
    "        self.df_train_ids_labeled_summary=df_train_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_train_ids_labeled=df_train_ids_labeled\n",
    "        \n",
    "        # \n",
    "        df_test_ids_labeled=pd.DataFrame({\"y_hat\":self.list_test_pred},index=self.list_ids_y_test).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_test_ids_labeled['selection_label']=np.where(df_test_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_test_ids_labeled=pd.merge(df_test_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_test_ids_labeled['sign_up_label']=df_test_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_test_ids_labeled['actual_shopping_label']=self.input_y_test_list\n",
    "        df_test_ids_labeled['actual_shopping_label']=df_test_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "        \n",
    "        self.df_test_ids_labeled_summary=df_test_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_test_ids_labeled=df_test_ids_labeled        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # 14\n",
    "    def save_outputs(self):\n",
    "        \n",
    "        writer=pd.ExcelWriter(self.output_path,engine=\"xlsxwriter\")\n",
    "        \n",
    "        self.db_row_counts.to_excel(writer,\"df_dataset_shape\")\n",
    "        self.df_date_range.to_excel(writer,\"df_date_range\")\n",
    "        self.df_y_train_count.to_excel(writer,\"df_y_train_count\")\n",
    "        self.df_y_test_count.to_excel(writer,\"df_y_test_count\")\n",
    "        self.summary_table_over.to_excel(writer,\"summary_table_over\")\n",
    "        self.summary_table_output.to_excel(writer,\"summary_table_output\")\n",
    "        self.df_step_table.to_excel(writer,\"step_table\",index=True)\n",
    "        self.df_confusion_table.to_excel(writer,\"select_score_matrix\",index=False)\n",
    "        \n",
    "        self.df_gainchart_train.to_excel(writer,\"gainchart_train\",index=False)\n",
    "        self.df_gainchart_test.to_excel(writer,\"gainchart_test\",index=False)\n",
    "        self.df_department_name.to_excel(writer,\"department_name\",index=False)\n",
    "        \n",
    "        self.df_train_ids_labeled_summary.to_excel(writer,\"train_id_summary\",index=False)\n",
    "        self.df_test_ids_labeled_summary.to_excel(writer,\"test_id_summary\",index=False)\n",
    "        \n",
    "        writer.save()\n",
    "        str_high_date=str(self.df_date_range.iloc[0,1])\n",
    "        str_dv_type=\"DV%i_%s\"%(self.n_week_DV,self.key_df_type)\n",
    "        self.df_train_ids_labeled.to_csv(self.output_folder+\"df_train_ids_labeled_%s_%s.csv\"%(str_high_date,str_dv_type),index=False)\n",
    "        self.df_test_ids_labeled.to_csv(self.output_folder+\"df_test_ids_labeled_%s_%s.csv\"%(str_high_date,str_dv_type),index=False)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_1_only DV 3 start:  2020-08-15 18:35:47.595065\n",
      "Starting select_from_model_n_features:  2020-08-15 18:35:48.178376\n",
      "selector.threshold_ -inf\n",
      "df_train_X.shape (175877, 55)\n",
      "X_train_scaled.shape (175877, 55)\n",
      "Done select_from_model_n_features 55:  2020-08-15 18:41:43.399115\n",
      "Starting select_REF:  2020-08-15 18:41:43.400008\n",
      "Done select_REF:  2020-08-15 18:43:30.390335\n",
      "df_train_X.shape (175877, 40)\n",
      "X_train_scaled.shape (175877, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151028\n",
      "         Iterations 8\n",
      "(175877, 39) column total_items dropped due to high vif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151034\n",
      "         Iterations 8\n",
      "(175877, 38) column department_110_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151034\n",
      "         Iterations 8\n",
      "(175877, 37) column department_111_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151036\n",
      "         Iterations 8\n",
      "(175877, 36) column department_353_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151038\n",
      "         Iterations 8\n",
      "(175877, 35) column department_410_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151041\n",
      "         Iterations 8\n",
      "(175877, 34) column department_410_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151042\n",
      "         Iterations 8\n",
      "(175877, 33) column department_310_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151045\n",
      "         Iterations 8\n",
      "(175877, 32) column department_360_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151049\n",
      "         Iterations 8\n",
      "(175877, 31) column department_170_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151052\n",
      "         Iterations 8\n",
      "(175877, 30) column department_550_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151056\n",
      "         Iterations 8\n",
      "(175877, 29) column department_351_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151060\n",
      "         Iterations 8\n",
      "(175877, 28) column department_114_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151064\n",
      "         Iterations 8\n",
      "(175877, 27) column weeks_since_sign_up dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151070\n",
      "         Iterations 8\n",
      "(175877, 26) column department_355_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151077\n",
      "         Iterations 8\n",
      "(175877, 25) column department_355_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151079\n",
      "         Iterations 8\n",
      "(175877, 24) column department_351_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151087\n",
      "         Iterations 8\n",
      "(175877, 23) column department_109_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151095\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.151095\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT t0.customer_id_hashed, t1.DV_cumulative_week_updated_1, t1.DV_cumulative_week_updated_2, t1.DV_cumulative_week_updated_3, t1.DV_cumulative_week_updated_4, t1.P_zip, t1.signed_online, t1.email_unsub_label, t1.nearest_BL_dist, t2_1.department_110_trans, t2_1.department_114_trans, t2_1.department_140_trans, t2_1.department_210_trans, t2_1.department_340_trans, t2_1.department_352_trans, t2_1.department_354_trans, t2_1.department_364_trans, t2_1.department_510_trans, t2_1.department_520_trans, t2_1.department_710_trans, t2_2.department_minus_one_1st_trans, t2_2.department_120_1st_trans, t2_2.department_160_1st_trans, t2_2.department_360_1st_trans, t2_2.department_520_1st_trans, t2_2.department_608_1st_trans, t2_2.department_612_1st_trans, t2_2.week_counts_to_now_recent_one from crm_table_id_list_test_20200704 as t0 left join table_pred_1_crm_up_to_20200704 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20200704 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20200704 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold_max_selfdefinedscore 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1455: FutureWarning: using a dict with renaming is deprecated and will be removed\n",
      "in a future version.\n",
      "\n",
      "For column-specific groupby renaming, use named aggregation\n",
      "\n",
      "    >>> df.groupby(...).agg(name=('column', aggfunc))\n",
      "\n",
      "  return super().aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sign up date range below: \n",
      " 2020-06-07 2020-07-04\n",
      "trans_1_only DV 3 done:  2020-08-15 18:48:53.662935\n"
     ]
    }
   ],
   "source": [
    "n_week_DV=3\n",
    "key_df_type=\"trans_1_only\"\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type,\n",
    "                               df_date_range=df_date_range,\n",
    "                               sql_engine=BL_engine,\n",
    "                               dict_cols_in_table=dict_cols_in_table\n",
    "                              )\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN,N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.pred_test_Y(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_gain_chart(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.check_shopper_type(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.save_outputs(SM_Logistic_Model_dvN)\n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_2_plus DV 2 start:  2020-08-15 18:48:53.677268\n",
      "Starting select_from_model_n_features:  2020-08-15 18:49:03.357001\n",
      "selector.threshold_ -inf\n",
      "df_train_X.shape (756648, 60)\n",
      "X_train_scaled.shape (756648, 60)\n",
      "Done select_from_model_n_features 60:  2020-08-15 18:58:17.628464\n",
      "Starting select_REF:  2020-08-15 18:58:17.630006\n",
      "Done select_REF:  2020-08-15 19:16:10.685867\n",
      "df_train_X.shape (756648, 40)\n",
      "X_train_scaled.shape (756648, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309609\n",
      "         Iterations 8\n",
      "(756648, 39) column trans_in_store dropped due to high vif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310795\n",
      "         Iterations 8\n",
      "(756648, 38) column department_120_recent_one dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310796\n",
      "         Iterations 8\n",
      "(756648, 37) column department_114_recent_one dropped due to p value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310796\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.310796\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT t0.customer_id_hashed, t1.DV_cumulative_week_updated_1, t1.DV_cumulative_week_updated_2, t1.DV_cumulative_week_updated_3, t1.DV_cumulative_week_updated_4, t1.P_zip, t1.signed_online, t1.email_unsub_label, t1.nearest_BL_dist, t2_1.weeks_since_sign_up, t2_1.unique_stores, t2_1.department_minus_one_trans, t2_1.department_111_trans, t2_1.department_114_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_150_trans, t2_1.department_170_trans, t2_1.department_360_trans, t2_1.department_364_trans, t2_1.department_530_trans, t2_1.department_710_trans, t2_2.total_sales_1st_trans, t2_2.department_360_1st_trans, t2_2.department_510_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.total_sales_recent_one_trans, t2_2.department_minus_one_recent_one, t2_2.department_110_recent_one, t2_2.department_210_recent_one, t2_2.department_340_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.total_sales_recent_two_trans, t2_2.department_minus_one_recent_two, t2_2.department_110_recent_two, t2_2.department_160_recent_two, t2_2.department_351_recent_two, t2_2.department_360_recent_two, t2_2.department_367_recent_two, t2_2.department_550_recent_two from crm_table_id_list_test_20200704 as t0 left join table_pred_1_crm_up_to_20200704 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20200704 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20200704 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is not null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:301: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n",
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold_max_selfdefinedscore 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1455: FutureWarning: using a dict with renaming is deprecated and will be removed\n",
      "in a future version.\n",
      "\n",
      "For column-specific groupby renaming, use named aggregation\n",
      "\n",
      "    >>> df.groupby(...).agg(name=('column', aggfunc))\n",
      "\n",
      "  return super().aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sign up date range below: \n",
      " 2020-06-07 2020-07-04\n",
      "trans_2_plus DV 2 done:  2020-08-15 19:34:35.552567\n"
     ]
    }
   ],
   "source": [
    "n_week_DV=2\n",
    "key_df_type=\"trans_2_plus\"\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type,\n",
    "                               df_date_range=df_date_range,\n",
    "                               sql_engine=BL_engine,\n",
    "                               dict_cols_in_table=dict_cols_in_table\n",
    "                              )\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN,N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.pred_test_Y(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_gain_chart(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.check_shopper_type(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.save_outputs(SM_Logistic_Model_dvN)\n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done: model building 2020-08-15 19:34:35.562873\n"
     ]
    }
   ],
   "source": [
    "print(\"Job done: model building\", datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
