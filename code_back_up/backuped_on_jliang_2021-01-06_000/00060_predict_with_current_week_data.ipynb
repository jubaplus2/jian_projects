{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "\n",
      "Start predict_with_current_week_data: 2020-12-31 11:23:47.284648\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import sqlalchemy\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "task_start_time=datetime.datetime.now()\n",
    "print(\"***************\\n\")\n",
    "print(\"Start predict_with_current_week_data: %s\"%str(task_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high correlated cols\n",
    "\n",
    "def remove_cols_with_high_coor(df_X,coorelation_threshold):\n",
    "    df_coor_X=df_X.corr().abs()\n",
    "    df_coor_X=df_coor_X.unstack().reset_index()\n",
    "    df_coor_X.columns=['iv_1','iv_2','coor']\n",
    "\n",
    "    df_coor_X_high=df_coor_X[df_coor_X['iv_1']!=df_coor_X['iv_2']]\n",
    "    df_coor_X_high=df_coor_X_high[df_coor_X_high['coor']>coorelation_threshold]\n",
    "    df_coor_X_high['high_coor_pairs']=df_coor_X_high[['iv_1','iv_2']].values.tolist()\n",
    "\n",
    "    list_highly_pairs=df_coor_X_high['high_coor_pairs'].tolist()\n",
    "    list_highly_pairs=[sorted(x) for x in list_highly_pairs]\n",
    "\n",
    "    list_highly_pairs=[str(x) for x in list_highly_pairs]\n",
    "    list_highly_pairs=list(set(list_highly_pairs))\n",
    "    list_highly_pairs=[eval(x) for x in list_highly_pairs]\n",
    "\n",
    "    list_col_keep_in_priority=['trans_in_store','total_items','total_trans_since_registration']\n",
    "    list_cols_to_remove=[]\n",
    "    list_cols_to_keep=[]\n",
    "\n",
    "    def remove_p_with_v(list_all_pairs,v_remove):\n",
    "        for p in list_all_pairs:\n",
    "            if v_remove in p:\n",
    "                list_all_pairs.remove(p)\n",
    "        return list_all_pairs\n",
    "    def remaining_unique_list(list_all_pairs):\n",
    "        res=[]\n",
    "        for x in list_all_pairs:\n",
    "            res.extend(x)\n",
    "        res=list(set(res))\n",
    "        return res\n",
    "    def update_paired_list_in_priority(l1_to_keep_priority,l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_unique=remaining_unique_list(l2_all_for_now)\n",
    "        for v_keep in l1_to_keep_priority:\n",
    "            if v_keep in list_unique:\n",
    "                l4_keep_for_now.append(v_keep)\n",
    "                list_removed_due_to_vkeep=[]\n",
    "                for p in l2_all_for_now:\n",
    "                    if v_keep in p:\n",
    "                        v_remove=[x for x in p if x!=v_keep][0]\n",
    "                        list_removed_due_to_vkeep.append(v_remove)\n",
    "                list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "                if len(list_removed_due_to_vkeep)>0:\n",
    "                    for v_remove in list_removed_due_to_vkeep:\n",
    "                        l3_remove_for_now.append(v_remove)\n",
    "                        l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                        if v_remove in l1_to_keep_priority:\n",
    "                            l1_to_keep_priority.remove(v_remove)\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now=list(set(l4_keep_for_now))\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "\n",
    "    def update_paired_list_v_total(l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_keep_unique_total=[]\n",
    "        for p in l2_all_for_now:\n",
    "            for v in p:\n",
    "                if \"total\" in v:\n",
    "                    list_keep_unique_total.append(v)\n",
    "        list_keep_unique_total=list(set(list_keep_unique_total))\n",
    "        for v_keep in list_keep_unique_total:\n",
    "            list_removed_due_to_vkeep=[]\n",
    "            for p in l2_all_for_now:\n",
    "                if v_keep in p:\n",
    "                    v_remove=[x for x in p if x!=v_keep][0]\n",
    "                    list_removed_due_to_vkeep.append(v_remove)\n",
    "            list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "            if len(list_removed_due_to_vkeep)>0:\n",
    "                for v_remove in list_removed_due_to_vkeep:\n",
    "                    l3_remove_for_now.append(v_remove)\n",
    "                    l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                    if v_remove in list_keep_unique_total:\n",
    "                        list_keep_unique_total.remove(v_remove)\n",
    "\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now.extend(list_keep_unique_total)\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "    def remove_remaining_arbitrary(l2_all_for_now,l3_remove_for_now):\n",
    "        list_remove_arbitrary=[]\n",
    "        if len(l2_all_for_now)==0:\n",
    "            return l2_all_for_now,l3_remove_for_now\n",
    "        while len(l2_all_for_now)>0:\n",
    "            for p in l2_all_for_now:\n",
    "                v_remove=p[0]\n",
    "                list_remove_arbitrary.append(v_remove)\n",
    "                l2_all_for_now.remove(p)\n",
    "                for p2 in l2_all_for_now:\n",
    "                    if v_remove in p2:\n",
    "                        l2_all_for_now.remove(p2)\n",
    "        l3_remove_for_now.extend(list_remove_arbitrary)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        return l2_all_for_now,l3_remove_for_now \n",
    "\n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_in_priority(l1_to_keep_priority=list_col_keep_in_priority,\n",
    "                                                 l2_all_for_now=list_highly_pairs,\n",
    "                                                 l3_remove_for_now=list_cols_to_remove,\n",
    "                                                 l4_keep_for_now=list_cols_to_keep\n",
    "                                                )\n",
    "\n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_v_total(\n",
    "                                                     l2_all_for_now=list_highly_pairs,\n",
    "                                                     l3_remove_for_now=list_cols_to_remove,\n",
    "                                                     l4_keep_for_now=list_cols_to_keep\n",
    "                                                    )\n",
    "\n",
    "    list_highly_pairs,list_cols_to_remove=remove_remaining_arbitrary(l2_all_for_now=list_highly_pairs,\n",
    "                                                                     l3_remove_for_now=list_cols_to_remove)\n",
    "\n",
    "    for col in list_cols_to_remove:\n",
    "        del df_X[col]\n",
    "        print(col, \"removed due to high coor with others\")\n",
    "    return df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_2_trans_plus_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "\n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "    f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "    score=(9*tp-8*fn*(1-f1_score)-fp)*accuracy\n",
    "    # the score ignored the f1 and overall accuracy due to low pctg\n",
    "\n",
    "\n",
    "\n",
    "    # consider the profit vs lost 10:1 (30%*$30) vs (cpc*frequecy or click)\n",
    "    # which means 1 missed (fn) is 10 times of 1 wrong targeted (fp)\n",
    "    # very aribitury\n",
    "    return score\n",
    "\n",
    "def scoring_1_trans_only_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "\n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "\n",
    "    r_1=tn/(tn+fp)\n",
    "    r_2=tp/(tp+fn)\n",
    "\n",
    "    # consider the 2 pctgs that matter\n",
    "    score=(1+r_1)*(1+r_2*(1+pctg))\n",
    "    '''\n",
    "    score=tp*4*(1-pctg*3)-fn*pctg*4-fp*0.1*(1-pctg)+tn*pctg*0.025 # just work for this\n",
    "    # 4=8*0.5 as the benefit * the posible inherit purchase rate\n",
    "    # the false negative is the missed should be getting benefit, but the pctg is the one that most will be 0s\n",
    "    # the false positive is only the one that spend money wrong, ~0.25 cost per reach on FB - 3 weeks, 4 times adjust\n",
    "    # true negative ignored\n",
    "    '''\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def write_aggregate_func_gain_chart(list_selected_features,df_pred_table_detail):\n",
    "    func_dict={\"customer_id_hashed\":\"count\"}\n",
    "    list_cols_for_ratios=['y_true','y_hat']\n",
    "    for col in list_selected_features:\n",
    "        if len(df_pred_table_detail[col].unique())==2:\n",
    "            func_dict.update({col:'sum'})\n",
    "            list_cols_for_ratios.append(col)\n",
    "        else:\n",
    "            func_dict.update({col:\"mean\"})\n",
    "    func_dict.update({\"y_true\":\"sum\"})\n",
    "    func_dict.update({\"y_hat\":\"sum\"})\n",
    "    # func_dict.update({\"pred_prob\":['max','min']})\n",
    "    return func_dict,list_cols_for_ratios\n",
    "\n",
    "\n",
    "def generate_gain_chart_function(df_X,list_y,list_ids,result_sm_model,threshold,list_selected_features):\n",
    "    df_X['pred_prob']=result_sm_model.predict(sm.add_constant(df_X)).values\n",
    "    df_X['customer_id_hashed']=list_ids\n",
    "    df_X['decile']=pd.qcut(df_X['pred_prob'], 10, labels=False)\n",
    "    df_X['decile']=df_X['decile'].apply(lambda x: \"D\"+str(10-x).zfill(2))\n",
    "    df_X['y_true']=list_y\n",
    "    df_X['y_hat']=np.where(df_X['pred_prob']>threshold,1,0)\n",
    "\n",
    "    agg_func,list_cols_to_get_ratio=write_aggregate_func_gain_chart(list_selected_features,df_X)\n",
    "    df_gainchart=df_X.groupby(\"decile\")[['customer_id_hashed']+list_selected_features+['y_true', 'y_hat']].agg(agg_func).reset_index()\n",
    "\n",
    "    df_prob_max=df_X.groupby(\"decile\")['pred_prob'].max().to_frame().reset_index().rename(columns={\"pred_prob\":\"max_prob\"})\n",
    "    df_prob_min=df_X.groupby(\"decile\")['pred_prob'].min().to_frame().reset_index().rename(columns={\"pred_prob\":\"min_prob\"})\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_max,on=\"decile\")\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_min,on=\"decile\")\n",
    "    df_gainchart.insert(2,\"actual_ratio\",df_gainchart['y_true']/df_gainchart['customer_id_hashed'])\n",
    "    df_gainchart.insert(3,\"pred_ratio\",df_gainchart['y_hat']/df_gainchart['customer_id_hashed'])\n",
    "\n",
    "    df_gainchart.insert(4,\"max_pred_prob\",df_gainchart['max_prob'])\n",
    "    df_gainchart.insert(5,\"min_pred_prob\",df_gainchart['min_prob'])\n",
    "    del df_gainchart['max_prob']\n",
    "    del df_gainchart['min_prob']\n",
    "    del df_X['customer_id_hashed']\n",
    "    del df_X['pred_prob']\n",
    "    del df_X['y_true']\n",
    "    del df_X['y_hat']\n",
    "\n",
    "    return df_gainchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n",
      "Job start: model building 2020-12-31 11:23:47.408914\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)\n",
    "print(\"Job start: model building\", datetime.datetime.now())\n",
    "\n",
    "dict_config=json.load(open(\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/config.json\"))\n",
    "high_date=dict_config['pos_end_date']\n",
    "# dict_tables=json.load(open(\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/table_names_%s.json\"%str(high_date).replace(\"-\",\"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhigh_date=dict_config[\\'pos_end_date\\'] # DV high date\\nusername=dict_config[\\'username\\']\\npassword=dict_config[\\'password\\']\\ndatabase=dict_config[\\'database\\']\\nrecent_n_month=dict_config[\\'recent_n_month\\']\\npath_dept_name=dict_config[\"path_dept_name\"]\\nmodel_output_folder=dict_config[\\'model_output_folder\\']\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "high_date=dict_config['pos_end_date'] # DV high date\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "recent_n_month=dict_config['recent_n_month']\n",
    "path_dept_name=dict_config[\"path_dept_name\"]\n",
    "model_output_folder=dict_config['model_output_folder']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'jliang',\n",
       " 'password': 'H1Dswk&Fxz',\n",
       " 'database': 'BigLots',\n",
       " 'pos_table': 'Pred_POS_Department',\n",
       " 'crm_table': 'BL_Rewards_Master',\n",
       " 'exposure_table': 'Pred_ExposureV3_BL_id',\n",
       " 'activity_table': 'Pred_ExpV2_Activity_BL_id',\n",
       " 'pos_start_date': '',\n",
       " 'pos_end_date': '2020-11-28',\n",
       " 'crm_start_date': '',\n",
       " 'crm_end_date': '2020-11-28',\n",
       " 'recent_n_month': 18,\n",
       " 'is_with_dcm': False,\n",
       " 'random_size': '',\n",
       " 'sample_ratio': 1,\n",
       " 'database_update_period': [],\n",
       " 'base_directory': '/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/MySQL_outfile/',\n",
       " 'folder_store_list': '/mnt/clients/juba/hqjubaapp02/sharefolder/biglots_data/static_files/Store_list/',\n",
       " 'folder_email_unsub': '/mnt/clients/juba/hqjubaapp02/sharefolder/biglots_data/Email_Subscription_Files/Unsubs/',\n",
       " 'path_TA_excel': '/mnt/clients/juba/hqjubaapp02/sharefolder/TraderArea/quarterly_TA_update/output_2020-10-08/BL_final_TA_updated_JL_2020-10-08.xlsx',\n",
       " 'path_json_zip_center': '/mnt/clients/juba/hqjubaapp02/sharefolder/Docs/Geo_mapping/updated_zip_centers_JL_2019-05-23.json',\n",
       " 'path_dept_name': '/mnt/clients/juba/hqjubaapp02/sharefolder/biglots_data/static_files/MediaStorm Data Extract - Department Names.txt',\n",
       " 'model_output_folder': '/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Outputs/',\n",
       " 'train_sample_size': 1000000,\n",
       " 'test_ratio': 0.25}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SM_Logistic_Model_dvN:\n",
    "    # 1\n",
    "    def __init__(self,n_week_DV,key_df_type,dict_config=dict_config):\n",
    "        func_name=\"__init__\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "\n",
    "# n_week_DV: 1-4\n",
    "# key_df_type: \"trans_1_only\" or \"trans_2_plus\" in the keys of dict_df_type\n",
    "# df_date_range: defined global df -- df_date_range\n",
    "# sql_engine: mysql engine to BigLots database\n",
    "\n",
    "        high_date=dict_config['pos_end_date'] # DV high date\n",
    "        username=dict_config['username']\n",
    "        password=dict_config['password']\n",
    "        database=dict_config['database']\n",
    "        self.recent_n_month=dict_config['recent_n_month']\n",
    "        path_dept_name=dict_config[\"path_dept_name\"]\n",
    "        model_output_folder=dict_config['model_output_folder']\n",
    "        self.folder_email_unsub=dict_config['folder_email_unsub']\n",
    "        self.sql_engine=sqlalchemy.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))\n",
    "        \n",
    "        json_file_IV_table=\"\"\n",
    "        self.table_df_1_train=\"\"\n",
    "        self.table_2_1_train=\"\"\n",
    "        self.table_2_2_train=\"\"\n",
    "        self.table_0_train=\"\"\n",
    "\n",
    "        self.n_week_DV=n_week_DV\n",
    "        self.key_df_type=key_df_type\n",
    "        \n",
    "        self.df_train_X=pd.DataFrame()\n",
    "        self.X_train_scaled=pd.DataFrame()\n",
    "        self.input_y_train_list=[]\n",
    "        self.list_ids_y_train=[]\n",
    "\n",
    "        self.X_features=self.df_train_X.columns.tolist()\n",
    "\n",
    "        self.df_test_X=pd.DataFrame()\n",
    "        self.input_y_test_list=[]\n",
    "        self.list_ids_y_test=[]\n",
    "\n",
    "        self.db_row_counts=pd.DataFrame()\n",
    "        self.df_y_train_count=pd.DataFrame()\n",
    "        self.df_y_test_count=pd.DataFrame()\n",
    "        self.pctg=None\n",
    "        self.threshold_max_selfdefinedscore=None\n",
    "        self.df_step_table=pd.DataFrame()\n",
    "        self.df_confusion_table=pd.DataFrame()\n",
    "        self.df_gainchart_train=pd.DataFrame()\n",
    "        self.df_gainchart_test=pd.DataFrame()\n",
    "\n",
    "        self.df_train_ids_labeled_summary=pd.DataFrame()\n",
    "        self.df_test_ids_labeled_summary=pd.DataFrame()\n",
    "\n",
    "        self.df_train_ids_labeled=pd.DataFrame()\n",
    "        self.df_test_ids_labeled=pd.DataFrame()\n",
    "\n",
    "        self.output_folder=model_output_folder+\"output_LastWeek_NoDCM_%s_%s/\"%(high_date,str(datetime.datetime.now().date()))\n",
    "        try:\n",
    "            os.stat(self.output_folder)\n",
    "        except:\n",
    "            os.mkdir(self.output_folder)\n",
    "\n",
    "        self.output_path=self.output_folder+\"BL_LRModeling_NoDCM_%s_DV%s_%s_JL_%s.xlsx\"%(key_df_type,str(n_week_DV),high_date,str(datetime.datetime.now().date()))\n",
    "        self.df_department_name=pd.read_table(path_dept_name,sep=\"|\").drop_duplicates()\n",
    "        \n",
    "        if key_df_type==\"trans_1_only\":\n",
    "            date_high_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()\n",
    "            self.DV_high_date=date_high_date\n",
    "            self.IV_high_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()-datetime.timedelta(days=21)\n",
    "            self.str_IV_high_date=str(self.IV_high_date)\n",
    "            self.str_8_digit_IV_high_date=self.str_IV_high_date.replace(\"-\",\"\")\n",
    "            self.sql_str_DV_start=\"'\"+str(date_high_date-datetime.timedelta(days=20))+\"'\"\n",
    "            self.sql_str_DV_end=\"'\"+str(high_date)+\"'\"\n",
    "            \n",
    "        elif key_df_type==\"trans_2_plus\":\n",
    "            date_high_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()\n",
    "            self.DV_high_date=date_high_date\n",
    "            self.IV_high_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()-datetime.timedelta(days=14)\n",
    "            self.str_IV_high_date=str(self.IV_high_date)\n",
    "            self.str_8_digit_IV_high_date=self.str_IV_high_date.replace(\"-\",\"\")\n",
    "            self.sql_str_DV_start=\"'\"+str(date_high_date-datetime.timedelta(days=13))+\"'\"\n",
    "            self.sql_str_DV_end=\"'\"+str(high_date)+\"'\"\n",
    "        if self.recent_n_month:\n",
    "            self.pos_start_date_id_filter = str(pd.to_datetime(self.IV_high_date).date()-datetime.timedelta(days=int(np.ceil(365*self.recent_n_month/12))))\n",
    "        else:\n",
    "            self.pos_start_date_id_filter = dict_config[\"pos_start_date\"]\n",
    "        \n",
    "        self.df_date_range=pd.DataFrame({\"IV\":[self.pos_start_date_id_filter,str(self.IV_high_date)],\n",
    "                                         \"DV\":[self.sql_str_DV_start.replace(\"'\",\"\"),self.sql_str_DV_end.replace(\"'\",\"\")]\n",
    "                                        },index=['start','end'])\n",
    "        \n",
    "        file_json_table=\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/table_names_%s.json\"%str(self.IV_high_date).replace(\"-\",\"\")\n",
    "        self.dict_tables_for_IV=json.load(open(file_json_table,\"r\"))\n",
    "        \n",
    "        \n",
    "        self.dict_tables_for_current_week={}\n",
    "        self.df_target_X=pd.DataFrame()\n",
    "        self.list_ids_y_target=[]\n",
    "        end_time=datetime.datetime.now() \n",
    "        \n",
    "        # add two list of email unsub IDs\n",
    "        list_unsubsription_files=glob.glob(self.folder_email_unsub+\"*.csv\")\n",
    "        list_unsubsription_files=[x for x in list_unsubsription_files if \"iber_File_Refresh__\" in x]\n",
    "        list_unsubsription_files.sort()\n",
    "        df_unsub_files=pd.DataFrame({\"file_path\":list_unsubsription_files})\n",
    "        df_unsub_files['date']=df_unsub_files['file_path'].apply(lambda x: x.split(\"ile_Refresh__\")[1][:8])\n",
    "        df_unsub_files['date']=pd.to_datetime(df_unsub_files['date']).dt.date\n",
    "        \n",
    "        df_unsub_files['day_diff_IV']=abs(df_unsub_files['date']-self.IV_high_date)\n",
    "        self.path_unsub_IV=df_unsub_files[df_unsub_files['day_diff_IV']==df_unsub_files['day_diff_IV'].min()]['file_path'].values.tolist()[0]\n",
    "        \n",
    "        df_unsub_files['day_diff_DV']=abs(df_unsub_files['date']-self.DV_high_date)\n",
    "        self.path_unsub_DV=df_unsub_files[df_unsub_files['day_diff_DV']==df_unsub_files['day_diff_DV'].min()]['file_path'].values.tolist()[0]\n",
    "        \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "            \n",
    "    # 2 \n",
    "    def pull_train_df_X(self):\n",
    "        func_name=\"pull_train_df_X\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "\n",
    "        self.table_df_1_train=self.dict_tables_for_IV['table_df_1']\n",
    "        self.table_2_1_train=self.dict_tables_for_IV['table_2_1']\n",
    "        self.table_2_2_train=self.dict_tables_for_IV['table_2_2']\n",
    "        self.table_0_train=self.dict_tables_for_IV['table_crm_id_list_train']\n",
    "        \n",
    "        \n",
    "        col_list_df0_train=pd.read_sql(\"desc %s\"%self.table_0_train,con=self.sql_engine)['Field'].values.tolist()\n",
    "        col_list_df_1_train=pd.read_sql(\"desc %s\"%self.table_df_1_train,con=self.sql_engine)['Field'].values.tolist()\n",
    "        col_list_2_1_train=pd.read_sql(\"desc %s\"%self.table_2_1_train,con=self.sql_engine)['Field'].values.tolist()\n",
    "        col_list_2_2_train=pd.read_sql(\"desc %s\"%self.table_2_2_train,con=self.sql_engine)['Field'].values.tolist()\n",
    "        \n",
    "        cols_no_need=['sign_up_location','customer_zip_code','nearest_BL_store','distc_to_sign_up','nearest_BL_dist',\n",
    "                      'week_1st_trans','week_recent_one_trans','week_recent_two_trans',\n",
    "                      'DV_cumulative_week_updated_1','DV_cumulative_week_updated_2','DV_cumulative_week_updated_3']\n",
    "        for col_remove in cols_no_need:\n",
    "            col_list_df_1_train=[x for x in col_list_df_1_train if x != col_remove and (x not in [\"customer_id_hashed\", \"sign_up_date\"])]\n",
    "            col_list_2_1_train=[x for x in col_list_2_1_train if x != col_remove and x!=\"id\"]\n",
    "            col_list_2_2_train=[x for x in col_list_2_2_train if x != col_remove and x!=\"id\"]\n",
    "        sql_str_cols_df0_train=str([\"t0.\"+x for x in col_list_df0_train]).replace(\"'\",\"\")[1:-1]  \n",
    "        sql_str_cols_df_1_train=str([\"t1.\"+x for x in col_list_df_1_train]).replace(\"'\",\"\")[1:-1]\n",
    "        sql_str_cols_2_1_train=str([\"t2_1.\"+x for x in col_list_2_1_train]).replace(\"'\",\"\")[1:-1]\n",
    "        sql_str_cols_2_2_train=str([\"t2_2.\"+x for x in col_list_2_2_train]).replace(\"'\",\"\")[1:-1]\n",
    "        sql_str_cols_all=\", \".join([sql_str_cols_df0_train,sql_str_cols_df_1_train,sql_str_cols_2_1_train,sql_str_cols_2_2_train])\n",
    "        \n",
    "        queary=\"SELECT %s from %s as t0 \\\n",
    "        left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed \\\n",
    "        left join %s as t2_1 on t0.customer_id_hashed=t2_1.id \\\n",
    "        left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%(sql_str_cols_all,self.table_0_train,self.table_df_1_train,\n",
    "                                                                self.table_2_1_train,self.table_2_2_train)\n",
    "        print(queary)\n",
    "        df_train_X=pd.read_sql(queary,con=self.sql_engine)\n",
    "        print(df_train_X.shape)\n",
    "        \n",
    "        if self.key_df_type==\"trans_1_only\":\n",
    "            self.df_train_X=df_train_X[pd.isnull(df_train_X['total_sales_recent_two_trans'])]\n",
    "        elif self.key_df_type==\"trans_2_plus\":\n",
    "            self.df_train_X=df_train_X[~pd.isnull(df_train_X['total_sales_recent_two_trans'])]\n",
    "            \n",
    "        self.db_row_counts=pd.DataFrame({\"records\":self.df_train_X.shape[0],\"IVs\":self.df_train_X.shape[1]},index=[\"X_train\"])\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 3    \n",
    "    def clean_train_univariate(self,r_variance=0.95):\n",
    "        func_name=\"clean_train_univariate\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        \n",
    "        # 3.1\n",
    "        list_cols=self.df_train_X.columns.tolist()\n",
    "        for col in list_cols:\n",
    "            if self.df_train_X[col].nunique()<=1:\n",
    "                del self.df_train_X[col]\n",
    "                print(datetime.datetime.now(),self.key_df_type,\"col_nunique<=1 dropped: %s\"%col)\n",
    "                \n",
    "        self.df_train_X=self.df_train_X.T.drop_duplicates().T\n",
    "        print(datetime.datetime.now(),\"done self.df_train.T.drop_duplicates().T\")\n",
    "        \n",
    "        # 3.2\n",
    "        for col in self.df_train_X.columns.tolist():\n",
    "            df_na=self.df_train_X[pd.isnull(self.df_train_X[col])]\n",
    "            if df_na.shape[0]>0:\n",
    "                print(\"Warning: nan detected in the self.df_train col: %s\"%col)\n",
    "        self.list_ids_y_train=self.df_train_X['customer_id_hashed'].tolist()\n",
    "        del self.df_train_X['customer_id_hashed']\n",
    "\n",
    "        # 3.3\n",
    "        threshold_variance_iv=r_variance*(1-r_variance)\n",
    "        selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "        df_redused_X=selector.fit_transform(self.df_train_X)\n",
    "        print(\"self.df_train_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "        indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "        self.df_train_X=self.df_train_X.iloc[:,indices]        \n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 4\n",
    "    def pull_train_df_Y(self):\n",
    "        func_name=\"pull_train_df_Y\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        query=\"select distinct customer_id_hashed as buyers from Pred_POS_Department where sales>=0 and transaction_dt between %s and %s;\"%(self.sql_str_DV_start,self.sql_str_DV_end)\n",
    "        print(query)\n",
    "        self.df_buyers=pd.read_sql(query,con=self.sql_engine)\n",
    "        df_train_ids=pd.DataFrame({\"custoemr_id_hashed\":self.list_ids_y_train},index=range(len(self.list_ids_y_train)))\n",
    "        df_train_ids=pd.merge(df_train_ids,self.df_buyers,left_on=\"custoemr_id_hashed\",right_on=\"buyers\",how=\"left\")\n",
    "        df_train_ids['y_true']=np.where(pd.isnull(df_train_ids['buyers']),0,1)\n",
    "        self.input_y_train_list=df_train_ids['y_true'].tolist()\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "            \n",
    "    # 5\n",
    "    def remove_correlated_cols(self,coorelation_threshold=0.8):\n",
    "        func_name=\"remove_correlated_cols\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        for col in self.df_train_X.columns.tolist():\n",
    "            self.df_train_X[col]=self.df_train_X[col].astype(float)\n",
    "        self.df_train_X=remove_cols_with_high_coor(df_X=self.df_train_X,coorelation_threshold=coorelation_threshold)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 6\n",
    "    def select_from_model_n_features(self, N_feature_select_from_models):\n",
    "        func_name=\"select_from_model_n_features\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        \n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print('start',datetime.datetime.now(),\"select_from_model_n_features\")\n",
    "        print(\"Starting select_from_model_n_features: \",datetime.datetime.now())\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(random_state=0,\n",
    "                                                                solver=\"saga\",\n",
    "                                                                max_iter=2000,\n",
    "                                                                n_jobs=24,\n",
    "                                                                tol=0.0001),\n",
    "                                   max_features=N_feature_select_from_models,\n",
    "                                   threshold=-np.inf).fit(self.X_train_scaled, self.input_y_train_list)\n",
    "\n",
    "        print(\"selector.threshold_\",selector.threshold_)\n",
    "        selector_support_FROMMODEL=selector.get_support()\n",
    "\n",
    "        self.X_features=[self.df_train_X.columns.tolist()[i] for i,v in enumerate(selector_support_FROMMODEL) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "    # 7\n",
    "    def select_REF(self,n_features_to_select): # should be RFE -- recursive feature elimination, wrong lable name\n",
    "        func_name=\"select_REF\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        \n",
    "        estimator = LogisticRegression(fit_intercept=True,solver='saga',max_iter=2000,n_jobs=24,tol=0.001)\n",
    "        selector = RFE(estimator,step=1,n_features_to_select=n_features_to_select)\n",
    "        selector = selector.fit(self.X_train_scaled, self.input_y_train_list)\n",
    "        selector_support_REF=selector.support_\n",
    "        print(\"Done select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        self.X_features=[self.df_train_X.columns.tolist()[i] for i,v in enumerate(selector_support_REF) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 8\n",
    "    def forwards_feature_elimination_based_on_p_and_vif(self,niter=50,method=\"lbfgs\",p_tol=0.1,vif_tol=5):\n",
    "        func_name=\"forwards_feature_elimination_based_on_p_and_vif\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        len_x_features=self.df_train_X.shape[1]\n",
    "        len_x_features_new=0\n",
    "        df_x_dropped=self.df_train_X.copy()\n",
    "        i_iter=0\n",
    "        while len_x_features_new<len_x_features and i_iter<=100:\n",
    "            i_iter+=1\n",
    "            len_x_features=df_x_dropped.shape[1]\n",
    "            mod=sm.Logit(self.input_y_train_list,sm.add_constant(df_x_dropped),niter=niter,method=method)\n",
    "            res=mod.fit()\n",
    "            table=res.summary2().tables[1]   \n",
    "            X=add_constant(scale(df_x_dropped))\n",
    "            list_cols=table.index.tolist()\n",
    "            table[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "\n",
    "            max_vif=table['VIF Factor'].max()\n",
    "            max_p=table['P>|z|'].max()\n",
    "\n",
    "            if max_vif>vif_tol:\n",
    "                col_name_to_drop=table.index[table['VIF Factor']==max_vif][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to high vif\"%col_name_to_drop)\n",
    "\n",
    "            elif max_p>p_tol:\n",
    "                col_name_to_drop=table.index[table['P>|z|']==max_p][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to p value\"%col_name_to_drop)\n",
    "            else:\n",
    "                i_iter+=100\n",
    "\n",
    "        self.df_train_X=df_x_dropped\n",
    "        self.X_features=df_x_dropped.columns.tolist()\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "\n",
    "    # 9\n",
    "    def run_sm_logR_model(self):\n",
    "        func_name=\"run_sm_logR_model\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        self.sm_model=sm.Logit(self.input_y_train_list,sm.add_constant(self.df_train_X),niter=50,method=\"lbfgs\")\n",
    "        self.res_of_model=self.sm_model.fit()\n",
    "        self.summary_table_over=self.res_of_model.summary2().tables[0].reset_index()\n",
    "        self.summary_table_output=self.res_of_model.summary2().tables[1].reset_index()\n",
    "\n",
    "        std=self.sm_model.exog.std(axis=0)\n",
    "        std[0] = 1\n",
    "        tt = self.res_of_model.t_test(np.diag(std))\n",
    "        df_std_coef=tt.summary_frame()\n",
    "        list_std_coefficients=df_std_coef['coef'].tolist()\n",
    "        self.summary_table_output['std_coef']=list_std_coefficients\n",
    "\n",
    "        self.list_train_pred=self.res_of_model.predict()\n",
    "        # \n",
    "\n",
    "        coefficient_of_dermination = r2_score(self.input_y_train_list, self.list_train_pred)\n",
    "        self.summary_table_over=self.summary_table_over.append(pd.DataFrame({\"index\":[8],0:\"calculated_r_squared\",1:coefficient_of_dermination},index=[8]))\n",
    "\n",
    "        #VIF\n",
    "        X=add_constant(self.X_train_scaled)\n",
    "        list_cols=self.summary_table_output['index'].tolist()\n",
    "        self.summary_table_output[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        self.summary_table_output=self.summary_table_output.sort_values(\"std_coef\")\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "\n",
    "    # 10\n",
    "    def select_test_df_from_mysql_past(self,n_limit_test=None):\n",
    "        # The old version that use the past available DVs in same range to generate gain_chart\n",
    "        # New version updated with last week IVs which don't have actual DVs yet\n",
    "        \n",
    "        # total_test_count=\n",
    "        # try no split first without saving test scaled X\n",
    "        func_name=\"select_test_df_from_mysql_past\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))  \n",
    "        cols_in_X=self.summary_table_output.iloc[:,0].values.tolist()\n",
    "        cols_in_X.remove(\"const\")\n",
    "        self.X_features=cols_in_X\n",
    "        table_name_t0=self.dict_tables_for_IV['table_crm_id_list_test']\n",
    "        col_list_t0=pd.read_sql(\"select * from %s limit 2\"%table_name_t0,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t1=self.dict_tables_for_IV['table_df_1']\n",
    "        col_list_t1=pd.read_sql(\"select * from %s limit 2\"%table_name_t1,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t2_1=self.dict_tables_for_IV['table_2_1']\n",
    "        col_list_t2_1=pd.read_sql(\"select * from %s limit 2\"%table_name_t2_1,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t2_2=self.dict_tables_for_IV['table_2_2']\n",
    "        col_list_t2_2=pd.read_sql(\"select * from %s limit 2\"%table_name_t2_2,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        col_list_t1=[x for x in col_list_t1 if x in cols_in_X]\n",
    "        col_list_t2_1=[x for x in col_list_t2_1 if x in cols_in_X]\n",
    "        col_list_t2_2=[x for x in col_list_t2_2 if x in cols_in_X]\n",
    "\n",
    "\n",
    "        sql_str_cols_df0_test=str([\"t0.\"+x for x in col_list_t0]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list=[sql_str_cols_df0_test]\n",
    "        list_tables=[\"t0\"]\n",
    "        # col_list_dv=[x for x in self.df_train_Y.columns.tolist()]\n",
    "        # sql_str_cols_dv=str([\"t1.\"+x for x in col_list_dv]).replace(\"'\",\"\")[1:-1]\n",
    "        # list_query_col_list.append(sql_str_cols_dv)\n",
    "        if len(col_list_t1)>0:\n",
    "            sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_t1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_df_1)\n",
    "            list_tables.append(\"t1\")\n",
    "        if len(col_list_t2_1)>0:\n",
    "            sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_t2_1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_1)\n",
    "            list_tables.append(\"t2_1\")\n",
    "        if len(col_list_t2_2)>0:\n",
    "            sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_t2_2]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_2)\n",
    "            list_tables.append(\"t2_2\")\n",
    "        str_cols_test=\", \".join(list_query_col_list)\n",
    "        select_clause=\"SELECT %s from %s as t0\"%(str_cols_test,table_name_t0)\n",
    "\n",
    "        list_of_join_clause=[]\n",
    "        if \"t1\" in list_tables:\n",
    "            str_join_clause_t1=\"left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed\"%table_name_t1\n",
    "            list_of_join_clause.append(str_join_clause_t1)\n",
    "        if \"t2_1\" in list_tables:\n",
    "            str_join_clause_t2_1=\"left join %s as t2_1 on t0.customer_id_hashed=t2_1.id\"%table_name_t2_1\n",
    "            list_of_join_clause.append(str_join_clause_t2_1)\n",
    "        if \"t2_2\" in list_tables:\n",
    "            str_join_clause_t2_2=\"left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%table_name_t2_2\n",
    "            list_of_join_clause.append(str_join_clause_t2_2)\n",
    "\n",
    "        if self.key_df_type==\"trans_1_only\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is null\"\n",
    "        elif self.key_df_type==\"trans_2_plus\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is not null\"\n",
    "        else:\n",
    "            where_clause=\"\"\n",
    "            print(\"key_df_type not specified, please choose either trans_1_only or trans_2_plus\")\n",
    "        if n_limit_test:\n",
    "            limit_clause=\"limit %i\"%n_limit_test\n",
    "            query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]+[limit_clause]).strip() \n",
    "        else:\n",
    "            query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]).strip() \n",
    "            \n",
    "        print(query_full)\n",
    "        self.df_test_X=pd.read_sql(query_full,con=self.sql_engine)\n",
    "        if \"nearest_BL_dist\" in self.df_test_X.columns.tolist():\n",
    "            self.df_test_X=self.df_test_X[pd.notnull(self.df_test_X['nearest_BL_dist'])]\n",
    "        for col in self.df_test_X.columns.tolist():\n",
    "            # df_nan=df[pd.isnull(df[col])]\n",
    "            if self.df_test_X[pd.isnull(self.df_test_X[col])].shape[0]>0:\n",
    "                raise ValueError(\"%s in the selected test df is null\"%col)       \n",
    "\n",
    "        self.list_ids_y_test=self.df_test_X['customer_id_hashed'].values.tolist()\n",
    "        del self.df_test_X['customer_id_hashed']\n",
    "\n",
    "        df_test_ids=pd.DataFrame({\"custoemr_id_hashed\":self.list_ids_y_test},index=range(len(self.list_ids_y_test)))\n",
    "        df_test_ids=pd.merge(df_test_ids,self.df_buyers,left_on=\"custoemr_id_hashed\",right_on=\"buyers\",how=\"left\")\n",
    "        df_test_ids['y_true']=np.where(pd.isnull(df_test_ids['buyers']),0,1)\n",
    "        self.input_y_test_list=df_test_ids['y_true'].tolist()\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 11\n",
    "    def run_updating_df_count(self):\n",
    "        func_name=\"run_updating_df_count\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))          \n",
    "        df_test_X_count=pd.DataFrame({\"records\":self.df_test_X.shape[0],\"IVs\":self.df_test_X.shape[1]},index=[\"X_test\"])\n",
    "        self.db_row_counts=self.db_row_counts.append(df_test_X_count)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "    # 12\n",
    "    def generate_DV_distribution(self):\n",
    "        func_name=\"generate_DV_distribution\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))   \n",
    "        count_1=sum(self.input_y_train_list)\n",
    "        count_0=len(self.input_y_train_list)-count_1\n",
    "        df_y_train_count=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[self.n_week_DV])\n",
    "        df_y_train_count.insert(0,\"set\",\"y_train\")\n",
    "\n",
    "\n",
    "        count_1=sum(self.input_y_test_list)\n",
    "        count_0=len(self.input_y_test_list)-count_1\n",
    "        df_y_test_count=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[self.n_week_DV])\n",
    "        df_y_test_count.insert(0,\"set\",\"y_test\")\n",
    "\n",
    "        self.df_y_train_count=df_y_train_count\n",
    "        self.df_y_test_count=df_y_test_count\n",
    "        self.pctg=(sum(self.input_y_train_list)+sum(self.input_y_test_list))/(len(self.input_y_train_list)+len(self.input_y_test_list))\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "    # 13\n",
    "    def pred_test_Y_past(self):\n",
    "        func_name=\"pred_test_Y_past\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))   \n",
    "        self.list_test_pred=self.res_of_model.predict(sm.add_constant(self.df_test_X)).tolist()\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "    # 14\n",
    "    def generate_step_table_of_test_SM(self,):\n",
    "        func_name=\"generate_step_table_of_test_SM\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))   \n",
    "        if self.key_df_type==\"trans_2_plus\":\n",
    "            threshold_list = [(x+1)/100 for x in range(0,100)] \n",
    "        elif self.key_df_type==\"trans_1_only\":\n",
    "            margin=max(np.round(self.pctg/2,2),0.04)\n",
    "            start_prob_pctg=max(0.001,int(np.floor((self.pctg-margin)*100))/100)\n",
    "            end_prob_pctg=int(np.floor((self.pctg+margin)*100))/100\n",
    "            threshold_list = [(x+1)/1000 for x in range(int(start_prob_pctg*1000),int(end_prob_pctg*1000))]\n",
    "        else:\n",
    "            print(\"Error of the key_df_type when run generate_step_table_of_test_SM\")\n",
    "\n",
    "            \n",
    "        list_prob_test=self.list_test_pred\n",
    "        df_output=pd.DataFrame()\n",
    "        for i in threshold_list:\n",
    "            y_test_pred=[1 if x>i else 0 for x in list_prob_test]\n",
    "\n",
    "            accuracy_score = metrics.accuracy_score(self.input_y_test_list,y_test_pred)    \n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(self.input_y_test_list, y_test_pred).ravel()\n",
    "            # \n",
    "            TPR=tp/(tp+fn) #recall\n",
    "            FNR=fn/(tp+fn)\n",
    "            FPR=fp/(fp+tn)\n",
    "            TNR=tn/(fp+tn)\n",
    "\n",
    "            PPV=tp/(tp+fp) #precission\n",
    "            f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "            df=pd.DataFrame({\"predicted_positive\":len([x for x in y_test_pred if x==1]),\n",
    "                             \"predicted_negative\":len([x for x in y_test_pred if x==0]),\n",
    "                             \"accuracy_score\":accuracy_score,\n",
    "                             'true_negative':tn,\n",
    "                             'false_positive':fp,\n",
    "                             'false_negative':fn,\n",
    "                             'true_positive':tp,\n",
    "                             'true_positive_rate':TPR,\n",
    "                             'false_negative_rate':FNR,\n",
    "                             'false_positive_rate':FPR,\n",
    "                             'true_negative_rate':TNR,\n",
    "                             'precission_(Positive predictive value)':PPV,\n",
    "                             'f1_score':f1_score\n",
    "                            },index=[i])\n",
    "            df_output=df_output.append(df)\n",
    "\n",
    "        self.df_step_table=df_output\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "    # 15\n",
    "    def select_best_scored_pred_prob(self):\n",
    "        func_name=\"select_best_scored_pred_prob\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))        \n",
    "        if self.key_df_type==\"trans_2_plus\":\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_2_trans_plus_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)  \n",
    "        elif self.key_df_type==\"trans_1_only\":\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_1_trans_only_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)\n",
    "        else:\n",
    "            print(\"Error of the key_df_type when run select_best_scored_pred_prob\")\n",
    "        threshold_max_selfdefinedscore=self.df_step_table[self.df_step_table['self_defined_score']==self.df_step_table['self_defined_score'].max()].index[0]\n",
    "        self.threshold_max_selfdefinedscore=threshold_max_selfdefinedscore\n",
    "        print(\"threshold_max_selfdefinedscore\",threshold_max_selfdefinedscore)\n",
    "        self.df_step_table=self.df_step_table.reset_index()\n",
    "        self.df_confusion_table=self.df_step_table.loc[self.df_step_table['index']==threshold_max_selfdefinedscore,:]\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 16\n",
    "    def generate_gain_chart_past(self):\n",
    "        func_name=\"generate_gain_chart_past\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        self.df_gainchart_train=generate_gain_chart_function(df_X=self.df_train_X,\n",
    "                                                             list_y=self.input_y_train_list,\n",
    "                                                             list_ids=self.list_ids_y_train,\n",
    "                                                             result_sm_model=self.res_of_model,\n",
    "                                                             threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                             list_selected_features=self.X_features)\n",
    "\n",
    "        self.df_gainchart_test=generate_gain_chart_function(df_X=self.df_test_X,\n",
    "                                                            list_y=self.input_y_test_list,\n",
    "                                                            list_ids=self.list_ids_y_test,\n",
    "                                                            result_sm_model=self.res_of_model,\n",
    "                                                            threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                            list_selected_features=self.X_features)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "    # 17\n",
    "    def check_shopper_type_past(self):\n",
    "        func_name=\"check_shopper_type_past\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        \n",
    "        recent_4_week_sign_up_end_dt=self.IV_high_date\n",
    "        recent_4_week_sign_up_start_dt=recent_4_week_sign_up_end_dt-datetime.timedelta(days=27)\n",
    "        str_start_sign_up=\"'\"+str(recent_4_week_sign_up_start_dt)+\"'\"\n",
    "        str_end_sign_up=\"'\"+str(recent_4_week_sign_up_end_dt)+\"'\"\n",
    "        print(\"new sign up date range below: \\n\",recent_4_week_sign_up_start_dt,recent_4_week_sign_up_end_dt)\n",
    "\n",
    "        df_recent_4_week_new_sings=pd.read_sql(\"select customer_id_hashed from BL_Rewards_Master where sign_up_date between %s and %s\"%(str_start_sign_up,str_end_sign_up),con=self.sql_engine)\n",
    "        df_recent_4_week_new_sings=df_recent_4_week_new_sings.drop_duplicates()\n",
    "        df_recent_4_week_new_sings['sign_up_label']=\"new_signs\"\n",
    "        \n",
    "        list_unsunsribe_ids_IV=pd.read_csv(self.path_unsub_IV,\n",
    "                             dtype=str,usecols=['customersummary_c_primaryscnhash'])['customersummary_c_primaryscnhash'].unique().tolist()\n",
    "        \n",
    "        # \n",
    "        df_train_ids_labeled=pd.DataFrame({\"y_hat\":self.list_train_pred},index=self.list_ids_y_train).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_train_ids_labeled['selection_label']=np.where(df_train_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_train_ids_labeled=pd.merge(df_train_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_train_ids_labeled['sign_up_label']=df_train_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_train_ids_labeled['actual_shopping_label']=self.input_y_train_list\n",
    "        df_train_ids_labeled['actual_shopping_label']=df_train_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "        df_train_ids_labeled['email_subscription_label']=np.where(df_train_ids_labeled['customer_id_hashed'].isin(list_unsunsribe_ids_IV),\"unsub\",\"default_subs\")\n",
    "        \n",
    "        \n",
    "        self.df_train_ids_labeled_summary=df_train_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label','email_subscription_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_train_ids_labeled=df_train_ids_labeled\n",
    "\n",
    "        # \n",
    "        df_test_ids_labeled=pd.DataFrame({\"y_hat\":self.list_test_pred},index=self.list_ids_y_test).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_test_ids_labeled['selection_label']=np.where(df_test_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_test_ids_labeled=pd.merge(df_test_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_test_ids_labeled['sign_up_label']=df_test_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_test_ids_labeled['actual_shopping_label']=self.input_y_test_list\n",
    "        df_test_ids_labeled['actual_shopping_label']=df_test_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "        df_test_ids_labeled['email_subscription_label']=np.where(df_test_ids_labeled['customer_id_hashed'].isin(list_unsunsribe_ids_IV),\"unsub\",\"default_subs\")\n",
    "        \n",
    "        self.df_test_ids_labeled_summary=df_test_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label','email_subscription_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_test_ids_labeled=df_test_ids_labeled        \n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "\n",
    "\n",
    "    # 18\n",
    "    def save_outputs_past(self):\n",
    "        func_name=\"save_outputs_past\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        writer=pd.ExcelWriter(self.output_path,engine=\"xlsxwriter\")\n",
    "\n",
    "        self.db_row_counts.to_excel(writer,\"df_dataset_shape\")\n",
    "        self.df_date_range.to_excel(writer,\"df_date_range\")\n",
    "        self.df_y_train_count.to_excel(writer,\"df_y_train_count\")\n",
    "        self.df_y_test_count.to_excel(writer,\"df_y_test_count\")\n",
    "        self.summary_table_over.to_excel(writer,\"summary_table_over\")\n",
    "        self.summary_table_output.to_excel(writer,\"summary_table_output\")\n",
    "        self.df_step_table.to_excel(writer,\"step_table\",index=True)\n",
    "        self.df_confusion_table.to_excel(writer,\"select_score_matrix\",index=False)\n",
    "\n",
    "        self.df_gainchart_train.to_excel(writer,\"gainchart_train\",index=False)\n",
    "        self.df_gainchart_test.to_excel(writer,\"gainchart_test\",index=False)\n",
    "        self.df_department_name.to_excel(writer,\"department_name\",index=False)\n",
    "\n",
    "        self.df_train_ids_labeled_summary.to_excel(writer,\"train_id_summary\",index=False)\n",
    "        self.df_test_ids_labeled_summary.to_excel(writer,\"test_id_summary\",index=False)\n",
    "\n",
    "        writer.save()\n",
    "        str_IV_high_date=str(self.IV_high_date).replace(\"'\",\"\")\n",
    "        str_dv_type=\"DV%i_%s\"%(self.n_week_DV,self.key_df_type)\n",
    "        str_DV_high_date=str(self.DV_high_date).replace(\"'\",\"\")\n",
    "        \n",
    "        self.df_train_ids_labeled.to_csv(self.output_folder+\"df_train_ids_labeled_%s_%s_%s.csv\"%(str_IV_high_date,str_dv_type,str_DV_high_date),index=False)\n",
    "        self.df_test_ids_labeled.to_csv(self.output_folder+\"df_test_ids_labeled_%s_%s_%s.csv\"%(str_IV_high_date,str_dv_type,str_DV_high_date),index=False)\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 19\n",
    "    def select_target_df_from_mysql_current(self,n_limit_test=None):\n",
    "        # Pull all the 18 months buyers up to the high date in our DB, and apply the previous model\n",
    "        func_name=\"select_target_df_from_mysql_current\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        \n",
    "        del self.df_train_X\n",
    "        del self.df_test_X\n",
    "        del self.df_train_ids_labeled\n",
    "        del self.df_test_ids_labeled\n",
    "        self.df_target_X=pd.DataFrame()\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        file_json_table=\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/table_names_%s.json\"%str(self.DV_high_date).replace(\"-\",\"\")\n",
    "        self.dict_tables_for_current_week=json.load(open(file_json_table,\"r\"))        \n",
    "        \n",
    "        cols_in_X=self.summary_table_output.iloc[:,0].values.tolist()\n",
    "        cols_in_X.remove(\"const\")\n",
    "        self.X_features=cols_in_X\n",
    "        table_name_t0=self.dict_tables_for_current_week['table_crm_id_list_test']\n",
    "        col_list_t0=pd.read_sql(\"select * from %s limit 2\"%table_name_t0,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t1=self.dict_tables_for_current_week['table_df_1']\n",
    "        col_list_t1=pd.read_sql(\"select * from %s limit 2\"%table_name_t1,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t2_1=self.dict_tables_for_current_week['table_2_1']\n",
    "        col_list_t2_1=pd.read_sql(\"select * from %s limit 2\"%table_name_t2_1,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        table_name_t2_2=self.dict_tables_for_current_week['table_2_2']\n",
    "        col_list_t2_2=pd.read_sql(\"select * from %s limit 2\"%table_name_t2_2,con=self.sql_engine).columns.tolist()\n",
    "\n",
    "        col_list_t1=[x for x in col_list_t1 if x in cols_in_X]\n",
    "        col_list_t2_1=[x for x in col_list_t2_1 if x in cols_in_X]\n",
    "        col_list_t2_2=[x for x in col_list_t2_2 if x in cols_in_X]\n",
    "\n",
    "\n",
    "        sql_str_cols_df0_test=str([\"t0.\"+x for x in col_list_t0]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list=[sql_str_cols_df0_test]\n",
    "        list_tables=[\"t0\"]\n",
    "        # col_list_dv=[x for x in self.df_train_Y.columns.tolist()]\n",
    "        # sql_str_cols_dv=str([\"t1.\"+x for x in col_list_dv]).replace(\"'\",\"\")[1:-1]\n",
    "        # list_query_col_list.append(sql_str_cols_dv)\n",
    "        if len(col_list_t1)>0:\n",
    "            sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_t1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_df_1)\n",
    "            list_tables.append(\"t1\")\n",
    "        if len(col_list_t2_1)>0:\n",
    "            sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_t2_1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_1)\n",
    "            list_tables.append(\"t2_1\")\n",
    "        if len(col_list_t2_2)>0:\n",
    "            sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_t2_2]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_2)\n",
    "            list_tables.append(\"t2_2\")\n",
    "        str_cols_test=\", \".join(list_query_col_list)\n",
    "        select_clause=\"SELECT %s from %s as t0\"%(str_cols_test,table_name_t0)\n",
    "\n",
    "        list_of_join_clause=[]\n",
    "        if \"t1\" in list_tables:\n",
    "            str_join_clause_t1=\"left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed\"%table_name_t1\n",
    "            list_of_join_clause.append(str_join_clause_t1)\n",
    "        if \"t2_1\" in list_tables:\n",
    "            str_join_clause_t2_1=\"left join %s as t2_1 on t0.customer_id_hashed=t2_1.id\"%table_name_t2_1\n",
    "            list_of_join_clause.append(str_join_clause_t2_1)\n",
    "        if \"t2_2\" in list_tables:\n",
    "            str_join_clause_t2_2=\"left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%table_name_t2_2\n",
    "            list_of_join_clause.append(str_join_clause_t2_2)\n",
    "\n",
    "        if self.key_df_type==\"trans_1_only\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is null\"\n",
    "        elif self.key_df_type==\"trans_2_plus\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is not null\"\n",
    "        else:\n",
    "            where_clause=\"\"\n",
    "            print(\"key_df_type not specified, please choose either trans_1_only or trans_2_plus\")\n",
    "        if n_limit_test:\n",
    "            limit_clause=\"limit %i\"%n_limit_test\n",
    "            query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]+[limit_clause]).strip() \n",
    "        else:\n",
    "            query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]).strip() \n",
    "            \n",
    "        print(query_full)\n",
    "        self.df_target_X=pd.read_sql(query_full,con=self.sql_engine)\n",
    "        if \"nearest_BL_dist\" in self.df_target_X.columns.tolist():\n",
    "            self.df_test_X=self.df_target_X[pd.notnull(self.df_target_X['nearest_BL_dist'])]\n",
    "        for col in self.df_target_X.columns.tolist():\n",
    "            # df_nan=df[pd.isnull(df[col])]\n",
    "            if self.df_target_X[pd.isnull(self.df_target_X[col])].shape[0]>0:\n",
    "                raise ValueError(\"%s in the selected test df is null\"%col)       \n",
    "\n",
    "        self.list_ids_y_target=self.df_target_X['customer_id_hashed'].values.tolist()\n",
    "        del self.df_target_X['customer_id_hashed']\n",
    "        \n",
    "        # Below passed since the true is not available not and will be predicted from the model\n",
    "        '''\n",
    "        df_test_ids=pd.DataFrame({\"custoemr_id_hashed\":self.list_ids_y_test},index=range(len(self.list_ids_y_test)))\n",
    "        df_test_ids=pd.merge(df_test_ids,self.df_buyers,left_on=\"custoemr_id_hashed\",right_on=\"buyers\",how=\"left\")\n",
    "        df_test_ids['y_true']=np.where(pd.isnull(df_test_ids['buyers']),0,1)\n",
    "        self.input_y_test_list=df_test_ids['y_true'].tolist()\n",
    "        '''\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 20\n",
    "    def pred_target_Y_current_week(self):\n",
    "        func_name=\"pred_target_Y_current_week\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        self.list_target_pred=self.res_of_model.predict(sm.add_constant(self.df_target_X)).tolist()\n",
    "        end_time=datetime.datetime.now() \n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "    # 21\n",
    "    def generate_and_write_target_id_with_label(self):\n",
    "        func_name=\"generate_and_write_target_id_with_label\"\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('start -- %s: %s'%(func_name,str(start_time)))\n",
    "        \n",
    "        recent_4_week_sign_up_end_dt=self.DV_high_date\n",
    "        recent_4_week_sign_up_start_dt=recent_4_week_sign_up_end_dt-datetime.timedelta(days=27)\n",
    "        str_start_sign_up=\"'\"+str(recent_4_week_sign_up_start_dt)+\"'\"\n",
    "        str_end_sign_up=\"'\"+str(recent_4_week_sign_up_end_dt)+\"'\"\n",
    "        print(\"new sign up date range below: \\n\",recent_4_week_sign_up_start_dt,recent_4_week_sign_up_end_dt)\n",
    "\n",
    "        df_recent_4_week_new_sings=pd.read_sql(\"select customer_id_hashed from BL_Rewards_Master where sign_up_date between %s and %s\"%(str_start_sign_up,str_end_sign_up),con=self.sql_engine)\n",
    "        df_recent_4_week_new_sings=df_recent_4_week_new_sings.drop_duplicates()\n",
    "        df_recent_4_week_new_sings['sign_up_label']=\"new_signs\"\n",
    "        # \n",
    "        df_target_ids_labeled=pd.DataFrame({\"y_hat\":self.list_target_pred},index=self.list_ids_y_target).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_target_ids_labeled['selection_label']=np.where(df_target_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_target_ids_labeled=pd.merge(df_target_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_target_ids_labeled['sign_up_label']=df_target_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        \n",
    "        list_unsunsribe_ids_DV=pd.read_csv(self.path_unsub_DV,\n",
    "                             dtype=str,usecols=['customersummary_c_primaryscnhash'])['customersummary_c_primaryscnhash'].unique().tolist()\n",
    "        df_target_ids_labeled['email_subscription_label']=np.where(df_target_ids_labeled['customer_id_hashed'].isin(list_unsunsribe_ids_DV),\"unsub\",\"default_subs\")\n",
    "        self.df_target_ids_labeled=df_target_ids_labeled\n",
    "        self.df_target_ids_labeled_summary=df_target_ids_labeled.groupby(['selection_label','sign_up_label','email_subscription_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        \n",
    "        self.df_target_ids_labeled.to_csv(self.output_folder+\"df_target_ids_labeled_%s_%s_%s.csv\"%(self.str_IV_high_date,self.key_df_type,str(self.DV_high_date)),index=False)\n",
    "        self.df_target_ids_labeled_summary.to_csv(self.output_folder+\"df_target_ids_labeled_summary_%s_%s_%s.csv\"%(self.str_IV_high_date,self.key_df_type,str(self.DV_high_date)),index=False)\n",
    "        end_time=datetime.datetime.now()\n",
    "        print('Done -- %s: %s'%(func_name,str(end_time)))\n",
    "        print(\"%s\\n\"%str(end_time-start_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_1_only DV 3 start:  2020-12-31 11:23:47.785254\n",
      "start -- __init__: 2020-12-31 11:23:47.785548\n",
      "Done -- __init__: 2020-12-31 11:23:47.926315\n",
      "0:00:00.140767\n",
      "\n",
      "start -- pull_train_df_X: 2020-12-31 11:23:47.938396\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.S_zip, t1.else_10_zip, t1.signed_online, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.total_trans_since_registration, t2_1.total_items, t2_1.total_sales, t2_1.trans_in_store, t2_1.unique_stores, t2_1.trans_online, t2_1.department_minus_one_trans, t2_1.department_108_trans, t2_1.department_109_trans, t2_1.department_110_trans, t2_1.department_111_trans, t2_1.department_114_trans, t2_1.department_115_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_150_trans, t2_1.department_160_trans, t2_1.department_170_trans, t2_1.department_210_trans, t2_1.department_230_trans, t2_1.department_250_trans, t2_1.department_270_trans, t2_1.department_310_trans, t2_1.department_320_trans, t2_1.department_330_trans, t2_1.department_340_trans, t2_1.department_350_trans, t2_1.department_351_trans, t2_1.department_352_trans, t2_1.department_353_trans, t2_1.department_354_trans, t2_1.department_355_trans, t2_1.department_360_trans, t2_1.department_361_trans, t2_1.department_362_trans, t2_1.department_363_trans, t2_1.department_364_trans, t2_1.department_365_trans, t2_1.department_366_trans, t2_1.department_367_trans, t2_1.department_370_trans, t2_1.department_410_trans, t2_1.department_425_trans, t2_1.department_470_trans, t2_1.department_480_trans, t2_1.department_510_trans, t2_1.department_520_trans, t2_1.department_521_trans, t2_1.department_526_trans, t2_1.department_530_trans, t2_1.department_540_trans, t2_1.department_550_trans, t2_1.department_560_trans, t2_1.department_608_trans, t2_1.department_610_trans, t2_1.department_612_trans, t2_1.department_615_trans, t2_1.department_710_trans, t2_1.department_800_trans, t2_2.purchase_channel_1st_trans, t2_2.total_sales_1st_trans, t2_2.total_units_1st_trans, t2_2.department_minus_one_1st_trans, t2_2.department_108_1st_trans, t2_2.department_109_1st_trans, t2_2.department_110_1st_trans, t2_2.department_111_1st_trans, t2_2.department_114_1st_trans, t2_2.department_115_1st_trans, t2_2.department_120_1st_trans, t2_2.department_130_1st_trans, t2_2.department_140_1st_trans, t2_2.department_150_1st_trans, t2_2.department_160_1st_trans, t2_2.department_170_1st_trans, t2_2.department_210_1st_trans, t2_2.department_230_1st_trans, t2_2.department_250_1st_trans, t2_2.department_270_1st_trans, t2_2.department_310_1st_trans, t2_2.department_320_1st_trans, t2_2.department_330_1st_trans, t2_2.department_340_1st_trans, t2_2.department_350_1st_trans, t2_2.department_351_1st_trans, t2_2.department_352_1st_trans, t2_2.department_353_1st_trans, t2_2.department_354_1st_trans, t2_2.department_355_1st_trans, t2_2.department_360_1st_trans, t2_2.department_361_1st_trans, t2_2.department_362_1st_trans, t2_2.department_363_1st_trans, t2_2.department_364_1st_trans, t2_2.department_365_1st_trans, t2_2.department_366_1st_trans, t2_2.department_367_1st_trans, t2_2.department_370_1st_trans, t2_2.department_410_1st_trans, t2_2.department_425_1st_trans, t2_2.department_470_1st_trans, t2_2.department_480_1st_trans, t2_2.department_510_1st_trans, t2_2.department_520_1st_trans, t2_2.department_521_1st_trans, t2_2.department_526_1st_trans, t2_2.department_530_1st_trans, t2_2.department_540_1st_trans, t2_2.department_550_1st_trans, t2_2.department_560_1st_trans, t2_2.department_608_1st_trans, t2_2.department_610_1st_trans, t2_2.department_612_1st_trans, t2_2.department_615_1st_trans, t2_2.department_710_1st_trans, t2_2.department_800_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.recent_one_trans_also_1st, t2_2.purchase_channel_1st_trans_recent_one, t2_2.total_sales_recent_one_trans, t2_2.total_units_recent_one_trans, t2_2.department_minus_one_recent_one, t2_2.department_108_recent_one, t2_2.department_109_recent_one, t2_2.department_110_recent_one, t2_2.department_111_recent_one, t2_2.department_114_recent_one, t2_2.department_115_recent_one, t2_2.department_120_recent_one, t2_2.department_130_recent_one, t2_2.department_140_recent_one, t2_2.department_150_recent_one, t2_2.department_160_recent_one, t2_2.department_170_recent_one, t2_2.department_210_recent_one, t2_2.department_230_recent_one, t2_2.department_250_recent_one, t2_2.department_270_recent_one, t2_2.department_310_recent_one, t2_2.department_320_recent_one, t2_2.department_330_recent_one, t2_2.department_340_recent_one, t2_2.department_350_recent_one, t2_2.department_351_recent_one, t2_2.department_352_recent_one, t2_2.department_353_recent_one, t2_2.department_354_recent_one, t2_2.department_355_recent_one, t2_2.department_360_recent_one, t2_2.department_361_recent_one, t2_2.department_362_recent_one, t2_2.department_363_recent_one, t2_2.department_364_recent_one, t2_2.department_365_recent_one, t2_2.department_366_recent_one, t2_2.department_367_recent_one, t2_2.department_370_recent_one, t2_2.department_410_recent_one, t2_2.department_425_recent_one, t2_2.department_470_recent_one, t2_2.department_480_recent_one, t2_2.department_510_recent_one, t2_2.department_520_recent_one, t2_2.department_521_recent_one, t2_2.department_526_recent_one, t2_2.department_530_recent_one, t2_2.department_540_recent_one, t2_2.department_550_recent_one, t2_2.department_560_recent_one, t2_2.department_608_recent_one, t2_2.department_610_recent_one, t2_2.department_612_recent_one, t2_2.department_615_recent_one, t2_2.department_710_recent_one, t2_2.department_800_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.purchase_channel_1st_trans_recent_two, t2_2.total_sales_recent_two_trans, t2_2.total_units_recent_two_trans, t2_2.department_minus_one_recent_two, t2_2.department_108_recent_two, t2_2.department_109_recent_two, t2_2.department_110_recent_two, t2_2.department_111_recent_two, t2_2.department_114_recent_two, t2_2.department_115_recent_two, t2_2.department_120_recent_two, t2_2.department_130_recent_two, t2_2.department_140_recent_two, t2_2.department_150_recent_two, t2_2.department_160_recent_two, t2_2.department_170_recent_two, t2_2.department_210_recent_two, t2_2.department_230_recent_two, t2_2.department_250_recent_two, t2_2.department_270_recent_two, t2_2.department_310_recent_two, t2_2.department_320_recent_two, t2_2.department_330_recent_two, t2_2.department_340_recent_two, t2_2.department_350_recent_two, t2_2.department_351_recent_two, t2_2.department_352_recent_two, t2_2.department_353_recent_two, t2_2.department_354_recent_two, t2_2.department_355_recent_two, t2_2.department_360_recent_two, t2_2.department_361_recent_two, t2_2.department_362_recent_two, t2_2.department_363_recent_two, t2_2.department_364_recent_two, t2_2.department_365_recent_two, t2_2.department_366_recent_two, t2_2.department_367_recent_two, t2_2.department_370_recent_two, t2_2.department_410_recent_two, t2_2.department_425_recent_two, t2_2.department_470_recent_two, t2_2.department_480_recent_two, t2_2.department_510_recent_two, t2_2.department_520_recent_two, t2_2.department_521_recent_two, t2_2.department_526_recent_two, t2_2.department_530_recent_two, t2_2.department_540_recent_two, t2_2.department_550_recent_two, t2_2.department_560_recent_two, t2_2.department_608_recent_two, t2_2.department_610_recent_two, t2_2.department_612_recent_two, t2_2.department_615_recent_two, t2_2.department_710_recent_two, t2_2.department_800_recent_two from crm_table_id_list_train_20201107 as t0         left join table_pred_1_crm_up_to_20201107 as t1 on t0.customer_id_hashed=t1.customer_id_hashed         left join all_NEall_id_pred_pos_2_1_pos_until_20201107 as t2_1 on t0.customer_id_hashed=t2_1.id         left join all_NEall_id_pred_pos_2_2_pos_until_20201107 as t2_2 on t0.customer_id_hashed=t2_2.id\n",
      "(1000000, 242)\n",
      "Done -- pull_train_df_X: 2020-12-31 11:33:08.501865\n",
      "0:09:20.563469\n",
      "\n",
      "start -- clean_train_univariate: 2020-12-31 11:33:08.658123\n",
      "2020-12-31 11:33:08.828840 trans_1_only col_nunique<=1 dropped: total_trans_since_registration\n",
      "2020-12-31 11:33:08.866804 trans_1_only col_nunique<=1 dropped: trans_in_store\n",
      "2020-12-31 11:33:08.897749 trans_1_only col_nunique<=1 dropped: unique_stores\n",
      "2020-12-31 11:33:08.927636 trans_1_only col_nunique<=1 dropped: trans_online\n",
      "2020-12-31 11:33:08.962009 trans_1_only col_nunique<=1 dropped: department_115_trans\n",
      "2020-12-31 11:33:08.998022 trans_1_only col_nunique<=1 dropped: department_250_trans\n",
      "2020-12-31 11:33:09.036061 trans_1_only col_nunique<=1 dropped: department_361_trans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 11:33:09.068883 trans_1_only col_nunique<=1 dropped: department_425_trans\n",
      "2020-12-31 11:33:09.097668 trans_1_only col_nunique<=1 dropped: department_526_trans\n",
      "2020-12-31 11:33:09.121948 trans_1_only col_nunique<=1 dropped: department_540_trans\n",
      "2020-12-31 11:33:09.227302 trans_1_only col_nunique<=1 dropped: department_115_1st_trans\n",
      "2020-12-31 11:33:09.319341 trans_1_only col_nunique<=1 dropped: department_250_1st_trans\n",
      "2020-12-31 11:33:09.412736 trans_1_only col_nunique<=1 dropped: department_361_1st_trans\n",
      "2020-12-31 11:33:09.498109 trans_1_only col_nunique<=1 dropped: department_425_1st_trans\n",
      "2020-12-31 11:33:09.579937 trans_1_only col_nunique<=1 dropped: department_526_1st_trans\n",
      "2020-12-31 11:33:09.653913 trans_1_only col_nunique<=1 dropped: department_540_1st_trans\n",
      "2020-12-31 11:33:09.688662 trans_1_only col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "2020-12-31 11:33:09.779642 trans_1_only col_nunique<=1 dropped: department_115_recent_one\n",
      "2020-12-31 11:33:09.864947 trans_1_only col_nunique<=1 dropped: department_250_recent_one\n",
      "2020-12-31 11:33:09.952438 trans_1_only col_nunique<=1 dropped: department_361_recent_one\n",
      "2020-12-31 11:33:10.030520 trans_1_only col_nunique<=1 dropped: department_425_recent_one\n",
      "2020-12-31 11:33:10.224557 trans_1_only col_nunique<=1 dropped: department_526_recent_one\n",
      "2020-12-31 11:33:10.402643 trans_1_only col_nunique<=1 dropped: department_540_recent_one\n",
      "2020-12-31 11:33:10.591874 trans_1_only col_nunique<=1 dropped: week_counts_to_now_recent_two\n",
      "2020-12-31 11:33:10.793995 trans_1_only col_nunique<=1 dropped: recent_two_trans_also_1st\n",
      "2020-12-31 11:33:10.999119 trans_1_only col_nunique<=1 dropped: purchase_channel_1st_trans_recent_two\n",
      "2020-12-31 11:33:11.198257 trans_1_only col_nunique<=1 dropped: total_sales_recent_two_trans\n",
      "2020-12-31 11:33:11.398291 trans_1_only col_nunique<=1 dropped: total_units_recent_two_trans\n",
      "2020-12-31 11:33:11.592458 trans_1_only col_nunique<=1 dropped: department_minus_one_recent_two\n",
      "2020-12-31 11:33:11.793646 trans_1_only col_nunique<=1 dropped: department_108_recent_two\n",
      "2020-12-31 11:33:11.992227 trans_1_only col_nunique<=1 dropped: department_109_recent_two\n",
      "2020-12-31 11:33:12.189259 trans_1_only col_nunique<=1 dropped: department_110_recent_two\n",
      "2020-12-31 11:33:12.380545 trans_1_only col_nunique<=1 dropped: department_111_recent_two\n",
      "2020-12-31 11:33:12.571348 trans_1_only col_nunique<=1 dropped: department_114_recent_two\n",
      "2020-12-31 11:33:12.759306 trans_1_only col_nunique<=1 dropped: department_115_recent_two\n",
      "2020-12-31 11:33:12.950532 trans_1_only col_nunique<=1 dropped: department_120_recent_two\n",
      "2020-12-31 11:33:13.137055 trans_1_only col_nunique<=1 dropped: department_130_recent_two\n",
      "2020-12-31 11:33:13.323637 trans_1_only col_nunique<=1 dropped: department_140_recent_two\n",
      "2020-12-31 11:33:13.504830 trans_1_only col_nunique<=1 dropped: department_150_recent_two\n",
      "2020-12-31 11:33:13.688614 trans_1_only col_nunique<=1 dropped: department_160_recent_two\n",
      "2020-12-31 11:33:13.865899 trans_1_only col_nunique<=1 dropped: department_170_recent_two\n",
      "2020-12-31 11:33:14.046117 trans_1_only col_nunique<=1 dropped: department_210_recent_two\n",
      "2020-12-31 11:33:14.221436 trans_1_only col_nunique<=1 dropped: department_230_recent_two\n",
      "2020-12-31 11:33:14.399330 trans_1_only col_nunique<=1 dropped: department_250_recent_two\n",
      "2020-12-31 11:33:14.571924 trans_1_only col_nunique<=1 dropped: department_270_recent_two\n",
      "2020-12-31 11:33:14.758773 trans_1_only col_nunique<=1 dropped: department_310_recent_two\n",
      "2020-12-31 11:33:14.932868 trans_1_only col_nunique<=1 dropped: department_320_recent_two\n",
      "2020-12-31 11:33:15.105976 trans_1_only col_nunique<=1 dropped: department_330_recent_two\n",
      "2020-12-31 11:33:15.273875 trans_1_only col_nunique<=1 dropped: department_340_recent_two\n",
      "2020-12-31 11:33:15.443104 trans_1_only col_nunique<=1 dropped: department_350_recent_two\n",
      "2020-12-31 11:33:15.609951 trans_1_only col_nunique<=1 dropped: department_351_recent_two\n",
      "2020-12-31 11:33:15.778981 trans_1_only col_nunique<=1 dropped: department_352_recent_two\n",
      "2020-12-31 11:33:15.943065 trans_1_only col_nunique<=1 dropped: department_353_recent_two\n",
      "2020-12-31 11:33:16.110646 trans_1_only col_nunique<=1 dropped: department_354_recent_two\n",
      "2020-12-31 11:33:16.270548 trans_1_only col_nunique<=1 dropped: department_355_recent_two\n",
      "2020-12-31 11:33:16.432013 trans_1_only col_nunique<=1 dropped: department_360_recent_two\n",
      "2020-12-31 11:33:16.589190 trans_1_only col_nunique<=1 dropped: department_361_recent_two\n",
      "2020-12-31 11:33:16.748745 trans_1_only col_nunique<=1 dropped: department_362_recent_two\n",
      "2020-12-31 11:33:16.904474 trans_1_only col_nunique<=1 dropped: department_363_recent_two\n",
      "2020-12-31 11:33:17.061504 trans_1_only col_nunique<=1 dropped: department_364_recent_two\n",
      "2020-12-31 11:33:17.214145 trans_1_only col_nunique<=1 dropped: department_365_recent_two\n",
      "2020-12-31 11:33:17.368178 trans_1_only col_nunique<=1 dropped: department_366_recent_two\n",
      "2020-12-31 11:33:17.515526 trans_1_only col_nunique<=1 dropped: department_367_recent_two\n",
      "2020-12-31 11:33:17.666733 trans_1_only col_nunique<=1 dropped: department_370_recent_two\n",
      "2020-12-31 11:33:17.812275 trans_1_only col_nunique<=1 dropped: department_410_recent_two\n",
      "2020-12-31 11:33:17.960686 trans_1_only col_nunique<=1 dropped: department_425_recent_two\n",
      "2020-12-31 11:33:18.103713 trans_1_only col_nunique<=1 dropped: department_470_recent_two\n",
      "2020-12-31 11:33:18.249892 trans_1_only col_nunique<=1 dropped: department_480_recent_two\n",
      "2020-12-31 11:33:18.389424 trans_1_only col_nunique<=1 dropped: department_510_recent_two\n",
      "2020-12-31 11:33:18.531560 trans_1_only col_nunique<=1 dropped: department_520_recent_two\n",
      "2020-12-31 11:33:18.670388 trans_1_only col_nunique<=1 dropped: department_521_recent_two\n",
      "2020-12-31 11:33:18.810175 trans_1_only col_nunique<=1 dropped: department_526_recent_two\n",
      "2020-12-31 11:33:18.946943 trans_1_only col_nunique<=1 dropped: department_530_recent_two\n",
      "2020-12-31 11:33:19.084120 trans_1_only col_nunique<=1 dropped: department_540_recent_two\n",
      "2020-12-31 11:33:19.219845 trans_1_only col_nunique<=1 dropped: department_550_recent_two\n",
      "2020-12-31 11:33:19.354742 trans_1_only col_nunique<=1 dropped: department_560_recent_two\n",
      "2020-12-31 11:33:19.484106 trans_1_only col_nunique<=1 dropped: department_608_recent_two\n",
      "2020-12-31 11:33:19.618655 trans_1_only col_nunique<=1 dropped: department_610_recent_two\n",
      "2020-12-31 11:33:19.747418 trans_1_only col_nunique<=1 dropped: department_612_recent_two\n",
      "2020-12-31 11:33:19.876086 trans_1_only col_nunique<=1 dropped: department_615_recent_two\n",
      "2020-12-31 11:33:20.000615 trans_1_only col_nunique<=1 dropped: department_710_recent_two\n",
      "2020-12-31 11:33:20.131193 trans_1_only col_nunique<=1 dropped: department_800_recent_two\n",
      "2020-12-31 11:36:01.354589 done self.df_train.T.drop_duplicates().T\n",
      "self.df_train_X reduced to the shape due to 0.95 variante (197449, 70)\n",
      "Done -- clean_train_univariate: 2020-12-31 11:36:03.376182\n",
      "0:02:54.718059\n",
      "\n",
      "start -- pull_train_df_Y: 2020-12-31 11:36:03.429605\n",
      "select distinct customer_id_hashed as buyers from Pred_POS_Department where sales>=0 and transaction_dt between '2020-11-08' and '2020-11-28';\n",
      "Done -- pull_train_df_Y: 2020-12-31 11:39:08.189968\n",
      "0:03:04.760363\n",
      "\n",
      "start -- remove_correlated_cols: 2020-12-31 11:39:08.196375\n",
      "Done -- remove_correlated_cols: 2020-12-31 11:39:15.941775\n",
      "0:00:07.745400\n",
      "\n",
      "start -- select_from_model_n_features: 2020-12-31 11:39:15.942086\n",
      "start 2020-12-31 11:39:16.389171 select_from_model_n_features\n",
      "Starting select_from_model_n_features:  2020-12-31 11:39:16.389630\n",
      "selector.threshold_ -inf\n",
      "df_train_X.shape (197449, 49)\n",
      "X_train_scaled.shape (197449, 49)\n",
      "Done -- select_from_model_n_features: 2020-12-31 11:41:40.084830\n",
      "0:02:24.142744\n",
      "\n",
      "start -- select_REF: 2020-12-31 11:41:40.085488\n",
      "Done select_REF:  2020-12-31 11:42:17.248059\n",
      "df_train_X.shape (197449, 40)\n",
      "X_train_scaled.shape (197449, 40)\n",
      "Done -- select_REF: 2020-12-31 11:42:17.514790\n",
      "0:00:37.429302\n",
      "\n",
      "start -- forwards_feature_elimination_based_on_p_and_vif: 2020-12-31 11:42:17.515008\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189279\n",
      "         Iterations 8\n",
      "(197449, 39) column department_114_1st_trans dropped due to p value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189280\n",
      "         Iterations 8\n",
      "(197449, 38) column department_160_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189282\n",
      "         Iterations 8\n",
      "(197449, 37) column department_330_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189284\n",
      "         Iterations 8\n",
      "(197449, 36) column department_140_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189288\n",
      "         Iterations 8\n",
      "(197449, 35) column department_364_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189291\n",
      "         Iterations 8\n",
      "(197449, 34) column department_364_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189293\n",
      "         Iterations 8\n",
      "(197449, 33) column department_320_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189297\n",
      "         Iterations 8\n",
      "(197449, 32) column department_120_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189302\n",
      "         Iterations 8\n",
      "(197449, 31) column department_150_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189307\n",
      "         Iterations 8\n",
      "(197449, 30) column department_710_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189312\n",
      "         Iterations 8\n",
      "(197449, 29) column department_130_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189318\n",
      "         Iterations 8\n",
      "(197449, 28) column department_520_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189322\n",
      "         Iterations 8\n",
      "(197449, 27) column department_608_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189328\n",
      "         Iterations 8\n",
      "(197449, 26) column department_111_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189334\n",
      "         Iterations 8\n",
      "(197449, 25) column department_310_1st_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189340\n",
      "         Iterations 8\n",
      "(197449, 24) column department_330_trans dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189347\n",
      "         Iterations 8\n",
      "Done -- forwards_feature_elimination_based_on_p_and_vif: 2020-12-31 11:44:27.133289\n",
      "0:02:09.618281\n",
      "\n",
      "start -- run_sm_logR_model: 2020-12-31 11:44:27.133692\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189347\n",
      "         Iterations 8\n",
      "Done -- run_sm_logR_model: 2020-12-31 11:44:32.154389\n",
      "0:00:05.020697\n",
      "\n",
      "start -- select_test_df_from_mysql_past: 2020-12-31 11:44:32.154712\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.S_zip, t1.signed_online, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.department_108_trans, t2_1.department_110_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_160_trans, t2_1.department_320_trans, t2_1.department_354_trans, t2_1.department_360_trans, t2_1.department_510_trans, t2_2.department_108_1st_trans, t2_2.department_210_1st_trans, t2_2.department_340_1st_trans, t2_2.department_360_1st_trans, t2_2.department_363_1st_trans, t2_2.department_367_1st_trans, t2_2.department_410_1st_trans, t2_2.department_615_1st_trans, t2_2.week_counts_to_now_recent_one from crm_table_id_list_test_20201107 as t0 left join table_pred_1_crm_up_to_20201107 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20201107 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20201107 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is null limit 3000000\n",
      "Done -- select_test_df_from_mysql_past: 2020-12-31 11:52:43.018740\n",
      "0:08:10.864028\n",
      "\n",
      "start -- run_updating_df_count: 2020-12-31 11:52:43.077995\n",
      "Done -- run_updating_df_count: 2020-12-31 11:52:43.079791\n",
      "0:00:00.001796\n",
      "\n",
      "start -- generate_DV_distribution: 2020-12-31 11:52:43.080019\n",
      "Done -- generate_DV_distribution: 2020-12-31 11:52:43.129580\n",
      "0:00:00.049561\n",
      "\n",
      "start -- pred_test_Y_past: 2020-12-31 11:52:43.130021\n",
      "Done -- pred_test_Y_past: 2020-12-31 11:52:44.291489\n",
      "0:00:01.161468\n",
      "\n",
      "start -- generate_step_table_of_test_SM: 2020-12-31 11:52:44.292212\n",
      "Done -- generate_step_table_of_test_SM: 2020-12-31 12:00:50.680385\n",
      "0:08:06.388173\n",
      "\n",
      "start -- select_best_scored_pred_prob: 2020-12-31 12:00:50.688019\n",
      "threshold_max_selfdefinedscore 0.061\n",
      "Done -- select_best_scored_pred_prob: 2020-12-31 12:00:50.697149\n",
      "0:00:00.009130\n",
      "\n",
      "start -- generate_gain_chart_past: 2020-12-31 12:00:50.697515\n",
      "Done -- generate_gain_chart_past: 2020-12-31 12:00:58.097445\n",
      "0:00:07.399930\n",
      "\n",
      "start -- check_shopper_type_past: 2020-12-31 12:00:58.097742\n",
      "new sign up date range below: \n",
      " 2020-10-11 2020-11-07\n",
      "Done -- check_shopper_type_past: 2020-12-31 12:01:58.614560\n",
      "0:01:00.516818\n",
      "\n",
      "start -- save_outputs_past: 2020-12-31 12:01:58.840335\n",
      "Done -- save_outputs_past: 2020-12-31 12:02:15.377734\n",
      "0:00:16.537399\n",
      "\n",
      "start -- select_target_df_from_mysql_current: 2020-12-31 12:02:15.378343\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.S_zip, t1.signed_online, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.department_108_trans, t2_1.department_110_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_160_trans, t2_1.department_320_trans, t2_1.department_354_trans, t2_1.department_360_trans, t2_1.department_510_trans, t2_2.department_108_1st_trans, t2_2.department_210_1st_trans, t2_2.department_340_1st_trans, t2_2.department_360_1st_trans, t2_2.department_363_1st_trans, t2_2.department_367_1st_trans, t2_2.department_410_1st_trans, t2_2.department_615_1st_trans, t2_2.week_counts_to_now_recent_one from crm_table_id_list_test_20201128 as t0 left join table_pred_1_crm_up_to_20201128 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20201128 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20201128 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is null\n",
      "Done -- select_target_df_from_mysql_current: 2020-12-31 12:14:05.691916\n",
      "0:11:50.313573\n",
      "\n",
      "start -- pred_target_Y_current_week: 2020-12-31 12:14:05.692299\n",
      "Done -- pred_target_Y_current_week: 2020-12-31 12:14:07.688994\n",
      "0:00:01.996695\n",
      "\n",
      "start -- generate_and_write_target_id_with_label: 2020-12-31 12:14:07.689673\n",
      "new sign up date range below: \n",
      " 2020-11-01 2020-11-28\n",
      "Done -- generate_and_write_target_id_with_label: 2020-12-31 12:15:27.590083\n",
      "0:01:19.900410\n",
      "\n",
      "trans_1_only DV 3 done:  2020-12-31 12:15:27.841094\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "n_week_DV=3\n",
    "key_df_type=\"trans_1_only\"\n",
    "\n",
    "\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type\n",
    "                              )\n",
    "# 2\n",
    "SM_Logistic_Model_dvN.pull_train_df_X(SM_Logistic_Model_dvN)\n",
    "# 3\n",
    "SM_Logistic_Model_dvN.clean_train_univariate(SM_Logistic_Model_dvN,r_variance=0.95)\n",
    "# 4\n",
    "SM_Logistic_Model_dvN.pull_train_df_Y(SM_Logistic_Model_dvN)\n",
    "# 5\n",
    "SM_Logistic_Model_dvN.remove_correlated_cols(SM_Logistic_Model_dvN,coorelation_threshold=0.8)\n",
    "# 6\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN, \n",
    "                                                   N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "# 7\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "# 8\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "# 9\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "# 10\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql_past(SM_Logistic_Model_dvN,n_limit_test=3*10**6)\n",
    "# 11\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "# 12\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "# 13\n",
    "SM_Logistic_Model_dvN.pred_test_Y_past(SM_Logistic_Model_dvN)\n",
    "# 14\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "# 15\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "# 16\n",
    "SM_Logistic_Model_dvN.generate_gain_chart_past(SM_Logistic_Model_dvN)\n",
    "# 17\n",
    "SM_Logistic_Model_dvN.check_shopper_type_past(SM_Logistic_Model_dvN)\n",
    "# 18\n",
    "SM_Logistic_Model_dvN.save_outputs_past(SM_Logistic_Model_dvN)\n",
    "# 19\n",
    "SM_Logistic_Model_dvN.select_target_df_from_mysql_current(SM_Logistic_Model_dvN,n_limit_test=None)\n",
    "# 20\n",
    "SM_Logistic_Model_dvN.pred_target_Y_current_week(SM_Logistic_Model_dvN)\n",
    "# 21\n",
    "SM_Logistic_Model_dvN.generate_and_write_target_id_with_label(SM_Logistic_Model_dvN)\n",
    "        \n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_2_plus DV 2 start:  2020-12-31 12:15:27.852209\n",
      "start -- __init__: 2020-12-31 12:15:27.852370\n",
      "Done -- __init__: 2020-12-31 12:15:29.207874\n",
      "0:00:01.355504\n",
      "\n",
      "start -- pull_train_df_X: 2020-12-31 12:15:29.222206\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.S_zip, t1.else_10_zip, t1.signed_online, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.total_trans_since_registration, t2_1.total_items, t2_1.total_sales, t2_1.trans_in_store, t2_1.unique_stores, t2_1.trans_online, t2_1.department_minus_one_trans, t2_1.department_108_trans, t2_1.department_109_trans, t2_1.department_110_trans, t2_1.department_111_trans, t2_1.department_114_trans, t2_1.department_115_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_150_trans, t2_1.department_160_trans, t2_1.department_170_trans, t2_1.department_210_trans, t2_1.department_230_trans, t2_1.department_250_trans, t2_1.department_270_trans, t2_1.department_310_trans, t2_1.department_320_trans, t2_1.department_330_trans, t2_1.department_340_trans, t2_1.department_350_trans, t2_1.department_351_trans, t2_1.department_352_trans, t2_1.department_353_trans, t2_1.department_354_trans, t2_1.department_355_trans, t2_1.department_360_trans, t2_1.department_361_trans, t2_1.department_362_trans, t2_1.department_363_trans, t2_1.department_364_trans, t2_1.department_365_trans, t2_1.department_366_trans, t2_1.department_367_trans, t2_1.department_370_trans, t2_1.department_410_trans, t2_1.department_425_trans, t2_1.department_470_trans, t2_1.department_480_trans, t2_1.department_510_trans, t2_1.department_520_trans, t2_1.department_521_trans, t2_1.department_526_trans, t2_1.department_530_trans, t2_1.department_540_trans, t2_1.department_550_trans, t2_1.department_560_trans, t2_1.department_608_trans, t2_1.department_610_trans, t2_1.department_612_trans, t2_1.department_615_trans, t2_1.department_710_trans, t2_1.department_800_trans, t2_2.purchase_channel_1st_trans, t2_2.total_sales_1st_trans, t2_2.total_units_1st_trans, t2_2.department_minus_one_1st_trans, t2_2.department_108_1st_trans, t2_2.department_109_1st_trans, t2_2.department_110_1st_trans, t2_2.department_111_1st_trans, t2_2.department_114_1st_trans, t2_2.department_115_1st_trans, t2_2.department_120_1st_trans, t2_2.department_130_1st_trans, t2_2.department_140_1st_trans, t2_2.department_150_1st_trans, t2_2.department_160_1st_trans, t2_2.department_170_1st_trans, t2_2.department_210_1st_trans, t2_2.department_230_1st_trans, t2_2.department_250_1st_trans, t2_2.department_270_1st_trans, t2_2.department_310_1st_trans, t2_2.department_320_1st_trans, t2_2.department_330_1st_trans, t2_2.department_340_1st_trans, t2_2.department_350_1st_trans, t2_2.department_351_1st_trans, t2_2.department_352_1st_trans, t2_2.department_353_1st_trans, t2_2.department_354_1st_trans, t2_2.department_355_1st_trans, t2_2.department_360_1st_trans, t2_2.department_361_1st_trans, t2_2.department_362_1st_trans, t2_2.department_363_1st_trans, t2_2.department_364_1st_trans, t2_2.department_365_1st_trans, t2_2.department_366_1st_trans, t2_2.department_367_1st_trans, t2_2.department_370_1st_trans, t2_2.department_410_1st_trans, t2_2.department_425_1st_trans, t2_2.department_470_1st_trans, t2_2.department_480_1st_trans, t2_2.department_510_1st_trans, t2_2.department_520_1st_trans, t2_2.department_521_1st_trans, t2_2.department_526_1st_trans, t2_2.department_530_1st_trans, t2_2.department_540_1st_trans, t2_2.department_550_1st_trans, t2_2.department_560_1st_trans, t2_2.department_608_1st_trans, t2_2.department_610_1st_trans, t2_2.department_612_1st_trans, t2_2.department_615_1st_trans, t2_2.department_710_1st_trans, t2_2.department_800_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.recent_one_trans_also_1st, t2_2.purchase_channel_1st_trans_recent_one, t2_2.total_sales_recent_one_trans, t2_2.total_units_recent_one_trans, t2_2.department_minus_one_recent_one, t2_2.department_108_recent_one, t2_2.department_109_recent_one, t2_2.department_110_recent_one, t2_2.department_111_recent_one, t2_2.department_114_recent_one, t2_2.department_115_recent_one, t2_2.department_120_recent_one, t2_2.department_130_recent_one, t2_2.department_140_recent_one, t2_2.department_150_recent_one, t2_2.department_160_recent_one, t2_2.department_170_recent_one, t2_2.department_210_recent_one, t2_2.department_230_recent_one, t2_2.department_250_recent_one, t2_2.department_270_recent_one, t2_2.department_310_recent_one, t2_2.department_320_recent_one, t2_2.department_330_recent_one, t2_2.department_340_recent_one, t2_2.department_350_recent_one, t2_2.department_351_recent_one, t2_2.department_352_recent_one, t2_2.department_353_recent_one, t2_2.department_354_recent_one, t2_2.department_355_recent_one, t2_2.department_360_recent_one, t2_2.department_361_recent_one, t2_2.department_362_recent_one, t2_2.department_363_recent_one, t2_2.department_364_recent_one, t2_2.department_365_recent_one, t2_2.department_366_recent_one, t2_2.department_367_recent_one, t2_2.department_370_recent_one, t2_2.department_410_recent_one, t2_2.department_425_recent_one, t2_2.department_470_recent_one, t2_2.department_480_recent_one, t2_2.department_510_recent_one, t2_2.department_520_recent_one, t2_2.department_521_recent_one, t2_2.department_526_recent_one, t2_2.department_530_recent_one, t2_2.department_540_recent_one, t2_2.department_550_recent_one, t2_2.department_560_recent_one, t2_2.department_608_recent_one, t2_2.department_610_recent_one, t2_2.department_612_recent_one, t2_2.department_615_recent_one, t2_2.department_710_recent_one, t2_2.department_800_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.purchase_channel_1st_trans_recent_two, t2_2.total_sales_recent_two_trans, t2_2.total_units_recent_two_trans, t2_2.department_minus_one_recent_two, t2_2.department_108_recent_two, t2_2.department_109_recent_two, t2_2.department_110_recent_two, t2_2.department_111_recent_two, t2_2.department_114_recent_two, t2_2.department_115_recent_two, t2_2.department_120_recent_two, t2_2.department_130_recent_two, t2_2.department_140_recent_two, t2_2.department_150_recent_two, t2_2.department_160_recent_two, t2_2.department_170_recent_two, t2_2.department_210_recent_two, t2_2.department_230_recent_two, t2_2.department_250_recent_two, t2_2.department_270_recent_two, t2_2.department_310_recent_two, t2_2.department_320_recent_two, t2_2.department_330_recent_two, t2_2.department_340_recent_two, t2_2.department_350_recent_two, t2_2.department_351_recent_two, t2_2.department_352_recent_two, t2_2.department_353_recent_two, t2_2.department_354_recent_two, t2_2.department_355_recent_two, t2_2.department_360_recent_two, t2_2.department_361_recent_two, t2_2.department_362_recent_two, t2_2.department_363_recent_two, t2_2.department_364_recent_two, t2_2.department_365_recent_two, t2_2.department_366_recent_two, t2_2.department_367_recent_two, t2_2.department_370_recent_two, t2_2.department_410_recent_two, t2_2.department_425_recent_two, t2_2.department_470_recent_two, t2_2.department_480_recent_two, t2_2.department_510_recent_two, t2_2.department_520_recent_two, t2_2.department_521_recent_two, t2_2.department_526_recent_two, t2_2.department_530_recent_two, t2_2.department_540_recent_two, t2_2.department_550_recent_two, t2_2.department_560_recent_two, t2_2.department_608_recent_two, t2_2.department_610_recent_two, t2_2.department_612_recent_two, t2_2.department_615_recent_two, t2_2.department_710_recent_two, t2_2.department_800_recent_two from crm_table_id_list_train_20201114 as t0         left join table_pred_1_crm_up_to_20201114 as t1 on t0.customer_id_hashed=t1.customer_id_hashed         left join all_NEall_id_pred_pos_2_1_pos_until_20201114 as t2_1 on t0.customer_id_hashed=t2_1.id         left join all_NEall_id_pred_pos_2_2_pos_until_20201114 as t2_2 on t0.customer_id_hashed=t2_2.id\n",
      "(1000000, 242)\n",
      "Done -- pull_train_df_X: 2020-12-31 12:28:22.316183\n",
      "0:12:53.093977\n",
      "\n",
      "start -- clean_train_univariate: 2020-12-31 12:28:22.482006\n",
      "2020-12-31 12:28:23.180118 trans_2_plus col_nunique<=1 dropped: department_115_trans\n",
      "2020-12-31 12:28:23.740013 trans_2_plus col_nunique<=1 dropped: department_115_1st_trans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 12:28:24.811659 trans_2_plus col_nunique<=1 dropped: department_540_1st_trans\n",
      "2020-12-31 12:28:24.959021 trans_2_plus col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "2020-12-31 12:28:25.852452 trans_2_plus col_nunique<=1 dropped: department_115_recent_one\n",
      "2020-12-31 12:28:27.015763 trans_2_plus col_nunique<=1 dropped: department_540_recent_one\n",
      "2020-12-31 12:28:27.878519 trans_2_plus col_nunique<=1 dropped: department_115_recent_two\n",
      "2020-12-31 12:28:28.770495 trans_2_plus col_nunique<=1 dropped: department_526_recent_two\n",
      "2020-12-31 12:28:29.450755 trans_2_plus col_nunique<=1 dropped: department_540_recent_two\n",
      "2020-12-31 13:41:28.603871 done self.df_train.T.drop_duplicates().T\n",
      "self.df_train_X reduced to the shape due to 0.95 variante (802965, 174)\n",
      "Done -- clean_train_univariate: 2020-12-31 13:41:50.274253\n",
      "1:13:27.792247\n",
      "\n",
      "start -- pull_train_df_Y: 2020-12-31 13:41:50.979938\n",
      "select distinct customer_id_hashed as buyers from Pred_POS_Department where sales>=0 and transaction_dt between '2020-11-15' and '2020-11-28';\n",
      "Done -- pull_train_df_Y: 2020-12-31 13:44:02.864283\n",
      "0:02:11.884345\n",
      "\n",
      "start -- remove_correlated_cols: 2020-12-31 13:44:02.885665\n",
      "department_110_trans removed due to high coor with others\n",
      "total_trans_since_registration removed due to high coor with others\n",
      "Done -- remove_correlated_cols: 2020-12-31 13:47:33.131206\n",
      "0:03:30.245541\n",
      "\n",
      "start -- select_from_model_n_features: 2020-12-31 13:47:33.132017\n",
      "start 2020-12-31 13:47:41.243769 select_from_model_n_features\n",
      "Starting select_from_model_n_features:  2020-12-31 13:47:41.243935\n",
      "selector.threshold_ -inf\n",
      "df_train_X.shape (802965, 60)\n",
      "X_train_scaled.shape (802965, 60)\n",
      "Done -- select_from_model_n_features: 2020-12-31 15:04:58.714277\n",
      "1:17:25.582260\n",
      "\n",
      "start -- select_REF: 2020-12-31 15:04:58.714571\n",
      "Done select_REF:  2020-12-31 15:15:57.640656\n",
      "df_train_X.shape (802965, 40)\n",
      "X_train_scaled.shape (802965, 40)\n",
      "Done -- select_REF: 2020-12-31 15:15:58.729627\n",
      "0:11:00.015056\n",
      "\n",
      "start -- forwards_feature_elimination_based_on_p_and_vif: 2020-12-31 15:15:58.730171\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.368852\n",
      "         Iterations 7\n",
      "(802965, 39) column total_sales dropped due to high vif\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.368914\n",
      "         Iterations 7\n",
      "(802965, 38) column trans_in_store dropped due to high vif\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369884\n",
      "         Iterations 7\n",
      "(802965, 37) column total_items dropped due to high vif\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369904\n",
      "         Iterations 7\n",
      "(802965, 36) column department_111_recent_two dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369904\n",
      "         Iterations 7\n",
      "(802965, 35) column department_108_recent_one dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369904\n",
      "         Iterations 7\n",
      "(802965, 34) column department_150_recent_one dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369905\n",
      "         Iterations 7\n",
      "Done -- forwards_feature_elimination_based_on_p_and_vif: 2020-12-31 15:20:44.464640\n",
      "0:04:45.734469\n",
      "\n",
      "start -- run_sm_logR_model: 2020-12-31 15:20:44.465002\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369905\n",
      "         Iterations 7\n",
      "Done -- run_sm_logR_model: 2020-12-31 15:21:19.520026\n",
      "0:00:35.055024\n",
      "\n",
      "start -- select_test_df_from_mysql_past: 2020-12-31 15:21:19.520752\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.department_108_trans, t2_1.department_111_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_150_trans, t2_1.department_340_trans, t2_1.department_354_trans, t2_1.department_360_trans, t2_1.department_362_trans, t2_1.department_367_trans, t2_1.department_470_trans, t2_1.department_480_trans, t2_1.department_510_trans, t2_1.department_520_trans, t2_1.department_530_trans, t2_1.department_608_trans, t2_1.department_612_trans, t2_1.department_615_trans, t2_2.total_sales_1st_trans, t2_2.total_units_1st_trans, t2_2.department_360_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.total_sales_recent_one_trans, t2_2.department_110_recent_one, t2_2.department_354_recent_one, t2_2.department_360_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.total_sales_recent_two_trans, t2_2.department_360_recent_two, t2_2.department_367_recent_two from crm_table_id_list_test_20201114 as t0 left join table_pred_1_crm_up_to_20201114 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20201114 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20201114 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is not null limit 3000000\n",
      "Done -- select_test_df_from_mysql_past: 2020-12-31 15:24:58.965015\n",
      "0:03:39.444263\n",
      "\n",
      "start -- run_updating_df_count: 2020-12-31 15:24:59.027521\n",
      "Done -- run_updating_df_count: 2020-12-31 15:24:59.029041\n",
      "0:00:00.001520\n",
      "\n",
      "start -- generate_DV_distribution: 2020-12-31 15:24:59.029521\n",
      "Done -- generate_DV_distribution: 2020-12-31 15:24:59.101357\n",
      "0:00:00.071836\n",
      "\n",
      "start -- pred_test_Y_past: 2020-12-31 15:24:59.101901\n",
      "Done -- pred_test_Y_past: 2020-12-31 15:25:03.260702\n",
      "0:00:04.158801\n",
      "\n",
      "start -- generate_step_table_of_test_SM: 2020-12-31 15:25:03.261047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:530: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done -- generate_step_table_of_test_SM: 2020-12-31 15:35:08.509244\n",
      "0:10:05.248197\n",
      "\n",
      "start -- select_best_scored_pred_prob: 2020-12-31 15:35:08.517299\n",
      "threshold_max_selfdefinedscore 0.13\n",
      "Done -- select_best_scored_pred_prob: 2020-12-31 15:35:08.528412\n",
      "0:00:00.011113\n",
      "\n",
      "start -- generate_gain_chart_past: 2020-12-31 15:35:08.528770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done -- generate_gain_chart_past: 2020-12-31 15:35:21.933560\n",
      "0:00:13.404790\n",
      "\n",
      "start -- check_shopper_type_past: 2020-12-31 15:35:21.933852\n",
      "new sign up date range below: \n",
      " 2020-10-18 2020-11-14\n",
      "Done -- check_shopper_type_past: 2020-12-31 15:36:25.837075\n",
      "0:01:03.903223\n",
      "\n",
      "start -- save_outputs_past: 2020-12-31 15:36:26.084323\n",
      "Done -- save_outputs_past: 2020-12-31 15:36:45.367288\n",
      "0:00:19.282965\n",
      "\n",
      "start -- select_target_df_from_mysql_current: 2020-12-31 15:36:45.368400\n",
      "SELECT t0.customer_id_hashed, t1.P_zip, t1.email_unsub_label, t2_1.weeks_since_sign_up, t2_1.department_108_trans, t2_1.department_111_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_150_trans, t2_1.department_340_trans, t2_1.department_354_trans, t2_1.department_360_trans, t2_1.department_362_trans, t2_1.department_367_trans, t2_1.department_470_trans, t2_1.department_480_trans, t2_1.department_510_trans, t2_1.department_520_trans, t2_1.department_530_trans, t2_1.department_608_trans, t2_1.department_612_trans, t2_1.department_615_trans, t2_2.total_sales_1st_trans, t2_2.total_units_1st_trans, t2_2.department_360_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.total_sales_recent_one_trans, t2_2.department_110_recent_one, t2_2.department_354_recent_one, t2_2.department_360_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.total_sales_recent_two_trans, t2_2.department_360_recent_two, t2_2.department_367_recent_two from crm_table_id_list_test_20201128 as t0 left join table_pred_1_crm_up_to_20201128 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20201128 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20201128 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is not null\n",
      "Done -- select_target_df_from_mysql_current: 2020-12-31 15:57:13.927552\n",
      "0:20:28.559152\n",
      "\n",
      "start -- pred_target_Y_current_week: 2020-12-31 15:57:13.941899\n",
      "Done -- pred_target_Y_current_week: 2020-12-31 15:57:39.010942\n",
      "0:00:25.069043\n",
      "\n",
      "start -- generate_and_write_target_id_with_label: 2020-12-31 15:57:39.011246\n",
      "new sign up date range below: \n",
      " 2020-11-01 2020-11-28\n",
      "Done -- generate_and_write_target_id_with_label: 2020-12-31 16:00:44.892856\n",
      "0:03:05.881610\n",
      "\n",
      "trans_2_plus DV 2 done:  2020-12-31 16:00:46.534308\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "n_week_DV=2\n",
    "key_df_type=\"trans_2_plus\"\n",
    "\n",
    "\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type\n",
    "                              )\n",
    "# 2\n",
    "SM_Logistic_Model_dvN.pull_train_df_X(SM_Logistic_Model_dvN)\n",
    "# 3\n",
    "SM_Logistic_Model_dvN.clean_train_univariate(SM_Logistic_Model_dvN,r_variance=0.95)\n",
    "# 4\n",
    "SM_Logistic_Model_dvN.pull_train_df_Y(SM_Logistic_Model_dvN)\n",
    "# 5\n",
    "SM_Logistic_Model_dvN.remove_correlated_cols(SM_Logistic_Model_dvN,coorelation_threshold=0.8)\n",
    "# 6\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN, \n",
    "                                                   N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "# 7\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "# 8\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "# 9\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "# 10\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql_past(SM_Logistic_Model_dvN,n_limit_test=3*10**6)\n",
    "# 11\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "# 12\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "# 13\n",
    "SM_Logistic_Model_dvN.pred_test_Y_past(SM_Logistic_Model_dvN)\n",
    "# 14\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "# 15\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "# 16\n",
    "SM_Logistic_Model_dvN.generate_gain_chart_past(SM_Logistic_Model_dvN)\n",
    "# 17\n",
    "SM_Logistic_Model_dvN.check_shopper_type_past(SM_Logistic_Model_dvN)\n",
    "# 18\n",
    "SM_Logistic_Model_dvN.save_outputs_past(SM_Logistic_Model_dvN)\n",
    "# 19\n",
    "SM_Logistic_Model_dvN.select_target_df_from_mysql_current(SM_Logistic_Model_dvN,n_limit_test=None)\n",
    "# 20\n",
    "SM_Logistic_Model_dvN.pred_target_Y_current_week(SM_Logistic_Model_dvN)\n",
    "# 21\n",
    "SM_Logistic_Model_dvN.generate_and_write_target_id_with_label(SM_Logistic_Model_dvN)\n",
    "        \n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "\n",
      "Finished predict_with_current_week_data: 2020-12-31 16:00:46.539579\n",
      "***************\n",
      "\n",
      "Duration: 4:36:59.254931\n"
     ]
    }
   ],
   "source": [
    "task_end_time=datetime.datetime.now()\n",
    "\n",
    "print(\"***************\\n\")\n",
    "print(\"Finished predict_with_current_week_data: %s\"%str(task_end_time))\n",
    "print(\"***************\\n\")\n",
    "print(\"Duration: %s\"%str(task_end_time-task_start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
