{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_output_df(input_zip_list,df_store_list,zip_centers):\n",
    "    import sqlalchemy as sql\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from haversine import haversine\n",
    "    import glob\n",
    "    import random\n",
    "    from multiprocessing import Pool\n",
    "    from itertools import repeat\n",
    "\n",
    "\n",
    "    df_output=pd.DataFrame()\n",
    "\n",
    "    for zip_cd in input_zip_list:\n",
    "        z_centroid=zip_centers[zip_cd]\n",
    "        min_dist=np.inf\n",
    "        nearest_store=None\n",
    "\n",
    "        for i,row in df_store_list.iterrows():\n",
    "            store=row['location_id']\n",
    "            store_loc=(row['latitude_meas'], row['longitude_meas'])\n",
    "            dist=haversine(z_centroid,store_loc,unit=\"mi\")\n",
    "            if dist<=min_dist:\n",
    "                min_dist=dist\n",
    "                nearest_store=store\n",
    "        df=pd.DataFrame({\"nearest_BL_store\":nearest_store,\"nearest_BL_dist\":min_dist},index=[zip_cd])\n",
    "        df=df.reset_index().rename(columns={\"index\":\"zip_cd\"})\n",
    "        df_output=df_output.append(df)\n",
    "    return df_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_crm_train_test():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start_Part_2: 2020-12-26 23:53:25.682467\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sql\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from haversine import haversine\n",
    "import glob\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "from dateutil.relativedelta import relativedelta\n",
    "print(\"Start_Part_2: %s\"%str(datetime.datetime.now()))\n",
    "\n",
    "with open('./config.json', 'rb') as f:\n",
    "    dict_config = json.load(f)\n",
    "\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "folder_store_list=dict_config['folder_store_list']\n",
    "path_TA_excel=dict_config['path_TA_excel']\n",
    "path_json_zip_center=dict_config['path_json_zip_center']\n",
    "pos_end_date=dict_config['pos_end_date']\n",
    "folder_store_list=dict_config['folder_store_list']\n",
    "folder_email_unsub=dict_config['folder_email_unsub']\n",
    "train_sample_size=dict_config['train_sample_size'] #1000000\n",
    "test_ratio=dict_config['test_ratio'] #0.25\n",
    "\n",
    "\n",
    "with open('./table_names_%s.json'%str(pos_end_date).replace(\"-\",\"\"), 'rb') as f:\n",
    "    dict_table_names = json.load(f)\n",
    "table_filtered_crm=dict_table_names['table_filtered_crm']\n",
    "\n",
    "\n",
    "BL_engine=sql.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def create_index(table_name, list_of_columns):\n",
    "    columns = ', '.join(list_of_columns)\n",
    "    query = \"CREATE INDEX id_index ON %s(%s)\" % (table_name, columns)\n",
    "    print(query)\n",
    "    with BL_engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "        result.close()\n",
    "    return\n",
    "def week_end_dt(date_input):\n",
    "    weekday_int=date_input.weekday()\n",
    "    if weekday_int==6:\n",
    "        return date_input+datetime.timedelta(days=6)\n",
    "    else:\n",
    "        return date_input+datetime.timedelta(days=5-weekday_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point 1\n"
     ]
    }
   ],
   "source": [
    "high_date=datetime.datetime.strptime(dict_config['crm_end_date'],\"%Y-%m-%d\").date()\n",
    "if dict_config['recent_n_month']:\n",
    "    recent_n_month=dict_config['recent_n_month']\n",
    "    pos_start_date_id_filter = str(high_date-datetime.timedelta(days=int(np.ceil(365*recent_n_month/12))))\n",
    "else:\n",
    "    pos_start_date_id_filter = dict_config[\"pos_start_date\"]\n",
    "\n",
    "sql_str_high_date=\"'%s'\"%str(high_date)\n",
    "sql_str_lastweekstart_date=\"'%s'\"%str(high_date-datetime.timedelta(days=6))\n",
    "# sql_sign_up_start_date=\"'%s'\"%str(sign_up_start_date)\n",
    "sql_POS_start_date=\"'%s'\"%str(pos_start_date_id_filter)\n",
    "str_week_end_d=str(high_date).replace(\"-\",\"\")\n",
    "print(\"check point 1\")\n",
    "\n",
    "\n",
    "path_store_list=glob.glob(folder_store_list+\"*.txt\")\n",
    "path_store_list.sort()\n",
    "path_store_list_ahead=[x for x in path_store_list if \"MediaStormStores%s\"%str_week_end_d[:6] in x][0]\n",
    "# updated 2020-10-03\n",
    "str_month_after=(datetime.datetime.strptime(str_week_end_d, '%Y%m%d') + relativedelta(months=1)).date()\n",
    "str_month_after=str(str_month_after).replace(\"-\",\"\")\n",
    "# path_store_list_after=[x for x in path_store_list if \"MediaStormStores%s\"%str_month_after in x][0]\n",
    "\n",
    "df_store_list=pd.read_csv(path_store_list_ahead,sep=\"|\")\n",
    "df_store_list=df_store_list[['location_id','address_line_1','address_line_2','city_nm','state_nm','zip_cd','latitude_meas','longitude_meas']]\n",
    "df_store_list['latitude_meas']=df_store_list['latitude_meas'].astype(float)\n",
    "df_store_list['longitude_meas']=df_store_list['longitude_meas'].astype(float)\n",
    "df_store_list['zip_cd']=df_store_list['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "df_store_list=df_store_list[~df_store_list['location_id'].isin(['145','6990'])]\n",
    "df_store_list['location_id']=df_store_list['location_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-05-23'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_start_date_id_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'S', 'zip_10']\n",
      "check point 2\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "TA_zips=pd.ExcelFile(path_TA_excel)\n",
    "TA_zips=TA_zips.parse(\"view_by_store\",dtype=str)\n",
    "\n",
    "df_temporary=TA_zips[['location_id','trans_P_zips_70_within_TA','trans_S_zips_70_within_TA','zips_in_10']]\n",
    "df_zip_by_store=pd.DataFrame()\n",
    "\n",
    "for ind,row in df_temporary.iterrows():\n",
    "    location_id=str(row['location_id'])\n",
    "    P_zips=eval(row['trans_P_zips_70_within_TA'])\n",
    "    S_zips=eval(row['trans_S_zips_70_within_TA'])\n",
    "    zip_10=eval(row['zips_in_10'])\n",
    "\n",
    "\n",
    "    df_P=pd.DataFrame(zip([location_id]*len(P_zips),P_zips))\n",
    "    if len(df_P)>0:\n",
    "        df_P.columns=['location_id','zip_cd']\n",
    "        df_P['zip_type']=\"P\"\n",
    "\n",
    "    df_S=pd.DataFrame(zip([location_id]*len(S_zips),S_zips))\n",
    "    if len(df_S)>0:\n",
    "        df_S.columns=['location_id','zip_cd']\n",
    "        df_S['zip_type']=\"S\"\n",
    "\n",
    "    df_10=pd.DataFrame(zip([location_id]*len(zip_10),zip_10))\n",
    "    if len(df_10)>0:\n",
    "        df_10.columns=['location_id','zip_cd']\n",
    "        df_10['zip_type']=\"zip_10\"\n",
    "\n",
    "    df_zip_by_store=df_zip_by_store.append(df_P).append(df_S).append(df_10)\n",
    "df_zip_by_store['location_id']=df_zip_by_store['location_id'].astype(str)\n",
    "df_store_list=df_store_list[['location_id','latitude_meas','longitude_meas']]\n",
    "df_store_zip=pd.merge(df_store_list,df_zip_by_store,on=\"location_id\",how=\"left\")\n",
    "df_store_zip_new=df_store_zip[pd.isnull(df_store_zip['zip_cd'])]\n",
    "df_store_zip_existing=df_store_zip[pd.notnull(df_store_zip['zip_cd'])]\n",
    "\n",
    "df_store_zip_new_no_loc=df_store_zip_new[df_store_zip_new['latitude_meas']==0]\n",
    "df_store_zip_new_with_loc=df_store_zip_new[df_store_zip_new['latitude_meas']!=0]\n",
    "df_store_zip_new_with_loc=df_store_zip_new_with_loc[['location_id','latitude_meas','longitude_meas']]\n",
    "df_store_zip_new_no_loc=df_store_zip_new_no_loc[['location_id','latitude_meas','longitude_meas']]\n",
    "if len(df_store_zip_new_no_loc)>0:\n",
    "    store_list_later=[x for x in path_store_list if x.split(\"MediaStormStores\")[1][:6]>str_week_end_d]\n",
    "    store_list_later=sorted(store_list_later,key=lambda x: os.stat(x).st_mtime)\n",
    "    for file in store_list_later:\n",
    "        df=pd.read_csv(file,dtype=str,sep=\"|\",usecols=['location_id','latitude_meas','longitude_meas'])\n",
    "        df=df[['location_id','latitude_meas','longitude_meas']]\n",
    "        df['latitude_meas']=df['latitude_meas'].astype(float)\n",
    "        df['longitude_meas']=df['longitude_meas'].astype(float)\n",
    "        df['location_id']=df['location_id'].astype(str)\n",
    "        df=df[df['location_id'].isin(df_store_zip_new_no_loc['location_id'].tolist())]\n",
    "        df=df[df['latitude_meas']!=0]\n",
    "        df_store_zip_new_with_loc=df_store_zip_new_with_loc.append(df)\n",
    "        df_store_zip_new_no_loc=df_store_zip_new_no_loc[~df_store_zip_new_no_loc['location_id'].isin(df['location_id'].tolist())]\n",
    "        if len(df_store_zip_new_no_loc)==0:\n",
    "            break\n",
    "    df_store_zip_new=df_store_zip_new_with_loc.reset_index()\n",
    "    del df_store_zip_new['index']\n",
    "    if len(df_store_zip_new_with_loc)>0:\n",
    "        del df_store_zip_new_with_loc\n",
    "    if len(df_store_zip_new_no_loc)>0:\n",
    "        del df_store_zip_new_no_loc\n",
    "\n",
    "zip_centers=json.load(open(path_json_zip_center,\"r\"))\n",
    "if len(df_store_zip_new)>0:\n",
    "\n",
    "\n",
    "    df_all_new_zip=pd.DataFrame()\n",
    "    for i,row in df_store_zip_new.iterrows():\n",
    "        store_coor=(row['latitude_meas'],row['longitude_meas'])\n",
    "        store_num=row['location_id']\n",
    "        list_store_zip=[]\n",
    "        for zip_cd, v in zip_centers.items():\n",
    "            dist=haversine(store_coor,v,unit=\"mi\")\n",
    "            if dist<=10:\n",
    "                list_store_zip.append(zip_cd)\n",
    "        df=pd.DataFrame({\"zip_cd\":list_store_zip,\"zip_type\":[\"zip_10\"]*len(list_store_zip)},index=[store_num]*len(list_store_zip))\n",
    "        df=df.reset_index().rename(columns={\"index\":\"location_id\"})\n",
    "        df_all_new_zip=df_all_new_zip.append(df)\n",
    "\n",
    "    df_store_zip_new=pd.merge(df_store_zip_new,df_all_new_zip,on=\"location_id\",how=\"left\")\n",
    "\n",
    "    df_store_zip=df_store_zip_existing.append(df_store_zip_new)\n",
    "else:\n",
    "    df_store_zip=df_store_zip_existing\n",
    "df_zip_type=df_store_zip[['zip_cd','zip_type']].drop_duplicates()\n",
    "df_zip_type=df_zip_type.sort_values(['zip_cd','zip_type'])\n",
    "print(df_zip_type['zip_type'].unique().tolist())\n",
    "df_unique_zip_type=df_zip_type.drop_duplicates(\"zip_cd\")\n",
    "\n",
    "list_P_zips=df_zip_type[df_zip_type['zip_type']==\"P\"]['zip_cd'].tolist()\n",
    "list_S_zips=df_zip_type[df_zip_type['zip_type']==\"S\"]['zip_cd'].tolist()\n",
    "list_10_zips=df_zip_type[df_zip_type['zip_type']==\"zip_10\"]['zip_cd'].tolist()\n",
    "\n",
    "df_store_list=df_store_zip[['location_id','latitude_meas','longitude_meas']].drop_duplicates().reset_index()\n",
    "del df_store_list['index']\n",
    "df_store_list=df_store_zip[['location_id','latitude_meas','longitude_meas']].drop_duplicates().reset_index()\n",
    "del df_store_list['index']\n",
    "# \n",
    "print(\"check point 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point 3\n",
      "(39647, 3) 39647 1411\n"
     ]
    }
   ],
   "source": [
    "processors=20\n",
    "\n",
    "list_all_zips=list(zip_centers.keys())\n",
    "len_chunck=int(np.ceil(len(list_all_zips)/processors))\n",
    "list_of_input_all_us_zip_list=[]\n",
    "\n",
    "for i in range(processors):\n",
    "    l=list_all_zips[i*len_chunck:(i+1)*len_chunck]\n",
    "    list_of_input_all_us_zip_list.append(l)\n",
    "\n",
    "p = Pool(processors)\n",
    "result=p.starmap(get_dist_output_df, zip(list_of_input_all_us_zip_list, repeat(df_store_list), repeat(zip_centers)))\n",
    "## result=p.map(get_dist_output_df, list_of_input_all_us_zip_list)\n",
    "# get_dist_output_df defined in the main py file, due to the thread need to be defined top-level\n",
    "df_zips_with_BL_store=pd.DataFrame()\n",
    "for res in result:\n",
    "    if res is not None:\n",
    "        df_zips_with_BL_store=df_zips_with_BL_store.append(res)\n",
    "p.close()\n",
    "p.join()\n",
    "print(\"check point 3\")\n",
    "\n",
    "\n",
    "print(df_zips_with_BL_store.shape,df_zips_with_BL_store['zip_cd'].nunique(),df_zips_with_BL_store['nearest_BL_store'].nunique())\n",
    "df_zips_with_BL_store['zip_cd']=df_zips_with_BL_store['zip_cd'].astype(str)\n",
    "df_zips_with_BL_store['zip_cd']=df_zips_with_BL_store['zip_cd'].apply(lambda x: x.zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 00:00:42.037202\n",
      "df_1_len 21967207\n",
      "df_1_id_nunique 21967207\n",
      "2020-12-27 00:10:14.079465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf_copy_sign_up=df_1[[\\'sign_up_location\\',\\'customer_zip_code\\']].drop_duplicates()\\ndf_copy_sign_up=df_copy_sign_up.reset_index()\\ndel df_copy_sign_up[\\'index\\']\\nprint(\"check point 4\")\\n\\n\\n# In[7]:\\n\\n\\n# distance to sign up stores\\ndf_store_all=pd.DataFrame(columns=[\\'location_id\\',\\'latitude_meas\\',\\'longitude_meas\\'])\\n\\nlist_all_stores=glob.glob(folder_store_list+\"*.txt\")\\nlist_all_stores=[x for x in list_all_stores if \"MediaStormStores\" in x]\\nlist_all_stores=sorted(list_all_stores,key=lambda x :x.split(\"MediaStormStores\")[1][:8])\\nlist_all_stores=[x for x in list_all_stores if x.split(\"MediaStormStores\")[1][:8]<=str(high_date+datetime.timedelta(days=2)).replace(\"-\",\"\")]\\nlist_all_stores.reverse()\\n\\nfor file in list_all_stores:\\n    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=[\\'location_id\\',\\'latitude_meas\\',\\'longitude_meas\\'])\\n    df=df[[\\'location_id\\',\\'latitude_meas\\',\\'longitude_meas\\']]\\n    df[\\'latitude_meas\\']=df[\\'latitude_meas\\'].astype(float)                   \\n    df[\\'longitude_meas\\']=df[\\'longitude_meas\\'].astype(float)   \\n    df=df[~df[\\'location_id\\'].isin([\\'145\\',\\'6990\\'])]\\n    df=df[~df[\\'location_id\\'].isin(df_store_all[\\'location_id\\'].tolist())]\\n    df_store_all=df_store_all.append(df)\\ndf_store_all[\\'store_coor\\']=df_store_all[[\\'latitude_meas\\',\\'longitude_meas\\']].values.tolist()                      \\ndict_store_all=df_store_all.set_index(\"location_id\").to_dict()[\\'store_coor\\']\\ndf_copy_sign_up[\\'distc_to_sign_up\\']=np.nan\\nfor i,row in df_copy_sign_up.iterrows():\\n    try:\\n        store_coor=dict_store_all[row[\\'sign_up_location\\']]\\n        zip_center=zip_centers[row[\\'customer_zip_code\\']]\\n        dist=haversine(store_coor,zip_center,unit=\"mi\")\\n        df_copy_sign_up.loc[i,\"distc_to_sign_up\"]=dist\\n\\n    except:\\n        continue\\ndf_1=pd.merge(df_1,df_copy_sign_up,on=[\\'sign_up_location\\',\\'customer_zip_code\\'],how=\"left\")\\nprint(\"check point 5\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IVs\n",
    "\n",
    "# distance to sign up location is not a good contributor in the model, so no need to include the part\n",
    "print(datetime.datetime.now())\n",
    "df_1=pd.read_sql(\"select t1.customer_id_hashed, sign_up_channel, sign_up_location, customer_zip_code, t1.sign_up_date from BL_Rewards_Master as t1 right join %s as t2 on t1.customer_id_hashed=t2.customer_id_hashed;\"%table_filtered_crm, con=BL_engine)\n",
    "df_1=df_1.sort_values(\"sign_up_date\",ascending=False)\n",
    "df_1=df_1.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "\n",
    "df_1_len=df_1.shape[0]\n",
    "df_1_id_nunique=df_1['customer_id_hashed'].nunique()\n",
    "print(\"df_1_len\",df_1_len)\n",
    "print(\"df_1_id_nunique\",df_1_id_nunique)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "df_1['customer_zip_code']=df_1['customer_zip_code'].astype(str)\n",
    "df_1['customer_zip_code']=df_1['customer_zip_code'].apply(lambda x: x.split(\"-\")[0].split(\" \")[0].zfill(5)[:5])\n",
    "# df_1['sign_up_date']=pd.to_datetime(df_1['sign_up_date'],format=\"%Y-%m-%d\").dt.date\n",
    "# df_1['weeks_since_sign_up']=df_1['sign_up_date'].apply(lambda x: int(np.ceil((high_date-x).days/7)))\n",
    "df_1['P_zip']=np.where(df_1['customer_zip_code'].isin(list_P_zips),1,0)\n",
    "df_1['S_zip']=np.where(df_1['customer_zip_code'].isin(list_S_zips),1,0)\n",
    "df_1['else_10_zip']=np.where(df_1['customer_zip_code'].isin(list_10_zips),1,0)\n",
    "# del df_1['customer_zip_code']\n",
    "df_1['signed_online']=np.where(df_1['sign_up_channel']==\"STORE\",0,1)\n",
    "del df_1['sign_up_channel']\n",
    "\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].fillna(\"-1\")\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].astype(float)\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].astype(int).astype(str)\n",
    "print(\"check point 4\")\n",
    "\n",
    "'''\n",
    "df_copy_sign_up=df_1[['sign_up_location','customer_zip_code']].drop_duplicates()\n",
    "df_copy_sign_up=df_copy_sign_up.reset_index()\n",
    "del df_copy_sign_up['index']\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# distance to sign up stores\n",
    "df_store_all=pd.DataFrame(columns=['location_id','latitude_meas','longitude_meas'])\n",
    "\n",
    "list_all_stores=glob.glob(folder_store_list+\"*.txt\")\n",
    "list_all_stores=[x for x in list_all_stores if \"MediaStormStores\" in x]\n",
    "list_all_stores=sorted(list_all_stores,key=lambda x :x.split(\"MediaStormStores\")[1][:8])\n",
    "list_all_stores=[x for x in list_all_stores if x.split(\"MediaStormStores\")[1][:8]<=str(high_date+datetime.timedelta(days=2)).replace(\"-\",\"\")]\n",
    "list_all_stores.reverse()\n",
    "\n",
    "for file in list_all_stores:\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['location_id','latitude_meas','longitude_meas'])\n",
    "    df=df[['location_id','latitude_meas','longitude_meas']]\n",
    "    df['latitude_meas']=df['latitude_meas'].astype(float)                   \n",
    "    df['longitude_meas']=df['longitude_meas'].astype(float)   \n",
    "    df=df[~df['location_id'].isin(['145','6990'])]\n",
    "    df=df[~df['location_id'].isin(df_store_all['location_id'].tolist())]\n",
    "    df_store_all=df_store_all.append(df)\n",
    "df_store_all['store_coor']=df_store_all[['latitude_meas','longitude_meas']].values.tolist()                      \n",
    "dict_store_all=df_store_all.set_index(\"location_id\").to_dict()['store_coor']\n",
    "df_copy_sign_up['distc_to_sign_up']=np.nan\n",
    "for i,row in df_copy_sign_up.iterrows():\n",
    "    try:\n",
    "        store_coor=dict_store_all[row['sign_up_location']]\n",
    "        zip_center=zip_centers[row['customer_zip_code']]\n",
    "        dist=haversine(store_coor,zip_center,unit=\"mi\")\n",
    "        df_copy_sign_up.loc[i,\"distc_to_sign_up\"]=dist\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "df_1=pd.merge(df_1,df_copy_sign_up,on=['sign_up_location','customer_zip_code'],how=\"left\")\n",
    "print(\"check point 5\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6871466\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "list_unsub=glob.glob(folder_email_unsub+\"*.csv\")\n",
    "df_unsub_files=pd.DataFrame({\"file_path\":list_unsub})\n",
    "df_unsub_files['date']=df_unsub_files['file_path'].apply(lambda x: x.split(\"ile_Refresh__\")[1][:8])\n",
    "df_unsub_files['date']=pd.to_datetime(df_unsub_files['date']).dt.date\n",
    "df_unsub_files['day_diff']=abs(df_unsub_files['date']-high_date)\n",
    "path_unsub=df_unsub_files[df_unsub_files['day_diff']==df_unsub_files['day_diff'].min()]['file_path'].values.tolist()[0]\n",
    "###### \n",
    "list_unsunsribe_ids=pd.read_csv(path_unsub,\n",
    "                         dtype=str,usecols=['customersummary_c_primaryscnhash'])['customersummary_c_primaryscnhash'].unique().tolist()\n",
    "\n",
    "print(len(list_unsunsribe_ids))\n",
    "df_1['email_unsub_label']=np.where(df_1['customer_id_hashed'].isin(list_unsunsribe_ids),1,0)\n",
    "del list_unsunsribe_ids\n",
    "df_zips_with_BL_store=df_zips_with_BL_store.rename(columns={\"zip_cd\":\"customer_zip_code\"})\n",
    "df_1=pd.merge(df_1,df_zips_with_BL_store,on=\"customer_zip_code\",how=\"left\")\n",
    "df_1=df_1.reset_index()\n",
    "del df_1['index']\n",
    "df_1=df_1.reset_index()\n",
    "del df_1['index']\n",
    "df_1=df_1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/clients/juba/hqjubaapp02/sharefolder/biglots_data/Email_Subscription_Files/Unsubs/BL_Email_UnSubscriber_File_Refresh__20201125050434.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_unsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndv_start_date=high_date+datetime.timedelta(days=1)\\ndv_end_date=high_date+datetime.timedelta(days=21)\\n\\nstr_sql_dv_start_date=\"\\'\"+str(dv_start_date)+\"\\'\"\\nstr_sql_dv_end_date=\"\\'\"+str(dv_end_date)+\"\\'\"\\nprint(str_sql_dv_start_date,str_sql_dv_end_date)\\nprint(datetime.datetime.now())\\ndf_dvs=pd.read_sql(\"select customer_id_hashed, transaction_dt from Pred_POS_Department where transaction_dt between %s and %s and sales >0\"%(str_sql_dv_start_date,str_sql_dv_end_date),con=BL_engine).drop_duplicates()\\nprint(datetime.datetime.now())\\nprint(\"check point 6\")\\n\\n\\n# In[36]:\\n\\n\\ndf_dvs[\\'week_end_dt\\']=df_dvs[\\'transaction_dt\\'].apply(week_end_dt)\\ndf_dvs=df_dvs[[\\'customer_id_hashed\\',\\'week_end_dt\\']].drop_duplicates()\\nlist_unique_weeks=df_dvs[\\'week_end_dt\\'].unique().tolist()\\nlist_unique_weeks.sort()\\ndf_dv_binary=df_dvs[df_dvs[\\'week_end_dt\\']==list_unique_weeks[0]][[\\'customer_id_hashed\\']]\\ndf_dv_binary[\\'DV_cumulative_week_updated_1\\']=1\\nfor i in range(1,3):\\n    w=list_unique_weeks[i]\\n    df=df_dvs[df_dvs[\\'week_end_dt\\']<=w][[\\'customer_id_hashed\\']].drop_duplicates()\\n    df[\\'DV_cumulative_week_updated_%d\\'%(i+1)]=1\\n    df_dv_binary=pd.merge(df_dv_binary,df,on=\"customer_id_hashed\",how=\"outer\")\\n    print(w,datetime.datetime.now())\\ndf_dv_binary=df_dv_binary.fillna(0)\\n\\ndf_1=pd.merge(df_dv_binary,df_1,on=\"customer_id_hashed\",how=\"right\")\\n\\nfor i in range(3):\\n    df_1[\\'DV_cumulative_week_updated_%d\\'%(i+1)]=df_1[\\'DV_cumulative_week_updated_%d\\'%(i+1)].fillna(0)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changed the DV to be pulled in the Predictive Running part, due to the need to apply 2+ and 1 only sepecreately\n",
    "# Design as here:\n",
    "# today is 2020-12-19, and the latest date in the data is 2020-12-12 (new week not received at this moment)\n",
    "# DV -- cumulative 2 weeks -- 2+ buyers, is the range between 2020-11-29 to 2020-12-12\n",
    "# DV -- cumulative 3 weeks -- 1 only buyers, is the range between 2020-11-22 to 2020-12-12\n",
    "\n",
    "# The script moved to the running part to pull seperately for the buyers\n",
    "'''\n",
    "dv_start_date=high_date+datetime.timedelta(days=1)\n",
    "dv_end_date=high_date+datetime.timedelta(days=21)\n",
    "\n",
    "str_sql_dv_start_date=\"'\"+str(dv_start_date)+\"'\"\n",
    "str_sql_dv_end_date=\"'\"+str(dv_end_date)+\"'\"\n",
    "print(str_sql_dv_start_date,str_sql_dv_end_date)\n",
    "print(datetime.datetime.now())\n",
    "df_dvs=pd.read_sql(\"select customer_id_hashed, transaction_dt from Pred_POS_Department where transaction_dt between %s and %s and sales >0\"%(str_sql_dv_start_date,str_sql_dv_end_date),con=BL_engine).drop_duplicates()\n",
    "print(datetime.datetime.now())\n",
    "print(\"check point 6\")\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "df_dvs['week_end_dt']=df_dvs['transaction_dt'].apply(week_end_dt)\n",
    "df_dvs=df_dvs[['customer_id_hashed','week_end_dt']].drop_duplicates()\n",
    "list_unique_weeks=df_dvs['week_end_dt'].unique().tolist()\n",
    "list_unique_weeks.sort()\n",
    "df_dv_binary=df_dvs[df_dvs['week_end_dt']==list_unique_weeks[0]][['customer_id_hashed']]\n",
    "df_dv_binary['DV_cumulative_week_updated_1']=1\n",
    "for i in range(1,3):\n",
    "    w=list_unique_weeks[i]\n",
    "    df=df_dvs[df_dvs['week_end_dt']<=w][['customer_id_hashed']].drop_duplicates()\n",
    "    df['DV_cumulative_week_updated_%d'%(i+1)]=1\n",
    "    df_dv_binary=pd.merge(df_dv_binary,df,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    print(w,datetime.datetime.now())\n",
    "df_dv_binary=df_dv_binary.fillna(0)\n",
    "\n",
    "df_1=pd.merge(df_dv_binary,df_1,on=\"customer_id_hashed\",how=\"right\")\n",
    "\n",
    "for i in range(3):\n",
    "    df_1['DV_cumulative_week_updated_%d'%(i+1)]=df_1['DV_cumulative_week_updated_%d'%(i+1)].fillna(0)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21967207, 12) 21967207\n",
      "check point 7\n"
     ]
    }
   ],
   "source": [
    "print(df_1.shape,df_1['customer_id_hashed'].nunique())\n",
    "if \"index\" in df_1.columns.tolist():\n",
    "    del df_1['index']\n",
    "\n",
    "print(\"check point 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1_train.shape (1000000, 1)\n",
      "df_1_test.shape (20967207, 1)\n",
      "check point 8\n",
      "CREATE INDEX id_index ON crm_table_id_list_train_20201121(customer_id_hashed)\n",
      "CREATE INDEX id_index ON crm_table_id_list_test_20201121(customer_id_hashed)\n",
      "CREATE INDEX id_index ON table_pred_1_crm_up_to_20201121(customer_id_hashed)\n",
      "Done_of_part_2: 2020-12-27 00:52:20.397060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef get_dist_output_df(input_zip_list,df_store_list,zip_centers):\\n    df_output=pd.DataFrame()\\n\\n    for zip_cd in input_zip_list:\\n        z_centroid=zip_centers[zip_cd]\\n        min_dist=np.inf\\n        nearest_store=None\\n\\n        for i,row in df_store_list.iterrows():\\n            store=row[\\'location_id\\']\\n            store_loc=(row[\\'latitude_meas\\'], row[\\'longitude_meas\\'])\\n            dist=haversine(z_centroid,store_loc,unit=\"mi\")\\n            if dist<=min_dist:\\n                min_dist=dist\\n                nearest_store=store\\n        df=pd.DataFrame({\"nearest_BL_store\":nearest_store,\"nearest_BL_dist\":min_dist},index=[zip_cd])\\n        df=df.reset_index().rename(columns={\"index\":\"zip_cd\"})\\n        df_output=df_output.append(df)\\n    return df_output\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test crm is still being wrote out in case that a validation summary view in need\n",
    "table_crm_id_list_train=\"crm_table_id_list_train_%s\"%str_week_end_d\n",
    "table_crm_id_list_test=\"crm_table_id_list_test_%s\"%str_week_end_d\n",
    "table_df_1=\"table_pred_1_crm_up_to_%s\"%str_week_end_d\n",
    "\n",
    "dict_table_names.update({\"table_crm_id_list_train\":table_crm_id_list_train})\n",
    "dict_table_names.update({\"table_crm_id_list_test\":table_crm_id_list_test})\n",
    "dict_table_names.update({\"table_df_1\":table_df_1})\n",
    "# split\n",
    "len_df_1=len(df_1)\n",
    "\n",
    "\n",
    "if len_df_1>train_sample_size/(1-test_ratio):\n",
    "    list_ind_train=random.sample(range(len_df_1), train_sample_size)\n",
    "else:\n",
    "    list_ind_train=random.sample(range(len_df_1), int(len_df_1*(1-test_ratio)))\n",
    "\n",
    "df_1=df_1.reset_index()\n",
    "df_1_train=df_1[['customer_id_hashed']][df_1['index'].isin(list_ind_train)]\n",
    "df_1_test=df_1[['customer_id_hashed']][~df_1['index'].isin(list_ind_train)]\n",
    "del df_1['index']\n",
    "\n",
    "\n",
    "print(\"df_1_train.shape\",df_1_train.shape)\n",
    "print(\"df_1_test.shape\",df_1_test.shape)\n",
    "chunksize=10**6\n",
    "\n",
    "dtype_id={\"customer_id_hashed\": sql.types.VARCHAR(length=64)}\n",
    "df_1_train.to_sql(name=table_crm_id_list_train,chunksize=chunksize,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_id)\n",
    "df_1_test.to_sql(name=table_crm_id_list_test,chunksize=chunksize,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_id)\n",
    "\n",
    "dtype_df_1={\n",
    "'customer_id_hashed':sql.types.VARCHAR(length=64),\n",
    "# 'DV_cumulative_week_updated_1':sql.types.Integer,\n",
    "# 'DV_cumulative_week_updated_2':sql.types.Integer,\n",
    "# 'DV_cumulative_week_updated_3':sql.types.Integer,\n",
    "# 'DV_cumulative_week_updated_4':sql.types.Integer,\n",
    "'sign_up_location':sql.types.VARCHAR(length=5),\n",
    "'customer_zip_code':sql.types.VARCHAR(length=5),\n",
    "'P_zip':sql.types.Integer,\n",
    "'S_zip':sql.types.Integer,\n",
    "'else_10_zip':sql.types.Integer,\n",
    "'signed_online':sql.types.Integer,\n",
    "'distc_to_sign_up':sql.types.Float,\n",
    "'email_unsub_label':sql.types.Integer,\n",
    "'nearest_BL_store':sql.types.VARCHAR(length=4),\n",
    "'nearest_BL_dist':sql.types.Float\n",
    "}\n",
    "\n",
    "df_1.to_sql(name=table_df_1,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_df_1,chunksize=chunksize)\n",
    "print(\"check point 8\")\n",
    "create_index(table_name=table_crm_id_list_train, list_of_columns=[\"customer_id_hashed\"])\n",
    "create_index(table_name=table_crm_id_list_test, list_of_columns=[\"customer_id_hashed\"])\n",
    "create_index(table_name=table_df_1, list_of_columns=[\"customer_id_hashed\"])\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "path_json_table_names=\"./table_names_%s.json\"%str(high_date).replace(\"-\",\"\")\n",
    "with open(path_json_table_names,\"w\") as json_file:\n",
    "    json.dump(dict_table_names,json_file)\n",
    "print(\"Done_of_part_2: %s\"%str(datetime.datetime.now()))\n",
    "\n",
    "'''\n",
    "def get_dist_output_df(input_zip_list,df_store_list,zip_centers):\n",
    "    df_output=pd.DataFrame()\n",
    "\n",
    "    for zip_cd in input_zip_list:\n",
    "        z_centroid=zip_centers[zip_cd]\n",
    "        min_dist=np.inf\n",
    "        nearest_store=None\n",
    "\n",
    "        for i,row in df_store_list.iterrows():\n",
    "            store=row['location_id']\n",
    "            store_loc=(row['latitude_meas'], row['longitude_meas'])\n",
    "            dist=haversine(z_centroid,store_loc,unit=\"mi\")\n",
    "            if dist<=min_dist:\n",
    "                min_dist=dist\n",
    "                nearest_store=store\n",
    "        df=pd.DataFrame({\"nearest_BL_store\":nearest_store,\"nearest_BL_dist\":min_dist},index=[zip_cd])\n",
    "        df=df.reset_index().rename(columns={\"index\":\"zip_cd\"})\n",
    "        df_output=df_output.append(df)\n",
    "    return df_output\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
