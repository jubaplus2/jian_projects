{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start_Part_2: 2020-08-26 21:28:11.369726\n"
     ]
    }
   ],
   "source": [
    "# def write_crm_train_test()\n",
    "import sqlalchemy as sql\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from haversine import haversine\n",
    "import glob\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import gc\n",
    "print(\"Start_Part_2: %s\"%str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.json', 'rb') as f:\n",
    "    dict_config = json.load(f)\n",
    "\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "folder_store_list=dict_config['folder_store_list']\n",
    "path_TA_excel=dict_config['path_TA_excel']\n",
    "path_json_zip_center=dict_config['path_json_zip_center']\n",
    "pos_end_date=dict_config['pos_end_date']\n",
    "folder_store_list=dict_config['folder_store_list']\n",
    "folder_email_unsub=dict_config['folder_email_unsub']\n",
    "\n",
    "with open('./table_names_%s.json'%str(pos_end_date).replace(\"-\",\"\"), 'rb') as f:\n",
    "    dict_table_names = json.load(f)\n",
    "table_filtered_crm=dict_table_names['table_filtered_crm']\n",
    "\n",
    "\n",
    "BL_engine=sql.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(table_name, list_of_columns):\n",
    "    columns = ', '.join(list_of_columns)\n",
    "    query = \"CREATE INDEX id_index ON %s(%s)\" % (table_name, columns)\n",
    "    print(query)\n",
    "    with BL_engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "    result.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def week_end_dt(date_input):\n",
    "    weekday_int=date_input.weekday()\n",
    "    if weekday_int==6:\n",
    "        return date_input+datetime.timedelta(days=6)\n",
    "    else:\n",
    "        return date_input+datetime.timedelta(days=5-weekday_int)\n",
    "    \n",
    "    \n",
    "def get_dist_output_df(input_zip_list,df_store_list,zip_centers):\n",
    "    df_output=pd.DataFrame()\n",
    "\n",
    "    for zip_cd in input_zip_list:\n",
    "        z_centroid=zip_centers[zip_cd]\n",
    "        min_dist=np.inf\n",
    "        nearest_store=None\n",
    "\n",
    "        for i,row in df_store_list.iterrows():\n",
    "            store=row['location_id']\n",
    "            store_loc=(row['latitude_meas'], row['longitude_meas'])\n",
    "            dist=haversine(z_centroid,store_loc,unit=\"mi\")\n",
    "            if dist<=min_dist:\n",
    "                min_dist=dist\n",
    "                nearest_store=store\n",
    "        df=pd.DataFrame({\"nearest_BL_store\":nearest_store,\"nearest_BL_dist\":min_dist},index=[zip_cd])\n",
    "        df=df.reset_index().rename(columns={\"index\":\"zip_cd\"})\n",
    "        df_output=df_output.append(df)\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point 1\n",
      "['P', 'S', 'zip_10']\n",
      "check point 2\n"
     ]
    }
   ],
   "source": [
    "high_date=datetime.datetime.strptime(dict_config['crm_end_date'],\"%Y-%m-%d\").date()\n",
    "if dict_config['recent_n_month']:\n",
    "    recent_n_month=dict_config['recent_n_month']\n",
    "    pos_start_date_id_filter = str(high_date-datetime.timedelta(days=int(np.ceil(365*recent_n_month/12))))\n",
    "else:\n",
    "    pos_start_date_id_filter = dict_config[\"pos_start_date\"]\n",
    "\n",
    "sql_str_high_date=\"'%s'\"%str(high_date)\n",
    "sql_str_lastweekstart_date=\"'%s'\"%str(high_date-datetime.timedelta(days=6))\n",
    "# sql_sign_up_start_date=\"'%s'\"%str(sign_up_start_date)\n",
    "sql_POS_start_date=\"'%s'\"%str(pos_start_date_id_filter)\n",
    "str_week_end_d=str(high_date).replace(\"-\",\"\")\n",
    "print(\"check point 1\")\n",
    "\n",
    "\n",
    "path_store_list=glob.glob(folder_store_list+\"*.txt\")\n",
    "path_store_list_ahead=[x for x in path_store_list if \"MediaStormStores%s\"%str_week_end_d[:6] in x][0]\n",
    "path_store_list_after=[x for x in path_store_list if \"MediaStormStores%s\"%str(int(str_week_end_d[:6])+1) in x][0]\n",
    "\n",
    "df_store_list=pd.read_csv(path_store_list_ahead,sep=\"|\")\n",
    "df_store_list=df_store_list[['location_id','address_line_1','address_line_2','city_nm','state_nm','zip_cd','latitude_meas','longitude_meas']]\n",
    "df_store_list['latitude_meas']=df_store_list['latitude_meas'].astype(float)\n",
    "df_store_list['longitude_meas']=df_store_list['longitude_meas'].astype(float)\n",
    "df_store_list['zip_cd']=df_store_list['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "df_store_list=df_store_list[~df_store_list['location_id'].isin(['145','6990'])]\n",
    "df_store_list['location_id']=df_store_list['location_id'].astype(str)\n",
    "# \n",
    "TA_zips=pd.ExcelFile(path_TA_excel)\n",
    "TA_zips=TA_zips.parse(\"view_by_store\",dtype=str)\n",
    "\n",
    "df_temporary=TA_zips[['location_id','trans_P_zips_70_within_TA','trans_S_zips_70_within_TA','zips_in_10']]\n",
    "df_zip_by_store=pd.DataFrame()\n",
    "\n",
    "for ind,row in df_temporary.iterrows():\n",
    "    location_id=str(row['location_id'])\n",
    "    P_zips=eval(row['trans_P_zips_70_within_TA'])\n",
    "    S_zips=eval(row['trans_S_zips_70_within_TA'])\n",
    "    zip_10=eval(row['zips_in_10'])\n",
    "\n",
    "\n",
    "    df_P=pd.DataFrame(zip([location_id]*len(P_zips),P_zips))\n",
    "    if len(df_P)>0:\n",
    "        df_P.columns=['location_id','zip_cd']\n",
    "        df_P['zip_type']=\"P\"\n",
    "\n",
    "    df_S=pd.DataFrame(zip([location_id]*len(S_zips),S_zips))\n",
    "    if len(df_S)>0:\n",
    "        df_S.columns=['location_id','zip_cd']\n",
    "        df_S['zip_type']=\"S\"\n",
    "\n",
    "    df_10=pd.DataFrame(zip([location_id]*len(zip_10),zip_10))\n",
    "    if len(df_10)>0:\n",
    "        df_10.columns=['location_id','zip_cd']\n",
    "        df_10['zip_type']=\"zip_10\"\n",
    "\n",
    "    df_zip_by_store=df_zip_by_store.append(df_P).append(df_S).append(df_10)\n",
    "df_zip_by_store['location_id']=df_zip_by_store['location_id'].astype(str)\n",
    "df_store_list=df_store_list[['location_id','latitude_meas','longitude_meas']]\n",
    "df_store_zip=pd.merge(df_store_list,df_zip_by_store,on=\"location_id\",how=\"left\")\n",
    "df_store_zip_new=df_store_zip[pd.isnull(df_store_zip['zip_cd'])]\n",
    "df_store_zip_existing=df_store_zip[pd.notnull(df_store_zip['zip_cd'])]\n",
    "\n",
    "df_store_zip_new_no_loc=df_store_zip_new[df_store_zip_new['latitude_meas']==0]\n",
    "df_store_zip_new_with_loc=df_store_zip_new[df_store_zip_new['latitude_meas']!=0]\n",
    "df_store_zip_new_with_loc=df_store_zip_new_with_loc[['location_id','latitude_meas','longitude_meas']]\n",
    "df_store_zip_new_no_loc=df_store_zip_new_no_loc[['location_id','latitude_meas','longitude_meas']]\n",
    "if len(df_store_zip_new_no_loc)>0:\n",
    "    store_list_later=[x for x in list_store_list if x.split(\"MediaStormStores\")[1][:6]>str_week_end_d]\n",
    "    store_list_later=sorted(store_list_later,key=lambda x: os.stat(x).st_mtime)\n",
    "    for file in store_list_later:\n",
    "        df=pd.read_csv(file,dtype=str,sep=\"|\",usecols=['location_id','latitude_meas','longitude_meas'])\n",
    "        df=df[['location_id','latitude_meas','longitude_meas']]\n",
    "        df['latitude_meas']=df['latitude_meas'].astype(float)\n",
    "        df['longitude_meas']=df['longitude_meas'].astype(float)\n",
    "        df['location_id']=df['location_id'].astype(str)\n",
    "        df=df[df['location_id'].isin(df_store_zip_new_no_loc['location_id'].tolist())]\n",
    "        df=df[df['latitude_meas']!=0]\n",
    "        df_store_zip_new_with_loc=df_store_zip_new_with_loc.append(df)\n",
    "        df_store_zip_new_no_loc=df_store_zip_new_no_loc[~df_store_zip_new_no_loc['location_id'].isin(df['location_id'].tolist())]\n",
    "        if len(df_store_zip_new_no_loc)==0:\n",
    "            break\n",
    "    df_store_zip_new=df_store_zip_new_with_loc.reset_index()\n",
    "    del df_store_zip_new['index']\n",
    "    if df_store_zip_new_with_loc:\n",
    "        del df_store_zip_new_with_loc\n",
    "    if df_store_zip_new_no_loc:\n",
    "        del df_store_zip_new_no_loc\n",
    "\n",
    "zip_centers=json.load(open(path_json_zip_center,\"r\"))\n",
    "if len(df_store_zip_new)>0:\n",
    "    del df_store_zip_new['zip_cd']\n",
    "    del df_store_zip_new['zip_type']\n",
    "\n",
    "    df_all_new_zip=pd.DataFrame()\n",
    "    for i,row in df_store_zip_new.iterrows():\n",
    "        store_coor=(row['latitude_meas'],row['longitude_meas'])\n",
    "        store_num=row['location_id']\n",
    "        list_store_zip=[]\n",
    "        for zip_cd, v in zip_centers.items():\n",
    "            dist=haversine(store_coor,v,unit=\"mi\")\n",
    "            if dist<=10:\n",
    "                list_store_zip.append(zip_cd)\n",
    "        df=pd.DataFrame({\"zip_cd\":list_store_zip,\"zip_type\":[\"zip_10\"]*len(list_store_zip)},index=[store_num]*len(list_store_zip))\n",
    "        df=df.reset_index().rename(columns={\"index\":\"location_id\"})\n",
    "        df_all_new_zip=df_all_new_zip.append(df)\n",
    "\n",
    "    df_store_zip_new=pd.merge(df_store_zip_new,df_all_new_zip,on=\"location_id\",how=\"left\")\n",
    "\n",
    "    df_store_zip=df_store_zip_existing.append(df_store_zip_new)\n",
    "else:\n",
    "    df_store_zip=df_store_zip_existing\n",
    "df_zip_type=df_store_zip[['zip_cd','zip_type']].drop_duplicates()\n",
    "df_zip_type=df_zip_type.sort_values(['zip_cd','zip_type'])\n",
    "print(df_zip_type['zip_type'].unique().tolist())\n",
    "df_unique_zip_type=df_zip_type.drop_duplicates(\"zip_cd\")\n",
    "\n",
    "list_P_zips=df_zip_type[df_zip_type['zip_type']==\"P\"]['zip_cd'].tolist()\n",
    "list_S_zips=df_zip_type[df_zip_type['zip_type']==\"S\"]['zip_cd'].tolist()\n",
    "list_10_zips=df_zip_type[df_zip_type['zip_type']==\"zip_10\"]['zip_cd'].tolist()\n",
    "\n",
    "df_store_list=df_store_zip[['location_id','latitude_meas','longitude_meas']].drop_duplicates().reset_index()\n",
    "del df_store_list['index']\n",
    "df_store_list=df_store_zip[['location_id','latitude_meas','longitude_meas']].drop_duplicates().reset_index()\n",
    "del df_store_list['index']\n",
    "# \n",
    "print(\"check point 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point 3\n",
      "(39647, 3) 39647 1404\n"
     ]
    }
   ],
   "source": [
    "processors=20\n",
    "\n",
    "list_all_zips=list(zip_centers.keys())\n",
    "len_chunck=int(np.ceil(len(list_all_zips)/processors))\n",
    "list_of_input_all_us_zip_list=[]\n",
    "\n",
    "for i in range(processors):\n",
    "    l=list_all_zips[i*len_chunck:(i+1)*len_chunck]\n",
    "    list_of_input_all_us_zip_list.append(l)\n",
    "\n",
    "p = Pool(processors)\n",
    "result=p.starmap(get_dist_output_df, zip(list_of_input_all_us_zip_list, repeat(df_store_list), repeat(zip_centers)))\n",
    "## result=p.map(get_dist_output_df, list_of_input_all_us_zip_list)\n",
    "# get_dist_output_df defined in the main py file, due to the thread need to be defined top-level\n",
    "df_zips_with_BL_store=pd.DataFrame()\n",
    "for res in result:\n",
    "    if res is not None:\n",
    "        df_zips_with_BL_store=df_zips_with_BL_store.append(res)\n",
    "p.close()\n",
    "p.join()\n",
    "print(\"check point 3\")\n",
    "\n",
    "\n",
    "print(df_zips_with_BL_store.shape,df_zips_with_BL_store['zip_cd'].nunique(),df_zips_with_BL_store['nearest_BL_store'].nunique())\n",
    "df_zips_with_BL_store['zip_cd']=df_zips_with_BL_store['zip_cd'].astype(str)\n",
    "df_zips_with_BL_store['zip_cd']=df_zips_with_BL_store['zip_cd'].apply(lambda x: x.zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-26 21:39:10.482830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1_len 20680748\n",
      "df_1_id_nunique 20680713\n",
      "2020-08-26 22:06:48.509445\n",
      "check point 4\n"
     ]
    }
   ],
   "source": [
    "# IVs\n",
    "print(datetime.datetime.now())\n",
    "df_1=pd.read_sql(\"select t1.customer_id_hashed, sign_up_channel, sign_up_location, customer_zip_code from BL_Rewards_Master as t1 \\\n",
    "right join %s as t2 on t1.customer_id_hashed=t2.customer_id_hashed;\"%table_filtered_crm, con=BL_engine)\n",
    "\n",
    "\n",
    "df_1_len=df_1.shape[0]\n",
    "df_1_id_nunique=df_1['customer_id_hashed'].nunique()\n",
    "print(\"df_1_len\",df_1_len)\n",
    "print(\"df_1_id_nunique\",df_1_id_nunique)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "df_1['customer_zip_code']=df_1['customer_zip_code'].astype(str)\n",
    "df_1['customer_zip_code']=df_1['customer_zip_code'].apply(lambda x: x.split(\"-\")[0].split(\" \")[0].zfill(5)[:5])\n",
    "# df_1['sign_up_date']=pd.to_datetime(df_1['sign_up_date'],format=\"%Y-%m-%d\").dt.date\n",
    "# df_1['weeks_since_sign_up']=df_1['sign_up_date'].apply(lambda x: int(np.ceil((high_date-x).days/7)))\n",
    "df_1['P_zip']=np.where(df_1['customer_zip_code'].isin(list_P_zips),1,0)\n",
    "df_1['S_zip']=np.where(df_1['customer_zip_code'].isin(list_S_zips),1,0)\n",
    "df_1['else_10_zip']=np.where(df_1['customer_zip_code'].isin(list_10_zips),1,0)\n",
    "# del df_1['customer_zip_code']\n",
    "df_1['signed_online']=np.where(df_1['sign_up_channel']==\"STORE\",0,1)\n",
    "del df_1['sign_up_channel']\n",
    "\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].fillna(\"-1\")\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].astype(float)\n",
    "df_1['sign_up_location']=df_1['sign_up_location'].astype(int).astype(str)\n",
    "\n",
    "df_copy_sign_up=df_1[['sign_up_location','customer_zip_code']].drop_duplicates()\n",
    "df_copy_sign_up=df_copy_sign_up.reset_index()\n",
    "del df_copy_sign_up['index']\n",
    "print(\"check point 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point 5\n",
      "6241656\n",
      "'2020-04-26' '2020-05-23'\n",
      "2020-08-26 22:56:15.354476\n",
      "2020-08-26 23:11:58.048610\n",
      "check point 6\n"
     ]
    }
   ],
   "source": [
    "# distance to sign up stores\n",
    "df_store_all=pd.DataFrame(columns=['location_id','latitude_meas','longitude_meas'])\n",
    "\n",
    "list_all_stores=glob.glob(folder_store_list+\"*.txt\")\n",
    "list_all_stores=[x for x in list_all_stores if \"MediaStormStores\" in x]\n",
    "list_all_stores=sorted(list_all_stores,key=lambda x :x.split(\"MediaStormStores\")[1][:8])\n",
    "list_all_stores=[x for x in list_all_stores if x.split(\"MediaStormStores\")[1][:8]<=str(high_date+datetime.timedelta(days=2)).replace(\"-\",\"\")]\n",
    "list_all_stores.reverse()\n",
    "\n",
    "for file in list_all_stores:\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['location_id','latitude_meas','longitude_meas'])\n",
    "    df=df[['location_id','latitude_meas','longitude_meas']]\n",
    "    df['latitude_meas']=df['latitude_meas'].astype(float)                   \n",
    "    df['longitude_meas']=df['longitude_meas'].astype(float)   \n",
    "    df=df[~df['location_id'].isin(['145','6990'])]\n",
    "    df=df[~df['location_id'].isin(df_store_all['location_id'].tolist())]\n",
    "    df_store_all=df_store_all.append(df)\n",
    "df_store_all['store_coor']=df_store_all[['latitude_meas','longitude_meas']].values.tolist()                      \n",
    "dict_store_all=df_store_all.set_index(\"location_id\").to_dict()['store_coor']\n",
    "df_copy_sign_up['distc_to_sign_up']=np.nan\n",
    "for i,row in df_copy_sign_up.iterrows():\n",
    "    try:\n",
    "        store_coor=dict_store_all[row['sign_up_location']]\n",
    "        zip_center=zip_centers[row['customer_zip_code']]\n",
    "        dist=haversine(store_coor,zip_center,unit=\"mi\")\n",
    "        df_copy_sign_up.loc[i,\"distc_to_sign_up\"]=dist\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "df_1=pd.merge(df_1,df_copy_sign_up,on=['sign_up_location','customer_zip_code'],how=\"left\")\n",
    "print(\"check point 5\")\n",
    "#\n",
    "list_unsub=glob.glob(folder_email_unsub+\"*.csv\")\n",
    "df_unsub_files=pd.DataFrame({\"file_path\":list_unsub})\n",
    "df_unsub_files['date']=df_unsub_files['file_path'].apply(lambda x: x.split(\"ile_Refresh__\")[1][:8])\n",
    "df_unsub_files['date']=pd.to_datetime(df_unsub_files['date']).dt.date\n",
    "df_unsub_files['day_diff']=abs(df_unsub_files['date']-high_date)\n",
    "path_unsub=df_unsub_files[df_unsub_files['day_diff']==df_unsub_files['day_diff'].min()]['file_path'].values.tolist()[0]\n",
    "###### \n",
    "list_unsunsribe_ids=pd.read_csv(path_unsub,\n",
    "                         dtype=str,usecols=['customersummary_c_primaryscnhash'])['customersummary_c_primaryscnhash'].unique().tolist()\n",
    "\n",
    "print(len(list_unsunsribe_ids))\n",
    "df_1['email_unsub_label']=np.where(df_1['customer_id_hashed'].isin(list_unsunsribe_ids),1,0)\n",
    "del list_unsunsribe_ids\n",
    "df_zips_with_BL_store=df_zips_with_BL_store.rename(columns={\"zip_cd\":\"customer_zip_code\"})\n",
    "df_1=pd.merge(df_1,df_zips_with_BL_store,on=\"customer_zip_code\",how=\"left\")\n",
    "df_1=df_1.reset_index()\n",
    "del df_1['index']\n",
    "df_1=df_1.reset_index()\n",
    "del df_1['index']\n",
    "df_1=df_1.reset_index()\n",
    "dv_start_date=high_date+datetime.timedelta(days=1)\n",
    "dv_end_date=high_date+datetime.timedelta(days=28)\n",
    "\n",
    "str_sql_dv_start_date=\"'\"+str(dv_start_date)+\"'\"\n",
    "str_sql_dv_end_date=\"'\"+str(dv_end_date)+\"'\"\n",
    "print(str_sql_dv_start_date,str_sql_dv_end_date)\n",
    "print(datetime.datetime.now())\n",
    "df_dvs=pd.read_sql(\"select customer_id_hashed, transaction_dt from Pred_POS_Department \\\n",
    "where transaction_dt between %s and %s and sales >0\"%(str_sql_dv_start_date,str_sql_dv_end_date),con=BL_engine).drop_duplicates()\n",
    "print(datetime.datetime.now())\n",
    "print(\"check point 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-09 2020-08-26 23:12:39.350504\n",
      "2020-05-16 2020-08-26 23:13:00.023355\n",
      "2020-05-23 2020-08-26 23:13:25.057942\n",
      "(20680748, 16) 20680713\n",
      "check point 7\n",
      "df_1_train.shape (1000000, 1)\n",
      "df_1_test.shape (19680748, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dvs['week_end_dt']=df_dvs['transaction_dt'].apply(week_end_dt)\n",
    "df_dvs=df_dvs[['customer_id_hashed','week_end_dt']].drop_duplicates()\n",
    "list_unique_weeks=df_dvs['week_end_dt'].unique().tolist()\n",
    "list_unique_weeks.sort()\n",
    "df_dv_binary=df_dvs[df_dvs['week_end_dt']==list_unique_weeks[0]][['customer_id_hashed']]\n",
    "df_dv_binary['DV_cumulative_week_updated_1']=1\n",
    "for i in range(1,4):\n",
    "    w=list_unique_weeks[i]\n",
    "    df=df_dvs[df_dvs['week_end_dt']<=w][['customer_id_hashed']].drop_duplicates()\n",
    "    df['DV_cumulative_week_updated_%d'%(i+1)]=1\n",
    "    df_dv_binary=pd.merge(df_dv_binary,df,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    print(w,datetime.datetime.now())\n",
    "df_dv_binary=df_dv_binary.fillna(0)\n",
    "\n",
    "df_1=pd.merge(df_dv_binary,df_1,on=\"customer_id_hashed\",how=\"right\")\n",
    "\n",
    "for i in range(4):\n",
    "    df_1['DV_cumulative_week_updated_%d'%(i+1)]=df_1['DV_cumulative_week_updated_%d'%(i+1)].fillna(0)\n",
    "\n",
    "print(df_1.shape,df_1['customer_id_hashed'].nunique())\n",
    "if \"index\" in df_1.columns.tolist():\n",
    "    del df_1['index']\n",
    "\n",
    "\n",
    "print(\"check point 7\")\n",
    "# self\n",
    "table_crm_id_list_train=\"crm_table_id_list_train_%s\"%str_week_end_d\n",
    "table_crm_id_list_test=\"crm_table_id_list_test_%s\"%str_week_end_d\n",
    "table_df_1=\"table_pred_1_crm_from_up_to_%s\"%str_week_end_d\n",
    "\n",
    "dict_table_names.update({\"table_crm_id_list_train\":table_crm_id_list_train})\n",
    "dict_table_names.update({\"table_crm_id_list_test\":table_crm_id_list_test})\n",
    "dict_table_names.update({\"table_df_1\":table_df_1})\n",
    "# split\n",
    "len_df_1=len(df_1)\n",
    "train_sample_size=10**6\n",
    "test_ratio=0.25\n",
    "if len_df_1>train_sample_size/(1-test_ratio):\n",
    "    list_ind_train=random.sample(range(len_df_1), train_sample_size)\n",
    "else:\n",
    "    list_ind_train=random.sample(range(len_df_1), int(len_df_1*(1-test_ratio)))\n",
    "\n",
    "df_1=df_1.reset_index()\n",
    "df_1_train=df_1[['customer_id_hashed']][df_1['index'].isin(list_ind_train)]\n",
    "df_1_test=df_1[['customer_id_hashed']][~df_1['index'].isin(list_ind_train)]\n",
    "del df_1['index']\n",
    "\n",
    "\n",
    "print(\"df_1_train.shape\",df_1_train.shape)\n",
    "print(\"df_1_test.shape\",df_1_test.shape)\n",
    "\n",
    "dtype_id={\"customer_id_hashed\": sql.types.VARCHAR(length=64)}\n",
    "df_1_train.to_sql(name=table_crm_id_list_train,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_id)\n",
    "df_1_test.to_sql(name=table_crm_id_list_test,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_id)\n",
    "del list_ind_train\n",
    "del df_1_train\n",
    "del df_1_test\n",
    "del df_dvs\n",
    "del df_dv_binary\n",
    "\n",
    "del df_unsub_files\n",
    "del df_zips_with_BL_store\n",
    "del df_zip_by_store\n",
    "del df_copy_sign_up\n",
    "del list_P_zips\n",
    "del list_S_zips\n",
    "del list_10_zips\n",
    "del df_zip_type\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=10**5\n",
    "dtype_df_1={\n",
    "'customer_id_hashed':sql.types.VARCHAR(length=64),\n",
    "'DV_cumulative_week_updated_1':sql.types.Integer,\n",
    "'DV_cumulative_week_updated_2':sql.types.Integer,\n",
    "'DV_cumulative_week_updated_3':sql.types.Integer,\n",
    "'DV_cumulative_week_updated_4':sql.types.Integer,\n",
    "'sign_up_location':sql.types.VARCHAR(length=5),\n",
    "'customer_zip_code':sql.types.VARCHAR(length=5),\n",
    "'P_zip':sql.types.Integer,\n",
    "'S_zip':sql.types.Integer,\n",
    "'else_10_zip':sql.types.Integer,\n",
    "'signed_online':sql.types.Integer,\n",
    "'distc_to_sign_up':sql.types.Float,\n",
    "'email_unsub_label':sql.types.Integer,\n",
    "'nearest_BL_store':sql.types.VARCHAR(length=4),\n",
    "'nearest_BL_dist':sql.types.Float\n",
    "}\n",
    "df_1.to_sql(name=table_df_1,\n",
    "    con=BL_engine, index=False, if_exists=\"replace\", dtype=dtype_df_1,chunksize=chunksize)\n",
    "print(\"check point 8\")\n",
    "create_index(table_name=table_crm_id_list_train, list_of_columns=[\"customer_id_hashed\"])\n",
    "create_index(table_name=table_crm_id_list_test, list_of_columns=[\"customer_id_hashed\"])\n",
    "create_index(table_name=table_df_1, list_of_columns=[\"customer_id_hashed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json_table_names=\"./table_names_%s.json\"%str(high_date).replace(\"-\",\"\")\n",
    "with open(path_json_table_names,\"w\") as json_file:\n",
    "    json.dump(dict_table_names,json_file)\n",
    "print(\"Done_of_part_2: %s\"%str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
