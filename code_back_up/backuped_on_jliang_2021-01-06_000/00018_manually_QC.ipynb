{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2020-02-05 11:37:30.994022\n",
      "Year: 2020\n",
      "Quarter: 1\n",
      "Current_Quarter: 2020_Q1\n",
      "current_week: 0\n",
      "Need to change the 0th week to 13th week\n",
      "Quarter: 4\n",
      "Current_Quarter: 2019_Q4\n",
      "current_week: 13\n",
      "Final Quarter Name: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.date(2019, 8, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import logging\n",
    "import glob\n",
    "# Change the print content to all of strings for the log writing\n",
    "samplerows=None\n",
    "logging.basicConfig(filename='/home/jian/BL_weekly_crontab/cron_5_weekly_migration_tracker/Tracking_Migration_Group_Performance.log', level=logging.INFO)\n",
    "logging.info(\"Start: \"+str(datetime.datetime.now()))\n",
    "print(\"Start: \"+str(datetime.datetime.now()))\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def recursive_file_gen(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root,file)\n",
    "            \n",
    "last_sturday = (datetime.datetime.now()-datetime.timedelta(days=(datetime.datetime.now().weekday()+2))).date()\n",
    "logging.info(\"last_sturday: \"+str(last_sturday))\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "last_day_of_2018Q4=datetime.date(2019,2,2)\n",
    "\n",
    "year_of_quarter=(last_sturday-last_day_of_2018Q4).days/(52*7)\n",
    "year_of_quarter=str(int(2019+np.floor(year_of_quarter)))\n",
    "logging.info(\"Year: \"+str(year_of_quarter))\n",
    "print(\"Year: \"+str(year_of_quarter))\n",
    "\n",
    "quarter_of_quarter=(last_sturday-last_day_of_2018Q4).days/7\n",
    "quarter_of_quarter=np.floor(quarter_of_quarter/13)%4\n",
    "quarter_of_quarter=str(int(1+quarter_of_quarter))\n",
    "logging.info(\"Quarter: \"+str(quarter_of_quarter))\n",
    "print(\"Quarter: \"+str(quarter_of_quarter))\n",
    "str_current_quarter=year_of_quarter+\"_Q\"+quarter_of_quarter\n",
    "\n",
    "logging.info(\"Current_Quarter: \"+str(str_current_quarter))\n",
    "print(\"Current_Quarter: \"+str(str_current_quarter))\n",
    "\n",
    "current_week=int((last_sturday-last_day_of_2018Q4).days/7%13)\n",
    "logging.info(\"current_week: \"+str(current_week))\n",
    "print(\"current_week: \"+str(current_week))\n",
    "\n",
    "if current_week==0:\n",
    "    logging.info(\"Need to change the 0th week to 13th week\")\n",
    "    print(\"Need to change the 0th week to 13th week\")\n",
    "    quarter_of_quarter=int(quarter_of_quarter)-1+4\n",
    "    year_of_quarter=str(int(year_of_quarter)-1)\n",
    "    str_current_quarter=year_of_quarter+\"_Q\"+str(quarter_of_quarter)\n",
    "    current_week=13\n",
    "    logging.info(\"Quarter: \"+str(quarter_of_quarter))\n",
    "    print(\"Quarter: \"+str(quarter_of_quarter))\n",
    "    logging.info(\"Current_Quarter: \"+str(str_current_quarter))\n",
    "    print(\"Current_Quarter: \"+str(str_current_quarter))\n",
    "    logging.info(\"current_week: \"+str(current_week))\n",
    "    print(\"current_week: \"+str(current_week))\n",
    "    \n",
    "logging.info(\"Final Quarter Name: \"+str(quarter_of_quarter))\n",
    "print(\"Final Quarter Name: \"+str(quarter_of_quarter))\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "'''\n",
    "Naming\n",
    "e.g. current is 2019Q3, 7th week\n",
    "Prior quarter: 2 quarter earlier, in example, 2019Q1\n",
    "Last/Recent quarter: 1 quarter earlier, in example, 2019Q2\n",
    "Current/Ongoing quarter: this quarter started but not end yet, in example, 2019Q3\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "current_quarter_beginning=last_day_of_2018Q4+datetime.timedelta(days=(int(year_of_quarter)-2019)*52*7+                                                                (int(quarter_of_quarter)-1)*13*7+1)\n",
    "\n",
    "logging.info(\"current_quarter_beginning: \"+str(current_quarter_beginning))\n",
    "\n",
    "\n",
    "recent_complete_quarter_End=current_quarter_beginning-datetime.timedelta(days=1)\n",
    "recent_complete_quarter_Start=recent_complete_quarter_End-datetime.timedelta(days=13*7-1)\n",
    "\n",
    "logging.info(\"recent_complete_quarter_End: \"+str(recent_complete_quarter_End))\n",
    "logging.info(\"recent_complete_quarter_Start: \"+str(recent_complete_quarter_Start))\n",
    "\n",
    "'''\n",
    "prior_quarter_End=recent_complete_quarter_End-datetime.timedelta(days=13*7)\n",
    "prior_quarter_Start=recent_complete_quarter_Start-datetime.timedelta(days=13*7)\n",
    "\n",
    "logging.info(\"prior_quarter_End: \"+str(prior_quarter_End))\n",
    "logging.info(\"prior_quarter_Start: \"+str(prior_quarter_Start))\n",
    "'''\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "recent_complete_quarter_Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_run=0\n",
    "\n",
    "'''\n",
    "force_run: variable to force run the migration group by each single id \n",
    "\n",
    "1--run; -1 -- dont't run; 0 only consider the current week number\n",
    "\n",
    "'''\n",
    "if current_week==1 or force_run==1:\n",
    "    \n",
    "    lapsed_date_begin=recent_complete_quarter_Start-datetime.timedelta(days=1)-datetime.timedelta(days=(4*365+1+6)) # Prior quarter back 4years\n",
    "    logging.info(\"lapsed_date_begin: \"+str(lapsed_date_begin))\n",
    "\n",
    "    logging.info(\"As the last week, re-caculate the migration group at the beginning of the ongoing quarter\")\n",
    "    lapsed=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",nrows=samplerows,usecols=['customer_id_hashed','transaction_date'],dtype=str).drop_duplicates()\n",
    "    lapsed=lapsed[lapsed['transaction_date']>=str(lapsed_date_begin)]\n",
    "    \n",
    "    chunksize_num = 10**7\n",
    "    filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "    dftrans_before_20180922=pd.DataFrame()\n",
    "    count_i=0\n",
    "\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str,nrows=samplerows,\n",
    "                             usecols=['customer_id_hashed','transaction_date']):\n",
    "        chunk = chunk.drop_duplicates()\n",
    "        chunk=chunk[chunk['transaction_date']>=str(lapsed_date_begin)]\n",
    "        dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "        count_i+=1\n",
    "        logging.info(str(count_i)+\" read chunk \"+str(datetime.datetime.now()))\n",
    "    dftrans_before_20180922=dftrans_before_20180922.drop_duplicates()  \n",
    "    \n",
    "    all_rewards_most_recent=dftrans_before_20180922.append(lapsed)\n",
    "    all_rewards_most_recent=all_rewards_most_recent.sort_values([\"customer_id_hashed\",\"transaction_date\"],ascending=[True,False])\n",
    "    all_rewards_most_recent=all_rewards_most_recent.drop_duplicates(['customer_id_hashed'])\n",
    "    logging.info(\"all_rewards_most_recent.shape: \"+str(all_rewards_most_recent.shape))\n",
    "\n",
    "    del chunk\n",
    "    del dftrans_before_20180922\n",
    "    gc.collect()\n",
    "\n",
    "    # Before the end of 2019Q1, up to 2019-02-09\n",
    "    historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "    historical_daily_data_list=list(recursive_file_gen(historical_daily_data_folder))\n",
    "    historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "    historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "    historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: datetime.datetime.strptime(x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1],\"%Y%m%d\").date())\n",
    "    historical_daily_df=historical_daily_df[(historical_daily_df['week_end_dt']<=datetime.date(2019,5,4)) & (historical_daily_df['week_end_dt']>datetime.date(2018,9,22))] # hard-code due to the 1st week of daily in item available\n",
    "    historical_daily_df=historical_daily_df.sort_values(\"week_end_dt\")\n",
    "    logging.info(\"historical_daily_df.shape\"+str(historical_daily_df.shape))\n",
    "    \n",
    "    \n",
    "    # All daily files since 2019\n",
    "    weekly_daily_df=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "    weekly_daily_df=[x for x in weekly_daily_df if (\"2018\" not in x) & (\"2017\" not in x) & (\"2016\" not in x) & (\"hist\" not in x.lower())]\n",
    "    weekly_daily_df=[x for x in weekly_daily_df if (\".txt\" in x) & (\"aily\" in x)]\n",
    "\n",
    "    weekly_daily_df=pd.DataFrame({\"file_path\":weekly_daily_df})\n",
    "    weekly_daily_df['week_end_dt']=weekly_daily_df['file_path'].apply(lambda x: datetime.datetime.strptime(x.split(\"/MediaStorm_\")[1][:10],\"%Y-%m-%d\").date())\n",
    "    weekly_daily_df=weekly_daily_df[weekly_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "    logging.info(\"weekly_daily_df.shape: \"+ str(weekly_daily_df.shape))\n",
    "\n",
    "    weekly_daily_df_in_recent_Q=weekly_daily_df[(weekly_daily_df['week_end_dt']>=recent_complete_quarter_Start) & (weekly_daily_df['week_end_dt']<=recent_complete_quarter_End)]\n",
    "    # For later use to read last quarter sales\n",
    "    logging.info(\"weekly_daily_df_in_recent_Q.shape: \"+ str(weekly_daily_df_in_recent_Q.shape))\n",
    "    \n",
    "    weekly_daily_df_upto_pri_quarter_end=weekly_daily_df[weekly_daily_df['week_end_dt']<recent_complete_quarter_Start]\n",
    "    weekly_daily_df_upto_pri_quarter_end=weekly_daily_df_upto_pri_quarter_end.append(historical_daily_df).sort_values(\"week_end_dt\")\n",
    "    \n",
    "    \n",
    "    rew_df_after_20180929_before_pri_quarter=pd.DataFrame()\n",
    "    rew_df_last_quarter_only=pd.DataFrame()\n",
    "\n",
    "    for file in weekly_daily_df_upto_pri_quarter_end['file_path'].tolist():\n",
    "        df=pd.read_table(file,sep=\"|\",dtype=str,nrows=samplerows,\n",
    "                         usecols=[\"customer_id_hashed\",\"transaction_dt\"]).drop_duplicates().rename(columns={\"transaction_dt\":\"transaction_date\"})\n",
    "        df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "        rew_df_after_20180929_before_pri_quarter=rew_df_after_20180929_before_pri_quarter.append(df)\n",
    "        # logging.info(file,datetime.datetime.now())\n",
    "        \n",
    "    for file in weekly_daily_df_in_recent_Q['file_path'].tolist():\n",
    "        df=pd.read_table(file,sep=\"|\",dtype=str,nrows=samplerows,\n",
    "                         usecols=[\"customer_id_hashed\",\"transaction_dt\"]).drop_duplicates().rename(columns={\"transaction_dt\":\"transaction_date\"})\n",
    "        df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "        rew_df_last_quarter_only=rew_df_last_quarter_only.append(df)\n",
    "        # logging.info(file,datetime.datetime.now()) \n",
    "        \n",
    "    all_rewards_most_recent=all_rewards_most_recent.append(rew_df_after_20180929_before_pri_quarter)\n",
    "    del rew_df_after_20180929_before_pri_quarter\n",
    "    all_rewards_most_recent=all_rewards_most_recent.sort_values([\"customer_id_hashed\",\"transaction_date\"],ascending=[True,False])\n",
    "    all_rewards_most_recent=all_rewards_most_recent.drop_duplicates(['customer_id_hashed'])\n",
    "    all_rewards_most_recent['transaction_date_before_the_Quarter']=all_rewards_most_recent['transaction_date']\n",
    "\n",
    "    all_rewards_most_recent_After=all_rewards_most_recent[['customer_id_hashed','transaction_date']].append(rew_df_last_quarter_only)\n",
    "    del rew_df_last_quarter_only\n",
    "    \n",
    "    all_rewards_most_recent_After=all_rewards_most_recent_After.sort_values([\"customer_id_hashed\",\"transaction_date\"],ascending=[True,False])\n",
    "    all_rewards_most_recent_After=all_rewards_most_recent_After.drop_duplicates(['customer_id_hashed']).rename(columns={\"transaction_date\":\"transaction_date_after_the_Quarter\"})\n",
    "\n",
    "    all_rewards_most_recent=pd.merge(all_rewards_most_recent,all_rewards_most_recent_After,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    del all_rewards_most_recent_After\n",
    "\n",
    "    gc.collect()\n",
    "    # Filled the na before with a later date 2019-12-31\n",
    "    all_rewards_most_recent['transaction_date_before_the_Quarter']=all_rewards_most_recent['transaction_date_before_the_Quarter'].fillna(\"2099-12-31\")\n",
    "    # Change here to use unique dates and merge\n",
    "    all_rewards_most_recent_date=all_rewards_most_recent[['transaction_date_before_the_Quarter']].drop_duplicates()\n",
    "    all_rewards_most_recent_date['date']=pd.to_datetime(all_rewards_most_recent_date['transaction_date_before_the_Quarter'],format=\"%Y-%m-%d\").dt.date\n",
    "    all_rewards_most_recent_date['Days_to_pre_Quarter_End']=recent_complete_quarter_Start-datetime.timedelta(days=1)-all_rewards_most_recent_date['date']\n",
    "    all_rewards_most_recent_date['Month_to_pre_Quarter_End']=all_rewards_most_recent_date['Days_to_pre_Quarter_End'].apply(lambda x: x.days/(365.25/12))\n",
    "    all_rewards_most_recent_date['Group_before_the_Quarter']=np.where((all_rewards_most_recent_date['Month_to_pre_Quarter_End']>12) & (all_rewards_most_recent_date['Month_to_pre_Quarter_End']<=48),\"Lapsed_13_48\",\n",
    "                                                           np.where(all_rewards_most_recent_date['Month_to_pre_Quarter_End']>48,\"WD_48+\",\n",
    "                                                                    np.where((all_rewards_most_recent_date['Month_to_pre_Quarter_End']>=0) & (all_rewards_most_recent_date['Month_to_pre_Quarter_End']<=12),\"Active\",\n",
    "                                                                            np.where(all_rewards_most_recent_date['Month_to_pre_Quarter_End']<0,\"NotAvailable_Before_Quarter\",\"NaN\")\n",
    "                                                                            )\n",
    "                                                                   )\n",
    "                                                           )\n",
    "    \n",
    "    all_rewards_most_recent=pd.merge(all_rewards_most_recent,all_rewards_most_recent_date,on=\"transaction_date_before_the_Quarter\",how=\"left\")\n",
    "    del all_rewards_most_recent_date\n",
    "    del all_rewards_most_recent['transaction_date_before_the_Quarter']\n",
    "    all_rewards_most_recent=all_rewards_most_recent.rename(columns={\"date\":\"transaction_date_before_the_Quarter\"})\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    all_rewards_most_recent_date=all_rewards_most_recent[['transaction_date_after_the_Quarter']].drop_duplicates()\n",
    "    all_rewards_most_recent_date['date']=pd.to_datetime(all_rewards_most_recent_date['transaction_date_after_the_Quarter'],format=\"%Y-%m-%d\").dt.date\n",
    "    all_rewards_most_recent_date['Days_to_recentt_Quarter_End']=recent_complete_quarter_End-all_rewards_most_recent_date['date']\n",
    "    all_rewards_most_recent_date['Month_to_recent_Quarter_End']=all_rewards_most_recent_date['Days_to_recentt_Quarter_End'].apply(lambda x: x.days/(365.25/12))\n",
    "    all_rewards_most_recent_date['Group_after_the_Quarter']=np.where((all_rewards_most_recent_date['Month_to_recent_Quarter_End']>12) & (all_rewards_most_recent_date['Month_to_recent_Quarter_End']<=48),\"Lapsed_13_48\",\n",
    "                                                           np.where(all_rewards_most_recent_date['Month_to_recent_Quarter_End']>48,\"WD_48+\",\n",
    "                                                                    np.where((all_rewards_most_recent_date['Month_to_recent_Quarter_End']>=0) & (all_rewards_most_recent_date['Month_to_recent_Quarter_End']<=12),\"Active\",\n",
    "                                                                            np.where(all_rewards_most_recent_date['Month_to_recent_Quarter_End']<0,\"NotAvailable_Before_Q4\",\"NaN\")\n",
    "                                                                            )\n",
    "                                                                   )\n",
    "                                                           )\n",
    "    \n",
    "    \n",
    "    all_rewards_most_recent=pd.merge(all_rewards_most_recent,all_rewards_most_recent_date,on=\"transaction_date_after_the_Quarter\",how=\"left\")\n",
    "    del all_rewards_most_recent_date\n",
    "    del all_rewards_most_recent['transaction_date_after_the_Quarter']\n",
    "    all_rewards_most_recent=all_rewards_most_recent.rename(columns={\"date\":\"transaction_date_after_the_Quarter\"})\n",
    "    \n",
    "    logging.info(str(all_rewards_most_recent['Group_before_the_Quarter'].unique().tolist()))\n",
    "    logging.info(str(all_rewards_most_recent['Group_after_the_Quarter'].unique()))\n",
    "    gc.collect()\n",
    "    \n",
    "    RecentQuarter_sales_by_id=pd.DataFrame()\n",
    "\n",
    "    for file in weekly_daily_df_in_recent_Q['file_path'].tolist():\n",
    "        df=pd.read_table(file,sep=\"|\",dtype=str,nrows=samplerows,\n",
    "                         usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','item_transaction_amt'])\n",
    "        df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "        df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "        df_sales=df.groupby(['customer_id_hashed'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales_recent_Quarter\"})\n",
    "\n",
    "        df_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates()\n",
    "        df_trans=df_trans.groupby([\"customer_id_hashed\"])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans_recent_Quarter\"})\n",
    "\n",
    "        df=pd.merge(df_sales,df_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "\n",
    "        RecentQuarter_sales_by_id=RecentQuarter_sales_by_id.append(df)\n",
    "        # logging.info(file,datetime.datetime.now())\n",
    "    RecentQuarter_sales_by_id=RecentQuarter_sales_by_id.groupby(\"customer_id_hashed\")['sales_recent_Quarter','trans_recent_Quarter'].sum().reset_index()\n",
    "    RecentQuarter_sales_by_id['Recent_Quarter_Shopping_Group']=\"Recent_Quarter_Shopped\"\n",
    "    \n",
    "    \n",
    "    all_rewards_most_recent=pd.merge(all_rewards_most_recent,RecentQuarter_sales_by_id,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    del RecentQuarter_sales_by_id\n",
    "    gc.collect()\n",
    "    # New sign ups in recent quarter\n",
    "    Recent_Quarter_new_sign_ups_files=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "    Recent_Quarter_new_sign_ups_files=[x for x in Recent_Quarter_new_sign_ups_files if (\"Master\" in x) & (\".txt\" in x) & (\"2018\" not in x) & (\"2017\" not in x) & (\"2016\" not in x)]\n",
    "    Recent_Quarter_new_sign_ups_files_df=pd.DataFrame({\"file_path\":Recent_Quarter_new_sign_ups_files})\n",
    "    Recent_Quarter_new_sign_ups_files_df['week_end_dt']=Recent_Quarter_new_sign_ups_files_df['file_path'].apply(lambda x: x.split(\"/MediaStorm_\")[1][:10])\n",
    "    Recent_Quarter_new_sign_ups_files_df=Recent_Quarter_new_sign_ups_files_df[Recent_Quarter_new_sign_ups_files_df['week_end_dt']<=str(recent_complete_quarter_End)]\n",
    "    Recent_Quarter_new_sign_ups_files_df=Recent_Quarter_new_sign_ups_files_df[Recent_Quarter_new_sign_ups_files_df['week_end_dt']>=str(recent_complete_quarter_Start)]\n",
    "    logging.info(\"Recent_Quarter_new_sign_ups_files_df.shape: \" + str(Recent_Quarter_new_sign_ups_files_df.shape))\n",
    "\n",
    "    # Only needed for the Q2\n",
    "    df_new_sign_the_gap=\"/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/MediaStorm Rewards Master P4 2019 - no transaction info.zip\"\n",
    "    df_new_sign_the_gap=pd.read_table(df_new_sign_the_gap,sep=\"|\",usecols=['customer_id_hashed','sign_up_date'])\n",
    "    \n",
    "    recent_new_sign_ups=df_new_sign_the_gap.copy()\n",
    "    for file in Recent_Quarter_new_sign_ups_files_df['file_path'].unique().tolist():\n",
    "        df=pd.read_table(file,sep=\"|\",usecols=['customer_id_hashed','sign_up_date'])\n",
    "        recent_new_sign_ups=recent_new_sign_ups.append(df)\n",
    "    recent_new_sign_ups=recent_new_sign_ups[(recent_new_sign_ups['sign_up_date']<=str(recent_complete_quarter_End))  & (recent_new_sign_ups['sign_up_date']>=str(recent_complete_quarter_Start))]\n",
    "    recent_new_sign_ups=recent_new_sign_ups.sort_values(['customer_id_hashed','sign_up_date']).drop_duplicates(\"customer_id_hashed\")\n",
    "    recent_new_sign_ups['NewRewards_RecentQuarter']=\"Recent_Quarter_New_Sign_Ups\"\n",
    "    logging.info(\"recent_new_sign_ups.shape: \"+str(recent_new_sign_ups.shape))\n",
    "    del recent_new_sign_ups['sign_up_date']\n",
    "    \n",
    "    all_rewards_most_recent=pd.merge(all_rewards_most_recent,recent_new_sign_ups,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    all_rewards_most_recent['NewRewards_RecentQuarter']=all_rewards_most_recent['NewRewards_RecentQuarter'].fillna(\"Old_Rewards_Members\")\n",
    "    del recent_new_sign_ups\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_rewards_most_recent['sales_recent_Quarter']=all_rewards_most_recent['sales_recent_Quarter'].fillna(0)\n",
    "    all_rewards_most_recent['trans_recent_Quarter']=all_rewards_most_recent['trans_recent_Quarter'].fillna(0)\n",
    "\n",
    "    all_rewards_most_recent['Recent_Quarter_Shopping_Group']=all_rewards_most_recent['Recent_Quarter_Shopping_Group'].fillna(\"Recent_Quarter_No_Shop\")\n",
    "    all_rewards_most_recent['Group_before_the_Quarter']=all_rewards_most_recent['Group_before_the_Quarter'].fillna(\"nan\")\n",
    "    all_rewards_most_recent['Group_after_the_Quarter']=all_rewards_most_recent['Group_after_the_Quarter'].fillna(\"nan\")\n",
    "\n",
    "    output_id_recentquarter_count=all_rewards_most_recent.groupby(['Group_before_the_Quarter','Group_after_the_Quarter','NewRewards_RecentQuarter','Recent_Quarter_Shopping_Group'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "    output_id_recentquarter_sales=all_rewards_most_recent.groupby(['Group_before_the_Quarter','Group_after_the_Quarter','NewRewards_RecentQuarter','Recent_Quarter_Shopping_Group'])['sales_recent_Quarter','trans_recent_Quarter'].sum().reset_index()\n",
    "\n",
    "    output_recentquarter=pd.merge(output_id_recentquarter_count,output_id_recentquarter_sales,on=['Group_before_the_Quarter','Group_after_the_Quarter','NewRewards_RecentQuarter','Recent_Quarter_Shopping_Group'],how=\"outer\")\n",
    "    all_rewards_most_recent.to_csv('/home/jian/celery/Migration_Performance/quarterly_report/migration_group_for_recent_quarter_before_'+                               year_of_quarter+\"Q\"+str(quarter_of_quarter)+\".csv\",index=False)\n",
    "    output_recentquarter.to_csv('/home/jian/celery/Migration_Performance/quarterly_report/summary/summary_migration_group_for_recent_quarter_before_'+                                   year_of_quarter+\"Q\"+str(quarter_of_quarter)+\".csv\",index=False)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "all_rewards_most_recent=glob.glob(\"/home/jian/celery/Migration_Performance/quarterly_report/*.csv\")\n",
    "all_rewards_most_recent=[x for x in all_rewards_most_recent if year_of_quarter+\"Q\"+str(quarter_of_quarter) in x]\n",
    "if len(all_rewards_most_recent)!=1:\n",
    "    logging.info(\"Error: multiple files about id by group at the begining of the quarter are saved, please check\")\n",
    "else:\n",
    "    df_all_rewards_most_recent=pd.read_csv(all_rewards_most_recent[0],dtype=str)\n",
    "    df_all_rewards_most_recent['sales_recent_Quarter']=df_all_rewards_most_recent['sales_recent_Quarter'].astype(float)\n",
    "    df_all_rewards_most_recent['trans_recent_Quarter']=df_all_rewards_most_recent['trans_recent_Quarter'].astype(float).astype(int)\n",
    "if 'sign_up_date' in df_all_rewards_most_recent.columns.tolist():\n",
    "    del df_all_rewards_most_recent['sign_up_date']\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# Read this onging quarter performance\n",
    "weekly_daily_df=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "weekly_daily_df=[x for x in weekly_daily_df if (\"2018\" not in x) & (\"2017\" not in x) & (\"2016\" not in x) & (\"hist\" not in x.lower())]\n",
    "weekly_daily_df=[x for x in weekly_daily_df if (\".txt\" in x) & (\"aily\" in x)]\n",
    "\n",
    "weekly_daily_df=pd.DataFrame({\"file_path\":weekly_daily_df})\n",
    "weekly_daily_df['week_end_dt']=weekly_daily_df['file_path'].apply(lambda x: datetime.datetime.strptime(x.split(\"/MediaStorm_\")[1][:10],\"%Y-%m-%d\").date())\n",
    "weekly_daily_df=weekly_daily_df[weekly_daily_df['week_end_dt']>current_quarter_beginning]\n",
    "\n",
    "\n",
    "if weekly_daily_df.shape[0]!=current_week:\n",
    "    logging.info(\"Error: ongoing quarter daily file count doesn't match\")\n",
    "else:\n",
    "    current_quarter_file_list=weekly_daily_df['file_path'].tolist()\n",
    "    current_quarter_sales=pd.DataFrame()\n",
    "    for file in current_quarter_file_list:\n",
    "        df=pd.read_table(file,sep=\"|\",dtype=str,nrows=samplerows,\n",
    "                         usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','item_transaction_amt'])\n",
    "        df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "        df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "        df_sales=df.groupby(['customer_id_hashed'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales_ongoing_Quarter\"})\n",
    "\n",
    "        df_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates()\n",
    "        df_trans=df_trans.groupby([\"customer_id_hashed\"])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans_ongoing_Quarter\"})\n",
    "\n",
    "        df=pd.merge(df_sales,df_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "\n",
    "        current_quarter_sales=current_quarter_sales.append(df)\n",
    "    del df\n",
    "    current_quarter_sales=current_quarter_sales.groupby('customer_id_hashed')['sales_ongoing_Quarter','trans_ongoing_Quarter'].sum().reset_index()\n",
    "    current_quarter_sales['Ongoing_Quarter_Shopping_Group']=\"Ongoing_Quarter_Shopped\"\n",
    "\n",
    "    Ongoing_Quarter_new_sign_ups_files=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "    Ongoing_Quarter_new_sign_ups_files=[x for x in Ongoing_Quarter_new_sign_ups_files if (\"Master\" in x) & (\".txt\" in x) & (\"2018\" not in x) & (\"2017\" not in x) & (\"2016\" not in x)]\n",
    "    Ongoing_Quarter_new_sign_ups_files_df=pd.DataFrame({\"file_path\":Ongoing_Quarter_new_sign_ups_files})\n",
    "    Ongoing_Quarter_new_sign_ups_files_df['week_end_dt']=Ongoing_Quarter_new_sign_ups_files_df['file_path'].apply(lambda x: x.split(\"/MediaStorm_\")[1][:10])\n",
    "    Ongoing_Quarter_new_sign_ups_files_df=Ongoing_Quarter_new_sign_ups_files_df[Ongoing_Quarter_new_sign_ups_files_df['week_end_dt']>=str(current_quarter_beginning)]\n",
    "    logging.info(\"Ongoing_Quarter_new_sign_ups_files_df.shape: \" + str(Ongoing_Quarter_new_sign_ups_files_df.shape))\n",
    "\n",
    "\n",
    "    ongoing_new_sign_ups=pd.DataFrame()\n",
    "    for file in Ongoing_Quarter_new_sign_ups_files_df['file_path'].unique().tolist():\n",
    "        df=pd.read_table(file,sep=\"|\",usecols=['customer_id_hashed','sign_up_date'])\n",
    "        ongoing_new_sign_ups=ongoing_new_sign_ups.append(df)\n",
    "    ongoing_new_sign_ups=ongoing_new_sign_ups[ongoing_new_sign_ups['sign_up_date']>=str(current_quarter_beginning)]\n",
    "    ongoing_new_sign_ups=ongoing_new_sign_ups.sort_values(['customer_id_hashed','sign_up_date']).drop_duplicates(\"customer_id_hashed\")\n",
    "    ongoing_new_sign_ups['NewRewards_OngoingQuarter']=\"Ongoing_Quarter_New_Sign_Ups\"\n",
    "    del ongoing_new_sign_ups['sign_up_date']\n",
    "    logging.info(\"Ongoing_new_sign_ups.shape: \"+str(ongoing_new_sign_ups.shape))\n",
    "\n",
    "    df_all_rewards_most_recent=pd.merge(df_all_rewards_most_recent,current_quarter_sales,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    del current_quarter_sales\n",
    "    df_all_rewards_most_recent['Ongoing_Quarter_Shopping_Group']=df_all_rewards_most_recent['Ongoing_Quarter_Shopping_Group'].fillna(\"Ongoing_Quarter_No_Shop\")\n",
    "    gc.collect()\n",
    "    \n",
    "    df_all_rewards_most_recent=pd.merge(df_all_rewards_most_recent,ongoing_new_sign_ups,on=\"customer_id_hashed\",how=\"outer\")\n",
    "    del ongoing_new_sign_ups\n",
    "    df_all_rewards_most_recent['NewRewards_OngoingQuarter']=df_all_rewards_most_recent['NewRewards_OngoingQuarter'].fillna(\"Old_Rewards_Members\")\n",
    "    \n",
    "    df_all_rewards_most_recent['sales_recent_Quarter']=df_all_rewards_most_recent['sales_recent_Quarter'].fillna(0)\n",
    "    df_all_rewards_most_recent['trans_recent_Quarter']=df_all_rewards_most_recent['trans_recent_Quarter'].fillna(0)\n",
    "    df_all_rewards_most_recent['sales_ongoing_Quarter']=df_all_rewards_most_recent['sales_ongoing_Quarter'].fillna(0)\n",
    "    df_all_rewards_most_recent['trans_ongoing_Quarter']=df_all_rewards_most_recent['trans_ongoing_Quarter'].fillna(0)\n",
    "    \n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "deminsion_cols=['Group_before_the_Quarter','Group_after_the_Quarter','NewRewards_RecentQuarter','Recent_Quarter_Shopping_Group','NewRewards_OngoingQuarter','Ongoing_Quarter_Shopping_Group']\n",
    "for col in deminsion_cols:\n",
    "    df_all_rewards_most_recent[col]=df_all_rewards_most_recent[col].fillna('nan')\n",
    "    logging.info(str(datetime.datetime.now())+col)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "output_id_count_everygroup=df_all_rewards_most_recent.groupby(deminsion_cols)['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "output_id_sales_everygroup=df_all_rewards_most_recent.groupby(deminsion_cols)['sales_recent_Quarter','trans_recent_Quarter','sales_ongoing_Quarter','trans_ongoing_Quarter'].sum().reset_index()\n",
    "\n",
    "output_everygroup=pd.merge(output_id_count_everygroup,output_id_sales_everygroup,on=deminsion_cols,how=\"outer\")\n",
    "\n",
    "\n",
    "output_everygroup['Recent_Quarteer_Sale_per_ID']=output_everygroup['sales_recent_Quarter']/output_everygroup['id_count']\n",
    "output_everygroup['Recent_Quarteer_Sale_per_ID']=output_everygroup['Recent_Quarteer_Sale_per_ID'].apply(lambda x: np.round(x,2))\n",
    "output_everygroup['Recent_Quarteer_Trans_per_ID']=output_everygroup['trans_recent_Quarter']/output_everygroup['id_count']\n",
    "output_everygroup['Recent_Quarteer_Trans_per_ID']=output_everygroup['Recent_Quarteer_Trans_per_ID'].apply(lambda x: np.round(x,2))\n",
    "\n",
    "output_everygroup['ongoing_Quarteer_Sale_per_ID']=output_everygroup['sales_ongoing_Quarter']/output_everygroup['id_count']\n",
    "output_everygroup['ongoing_Quarteer_Sale_per_ID']=output_everygroup['ongoing_Quarteer_Sale_per_ID'].apply(lambda x: np.round(x,2))\n",
    "output_everygroup['ongoing_Quarteer_Trans_per_ID']=output_everygroup['trans_ongoing_Quarter']/output_everygroup['id_count']\n",
    "output_everygroup['ongoing_Quarteer_Trans_per_ID']=output_everygroup['ongoing_Quarteer_Trans_per_ID'].apply(lambda x: np.round(x,2))\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "output_everygroup['before_ongoing_quarter_label']=np.where(output_everygroup['Group_after_the_Quarter']==\"Active\",\"Active\",\n",
    "                                                          np.where(output_everygroup['Group_after_the_Quarter']==\"Lapsed_13_48\",\"Lapsed\",\n",
    "                                                                  np.where(((output_everygroup['Group_after_the_Quarter']==\"nan\") &\\\n",
    "                                                                            (output_everygroup['NewRewards_RecentQuarter']==\"Recent_Quarter_New_Sign_Ups\") &\\\n",
    "                                                                            (output_everygroup['Recent_Quarter_Shopping_Group']==\"Recent_Quarter_No_Shop\")),\" Sign Up No Purchase (Previous Quarter)\",\"nan\")\n",
    "                                                                  )\n",
    "                                                          )\n",
    "count_pre=output_everygroup.groupby(\"before_ongoing_quarter_label\")['id_count'].sum().to_frame().reset_index()\n",
    "count_pre=count_pre[count_pre['before_ongoing_quarter_label']!=\"nan\"]\n",
    "\n",
    "output_everygroup['in_quarter_label']=np.where(((output_everygroup['Group_after_the_Quarter']==\"Active\") &                                                (output_everygroup['Ongoing_Quarter_Shopping_Group']==\"Ongoing_Quarter_Shopped\")),\"Active Shopper\",\n",
    "                                               np.where(((output_everygroup['Group_after_the_Quarter']==\"Lapsed_13_48\") &\\\n",
    "                                                         (output_everygroup['Ongoing_Quarter_Shopping_Group']==\"Ongoing_Quarter_Shopped\")),\"Activated Lapsed\",\n",
    "                                                        np.where(((output_everygroup['NewRewards_RecentQuarter']==\"Recent_Quarter_New_Sign_Ups\") &\\\n",
    "                                                                 (output_everygroup['Recent_Quarter_Shopping_Group']==\"Recent_Quarter_No_Shop\") &\\\n",
    "                                                                 (output_everygroup['Ongoing_Quarter_Shopping_Group']==\"Ongoing_Quarter_Shopped\") &\\\n",
    "                                                                 (output_everygroup['Group_before_the_Quarter']==\"nan\") &\\\n",
    "                                                                 (output_everygroup['Group_after_the_Quarter']==\"nan\")),\"Activated Recent Sign Up No Purchase\",\n",
    "                                                                 np.where(((output_everygroup['Group_after_the_Quarter'].isin(['WD_48+','nan'])) &\\\n",
    "                                                                            (output_everygroup['Ongoing_Quarter_Shopping_Group']==\"Ongoing_Quarter_Shopped\") &\\\n",
    "                                                                            (output_everygroup['NewRewards_OngoingQuarter']==\"Old_Rewards_Members\") &\\\n",
    "                                                                            (output_everygroup['NewRewards_RecentQuarter']!=\"Recent_Quarter_New_Sign_Ups\")),\"Resurrected Lapsed\",\n",
    "                                                                           np.where(((output_everygroup['NewRewards_OngoingQuarter']==\"Ongoing_Quarter_New_Sign_Ups\") &\\\n",
    "                                                                                     (output_everygroup['Ongoing_Quarter_Shopping_Group']==\"Ongoing_Quarter_Shopped\") &\\\n",
    "                                                                                     (output_everygroup['Group_after_the_Quarter']==\"nan\")),\"New Rewards Purchaser\",\n",
    "                                                                                    \"nan\")\n",
    "                                                                          )\n",
    "                                                                )\n",
    "                                                       )\n",
    "                                              )\n",
    "count_in_ongoing=output_everygroup.groupby(\"in_quarter_label\")['id_count'].sum().to_frame().reset_index()\n",
    "count_in_ongoing=count_in_ongoing[count_in_ongoing['in_quarter_label']!=\"nan\"]\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "count_in_ongoing\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "writer_my=pd.ExcelWriter(\"/home/jian/celery/Migration_Performance/weekly_report/BL_tracking_migration_status_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "output_everygroup.to_excel(writer_my,\"pivot\",index=False)\n",
    "count_pre.to_excel(writer_my,\"count_pre\",index=False)\n",
    "count_in_ongoing.to_excel(writer_my,\"count_in_ongoing\",index=False)\n",
    "writer_my.save()\n",
    "\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "writer_Simeng=pd.ExcelWriter(\"/home/simeng/outputs_\"+str(last_sturday)+\"/BL_tracking_migration_status_JL_\"+str(last_sturday)+\".xlsx\",engine=\"xlsxwriter\")\n",
    "output_everygroup.to_excel(writer_Simeng,\"pivot\",index=False)\n",
    "count_pre.to_excel(writer_Simeng,\"count_pre\",index=False)\n",
    "count_in_ongoing.to_excel(writer_Simeng,\"count_in_ongoing\",index=False)\n",
    "writer_Simeng.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
