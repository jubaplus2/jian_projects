{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated reqeust that need to include all shopper in the past 48 months from JT\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sqlalchemy\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Start on: 2019-02-03\n",
      "Lapsed & Store_allocation Start on: 2018-08-05\n",
      "Lapsed 19-48 Start on: 2016-02-07\n"
     ]
    }
   ],
   "source": [
    "# up to the end of 2019Q4, latest data\n",
    "samplerows = None\n",
    "\n",
    "lastdate_date = datetime.date(2020,2,1) # Recent Saturday\n",
    "Beginning_12_months_ago = str(lastdate_date-datetime.timedelta(days=52*7-1)) # Sunday\n",
    "Beginning_18_months_ago=str(lastdate_date-datetime.timedelta(days=52*7*1.5-1)) # Sunday\n",
    "Beginning_48_months_ago=str(lastdate_date-datetime.timedelta(days=52*7*4-1)) # Sunday\n",
    "\n",
    "lastdate_str=str(lastdate_date)\n",
    "print(\"Active Start on: \"+Beginning_12_months_ago) #>=\n",
    "print(\"Lapsed & Store_allocation Start on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Lapsed 19-48 Start on: \"+Beginning_48_months_ago) #>=\n",
    "\n",
    "def recursive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_item_files=glob.glob(\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/*.txt\")\n",
    "\n",
    "daily_item_files_2019_20202=list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "daily_item_files_2019_20202_part2=list(recursive_file_gen(\"/home/jian/BigLots/2020_by_weeks/\"))\n",
    "daily_item_files_2019_20202=daily_item_files_2019_20202+daily_item_files_2019_20202_part2\n",
    "del daily_item_files_2019_20202_part2\n",
    "\n",
    "daily_item_files_2019_20202=[x for x in daily_item_files_2019_20202 if x[-4:]==\".txt\" and \"daily\" in x.lower()]\n",
    "\n",
    "daily_subclass_files_2018=list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\"))\n",
    "daily_subclass_files_2018=[x for x in daily_subclass_files_2018 if x[-4:]==\".txt\" and \"daily\" in x.lower()]\n",
    "\n",
    "historical_item_files.sort()\n",
    "daily_item_files_2019_20202.sort()\n",
    "daily_subclass_files_2018.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_date_historical_item: 2018-08-05\n",
      "max_date_historical_item: 2019-02-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 1: item 2018-08-05 to 2019-02-09\n",
    "\n",
    "df=pd.read_table(historical_item_files[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_historical_item: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "# min_date_historical_item: 2018-08-05\n",
    "\n",
    "\n",
    "df=pd.read_table(historical_item_files[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_historical_item: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "# max_date_historical_item: 2019-02-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_date_daily_item: 2019-02-10\n",
      "max_date_daily_item: 2020-02-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 2: item 2019-02-10 to 2020-02-02\n",
    "\n",
    "daily_item_files_2019_20202=[x for x in daily_item_files_2019_20202 if x.split(\"_weeks/MediaStorm_\")[1][:10]>=\"2019-02-10\"]\n",
    "\n",
    "\n",
    "#QC\n",
    "df=pd.read_table(daily_item_files_2019_20202[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_daily_item: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "# min_date_daily_item: 2019-02-10\n",
    "\n",
    "\n",
    "df=pd.read_table(daily_item_files_2019_20202[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_daily_item: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "# max_date_daily_item: 2020-02-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "list_all_files_18=historical_item_files+daily_item_files_2019_20202\n",
    "\n",
    "print(len(list_all_files_18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-02 2018-06-09\n",
      "min_date_sub_data 2016-07-02\n",
      "2018-06-16 2018-08-04\n",
      "2016-07-02 2018-08-04\n"
     ]
    }
   ],
   "source": [
    "subclass_file_list=glob.glob(\"/home/jian/BigLots/hist_daily_data_subclasslevel/*.txt\")\n",
    "df_subclass_file=pd.DataFrame({\"file_path\":subclass_file_list})\n",
    "df_subclass_file['week_end_dt']=df_subclass_file['file_path'].apply(lambda x: x.split(\"es_week_ending_\")[1][:10])\n",
    "df_subclass_file=df_subclass_file[df_subclass_file['week_end_dt']>=Beginning_48_months_ago]\n",
    "df_subclass_file=df_subclass_file[df_subclass_file['week_end_dt']<Beginning_18_months_ago]\n",
    "df_subclass_file=df_subclass_file.sort_values(\"week_end_dt\")\n",
    "print(df_subclass_file['week_end_dt'].min(),df_subclass_file['week_end_dt'].max())\n",
    "\n",
    "min_date_sub_data=df_subclass_file['week_end_dt'].min()\n",
    "print(\"min_date_sub_data\",min_date_sub_data)\n",
    "\n",
    "################\n",
    "daily_sub_file=list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\"))\n",
    "daily_sub_file=[x for x in daily_sub_file if \"dailysales\" in x.lower()]\n",
    "daily_sub_file=[x for x in daily_sub_file if \"eks/MediaStorm_\" in x]\n",
    "df_daily_sub_file=pd.DataFrame({\"file_path\":daily_sub_file})\n",
    "\n",
    "df_daily_sub_file['week_end_dt']=df_daily_sub_file['file_path'].apply(lambda x: x.split(\"s/MediaStorm_\")[1][:10])\n",
    "df_daily_sub_file=df_daily_sub_file[df_daily_sub_file['week_end_dt']>=df_subclass_file['week_end_dt'].max()]\n",
    "df_daily_sub_file=df_daily_sub_file[df_daily_sub_file['week_end_dt']<Beginning_18_months_ago]\n",
    "df_daily_sub_file=df_daily_sub_file.sort_values(\"week_end_dt\")\n",
    "\n",
    "print(df_daily_sub_file['week_end_dt'].min(),df_daily_sub_file['week_end_dt'].max())\n",
    "\n",
    "\n",
    "################\n",
    "df_sub_data_files=df_subclass_file.append(df_daily_sub_file)\n",
    "df_sub_data_files=df_sub_data_files.sort_values(\"week_end_dt\")\n",
    "del df_subclass_file\n",
    "del df_daily_sub_file\n",
    "\n",
    "print(df_sub_data_files['week_end_dt'].min(),df_sub_data_files['week_end_dt'].max())\n",
    "\n",
    "min_date_sub_data=df_sub_data_files['week_end_dt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>week_end_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>/home/jian/BigLots/hist_daily_data_subclasslev...</td>\n",
       "      <td>2016-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>/home/jian/BigLots/hist_daily_data_subclasslev...</td>\n",
       "      <td>2016-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>/home/jian/BigLots/hist_daily_data_subclasslev...</td>\n",
       "      <td>2016-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>/home/jian/BigLots/hist_daily_data_subclasslev...</td>\n",
       "      <td>2016-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>/home/jian/BigLots/hist_daily_data_subclasslev...</td>\n",
       "      <td>2016-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jian/BigLots/2018_by_weeks/MediaStorm_20...</td>\n",
       "      <td>2018-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jian/BigLots/2018_by_weeks/MediaStorm_20...</td>\n",
       "      <td>2018-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jian/BigLots/2018_by_weeks/MediaStorm_20...</td>\n",
       "      <td>2018-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/jian/BigLots/2018_by_weeks/MediaStorm_20...</td>\n",
       "      <td>2018-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/jian/BigLots/2018_by_weeks/MediaStorm_20...</td>\n",
       "      <td>2018-08-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path week_end_dt\n",
       "61  /home/jian/BigLots/hist_daily_data_subclasslev...  2016-07-02\n",
       "70  /home/jian/BigLots/hist_daily_data_subclasslev...  2016-07-09\n",
       "67  /home/jian/BigLots/hist_daily_data_subclasslev...  2016-07-16\n",
       "96  /home/jian/BigLots/hist_daily_data_subclasslev...  2016-07-23\n",
       "47  /home/jian/BigLots/hist_daily_data_subclasslev...  2016-07-30\n",
       "..                                                ...         ...\n",
       "0   /home/jian/BigLots/2018_by_weeks/MediaStorm_20...  2018-07-07\n",
       "3   /home/jian/BigLots/2018_by_weeks/MediaStorm_20...  2018-07-14\n",
       "4   /home/jian/BigLots/2018_by_weeks/MediaStorm_20...  2018-07-21\n",
       "5   /home/jian/BigLots/2018_by_weeks/MediaStorm_20...  2018-07-28\n",
       "10  /home/jian/BigLots/2018_by_weeks/MediaStorm_20...  2018-08-04\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-02-07 2016-06-25\n"
     ]
    }
   ],
   "source": [
    "# 1 out of the 2\n",
    "\n",
    "lapsed_part0=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed','transaction_date'],dtype=str)\n",
    "\n",
    "lapsed_part0=lapsed_part0[(lapsed_part0['transaction_date']>=Beginning_48_months_ago) & (lapsed_part0['transaction_date']<\"2016-06-26\")] # as the Sunday of the 1st week in the subclass level data\n",
    "print(lapsed_part0['transaction_date'].min(),lapsed_part0['transaction_date'].max())\n",
    "\n",
    "lapsed_part0=lapsed_part0[['customer_id_hashed']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2020-02-09 18:47:46.767827\n",
      "11 2020-02-09 19:00:52.497135\n",
      "21 2020-02-09 19:12:15.857093\n",
      "31 2020-02-09 19:28:42.890738\n",
      "41 2020-02-09 19:42:24.216343\n",
      "51 2020-02-09 19:47:48.689216\n",
      "61 2020-02-09 19:50:35.326618\n",
      "71 2020-02-09 19:53:21.864677\n",
      "81 2020-02-09 19:57:07.775951\n",
      "91 2020-02-09 19:59:44.332241\n",
      "101 2020-02-09 20:02:21.022058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 out of the 2\n",
    "\n",
    "i_counter=0\n",
    "list_historical_19_48_weekly_ids=[]\n",
    "for file in df_sub_data_files['file_path'].tolist():\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed'])\n",
    "    df=df[pd.notnull(df['customer_id_hashed'])].drop_duplicates()\n",
    "    \n",
    "    list_historical_19_48_weekly_ids.append(df)\n",
    "    i_counter+=1\n",
    "    if i_counter%10==1:\n",
    "        print(i_counter,datetime.datetime.now())\n",
    "lapsed_part1=pd.concat(list_historical_19_48_weekly_ids)\n",
    "del list_historical_19_48_weekly_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lapsed_1948_ids=lapsed_part0.append(lapsed_part1).drop_duplicates()\n",
    "del lapsed_part0\n",
    "del lapsed_part1\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2f816ecb3c8c342311a4aae16e8514a6a1198df5e6e0fc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>d66b069d8075e9db9e8424f501f002fcb3fcd57d7011e3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   customer_id_hashed\n",
       "7   2f816ecb3c8c342311a4aae16e8514a6a1198df5e6e0fc...\n",
       "10  d66b069d8075e9db9e8424f501f002fcb3fcd57d7011e3..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lapsed_1948_ids.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2020-02-09 20:08:52.054566\n",
      "11 2020-02-09 20:15:47.232033\n",
      "21 2020-02-09 20:26:50.209457\n",
      "31 2020-02-09 20:34:28.672993\n",
      "41 2020-02-09 20:42:25.883734\n",
      "51 2020-02-09 20:50:41.060554\n",
      "61 2020-02-09 20:58:42.402246\n",
      "71 2020-02-09 21:09:20.228360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_df_shoppers_0_18=[]\n",
    "i_counter=0\n",
    "for file in list_all_files_18:\n",
    "    try:\n",
    "        df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','transaction_date'])\n",
    "    except:\n",
    "        df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','transaction_dt']).rename(columns={\"transaction_dt\":\"transaction_date\"})\n",
    "    df=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df=df.sort_values([\"customer_id_hashed\",'transaction_date'],ascending=[True,False]).drop_duplicates()\n",
    "    list_df_shoppers_0_18.append(df)\n",
    "    i_counter+=1\n",
    "    if i_counter%10==1:\n",
    "        print(i_counter,datetime.datetime.now())\n",
    "df_shoppers_0_18=pd.concat(list_df_shoppers_0_18)\n",
    "del list_df_shoppers_0_18\n",
    "df_shoppers_0_18=df_shoppers_0_18.sort_values([\"customer_id_hashed\",'transaction_date'],ascending=[True,False]).drop_duplicates(\"customer_id_hashed\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13310200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4359e79c792c02c04e031f59a420cd4429943c97af38d4...</td>\n",
       "      <td>Lapsed_19_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>f0b12c4a358b937ebd0f0fcb526ef9370d7de500b01701...</td>\n",
       "      <td>Lapsed_19_48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   customer_id_hashed         group\n",
       "12  4359e79c792c02c04e031f59a420cd4429943c97af38d4...  Lapsed_19_48\n",
       "61  f0b12c4a358b937ebd0f0fcb526ef9370d7de500b01701...  Lapsed_19_48"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lapsed_1948_ids=df_lapsed_1948_ids[df_lapsed_1948_ids['customer_id_hashed'].isin(df_shoppers_0_18['customer_id_hashed'].tolist())]\n",
    "df_lapsed_1948_ids['group']=\"Lapsed_19_48\"\n",
    "print(df_lapsed_1948_ids.shape)\n",
    "df_lapsed_1948_ids.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35943448, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3952541</th>\n",
       "      <td>000000ebcf6c6a2f4302291cc9babb0760208fc683b3b5...</td>\n",
       "      <td>Active_0_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738123</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>Lapsed_13_18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        customer_id_hashed         group\n",
       "3952541  000000ebcf6c6a2f4302291cc9babb0760208fc683b3b5...   Active_0_12\n",
       "9738123  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...  Lapsed_13_18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shoppers_0_18['group']=np.where(df_shoppers_0_18['transaction_date']>=Beginning_12_months_ago,\"Active_0_12\",\"Lapsed_13_18\")\n",
    "df_shoppers=df_shoppers_0_18[['customer_id_hashed','group']].append(df_lapsed_1948_ids)\n",
    "print(df_shoppers.shape)\n",
    "df_shoppers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_shoppers_0_18\n",
    "del df_lapsed_1948_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35943448"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary=df_shoppers.groupby(\"group\")['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "df_summary['customer_id_hashed'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>zip_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75040</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20743</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zip_cd zip_type\n",
       "0  75040        P\n",
       "1  20743        P"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zip_type=pd.read_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2020Q1/BL_ta_zip_relabeled.csv\",dtype=str)\n",
    "df_zip_type.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-09 21:36:15.545690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_id_zip.shape (32929931, 2)\n",
      "df_id_zip['customer_id_hashed'].nunique() 32929922\n",
      "2020-02-09 21:44:43.928261\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "BL_SQL_CONNECTION= 'mysql+pymysql://jian:JubaPlus-2017@localhost/BigLots' \n",
    "BL_engine = sqlalchemy.create_engine(\n",
    "        BL_SQL_CONNECTION, \n",
    "        pool_recycle=1800\n",
    "    )\n",
    "\n",
    "df_id_zip=pd.read_sql('select customer_id_hashed, customer_zip_code from BL_Rewards_Master',con=BL_engine)\n",
    "\n",
    "print(\"df_id_zip.shape \"+str(df_id_zip.shape))\n",
    "print(\"df_id_zip['customer_id_hashed'].nunique() \"+str(df_id_zip['customer_id_hashed'].nunique()))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max(sign_up_date)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  max(sign_up_date)\n",
       "0        2020-02-01"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('select max(sign_up_date) from BL_Rewards_Master',con=BL_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_zip['customer_zip_code']=df_id_zip['customer_zip_code'].fillna(\"nozip\")\n",
    "df_id_zip['zip_cd']=df_id_zip['customer_zip_code'].apply(lambda x: x.split(\"-\")[0][:5].zfill(5))\n",
    "del df_id_zip['customer_zip_code']\n",
    "gc.collect()\n",
    "df_id_zip=pd.merge(df_id_zip,df_zip_type,on=\"zip_cd\",how=\"left\")\n",
    "df_id_zip['zip_type']=df_id_zip['zip_type'].fillna(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35943451, 4)\n"
     ]
    }
   ],
   "source": [
    "df_shoppers=pd.merge(df_shoppers,df_id_zip,on=\"customer_id_hashed\",how=\"left\")\n",
    "df_shoppers['zip_type']=df_shoppers['zip_type'].fillna(\"T\")\n",
    "\n",
    "print(df_shoppers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
