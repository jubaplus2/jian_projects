{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "price_threshold=10\n",
    "\n",
    "logging.basicConfig(filename='V2.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_saturday=datetime.date(2019,1,12) # To be changed to the running Tuesday\n",
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "most_recent_daily_data=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "most_recent_daily_data=[x for x in most_recent_daily_data if (\"MediaStormDailySales\" in x) and (str(last_saturday) in x)]\n",
    "\n",
    "if len(most_recent_daily_data)==1:\n",
    "    most_recent_daily_data=most_recent_daily_data[0]\n",
    "else:\n",
    "    most_recent_daily_data=np.nan\n",
    "    logging.info(\"Last Weekly Daily Data Error\", str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-12/MediaStormDailySales20190115-113411-782.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_sub_class_id: [1 2 3]\n",
      "len_class_code_id: [5]\n",
      "Row_RawData: (6159646, 8)\n",
      "Unique_id: 1091276\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_table(most_recent_daily_data,dtype=str,sep=\"|\")\n",
    "print(\"len_sub_class_id:\",data['subclass_id'].apply(lambda x: len(x)).unique())\n",
    "print(\"len_class_code_id:\",data['class_code_id'].apply(lambda x: len(x)).unique())\n",
    "data['subclass_id']=data['subclass_id'].apply(lambda x: x.zfill(3))\n",
    "data['product_comb']=data['class_code_id']+\"-\"+data['subclass_id']\n",
    "del data['class_code_id']\n",
    "del data['subclass_id']\n",
    "data=data[~pd.isnull(data['customer_id_hashed'])]\n",
    "data['subclass_transaction_amt']=data['subclass_transaction_amt'].astype(float)\n",
    "data['subclass_transaction_units']=data['subclass_transaction_units'].astype(int)\n",
    "data['price']=data['subclass_transaction_amt']/data['subclass_transaction_units']\n",
    "print(\"Row_RawData:\",data.shape)\n",
    "print(\"Unique_id:\", len(data['customer_id_hashed'].unique()))\n",
    "data=data[(data['subclass_transaction_amt']>0) & (data['subclass_transaction_units']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['product_comb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxonomy=pd.read_csv(\"/home/jian/BigLots/static_files/ProductTaxonomy/MediaStormProductTaxonomy20190101-135843-066.txt\",dtype=str,sep=\"|\")\n",
    "taxonomy['subclass_id']=taxonomy['subclass_id'].apply(lambda x: x.zfill(3))\n",
    "division_id_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Division Names.txt\",dtype=str,sep=\"|\")\n",
    "department_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",dtype=str,sep=\"|\")\n",
    "class_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Class Names.txt\",dtype=str,sep=\"|\",encoding ='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_item_avg_price=data[['product_comb','price']].groupby(['product_comb'])['price'].mean().to_frame().reset_index()\n",
    "data_item_avg_price=data_item_avg_price.rename(columns={\"price\":\"avg_price\"})\n",
    "\n",
    "data_item_avg_price['class_code_id']=data_item_avg_price['product_comb'].apply(lambda x: x.split(\"-\")[0])\n",
    "data_item_avg_price['subclass_id']=data_item_avg_price['product_comb'].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,taxonomy,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "\n",
    "\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,division_id_id_name,on=\"division_id\",how=\"left\")\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,department_id_name,on=\"department_id\",how=\"left\")\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,class_id_name,on=\"class_code_id\",how=\"left\")\n",
    "data_item_avg_price=data_item_avg_price[['product_comb','avg_price','division_id','division_desc','department_id','department_desc',\n",
    "                                         'class_code_id','class_code_desc','subclass_id','subclass_desc']]\n",
    "\n",
    "data_item_avg_price.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/2018_Q4/Product_Basket/Price_\"+str(last_saturday)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6082846, 8)\n",
      "(480861, 8)\n"
     ]
    }
   ],
   "source": [
    "# $10 of all items as in the email on 2019-01-14\n",
    "\n",
    "product_comb_under_10_set=set(data_item_avg_price[data_item_avg_price['avg_price']<price_threshold]['product_comb'].unique().tolist())\n",
    "\n",
    "print(data.shape)\n",
    "data=data[~data['product_comb'].isin(product_comb_under_10_set)]\n",
    "data_under_10=data[data['product_comb'].isin(product_comb_under_10_set)]\n",
    "data=data.reset_index()\n",
    "del data['index']\n",
    "print(data.shape)\n",
    "dict_item_avg_price=data_item_avg_price.set_index(['product_comb'])['avg_price'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_comb_10_and_above_list=data_item_avg_price[data_item_avg_price['avg_price']>=10]['product_comb'].unique().tolist()\n",
    "\n",
    "data_transactions_list=data.groupby(['location_id','transaction_dt','transaction_id','customer_id_hashed'])['product_comb'].apply(list).to_frame().reset_index().rename(columns={\"product_comb\":\"basket_list\"})\n",
    "data_transactions_units_sales=data.groupby(['location_id','transaction_dt','transaction_id','customer_id_hashed'])['subclass_transaction_units','subclass_transaction_amt'].sum().reset_index().rename(columns={\"subclass_transaction_units\":\"total_item_units\",\"subclass_transaction_amt\":\"total_item_revenue\"})\n",
    "\n",
    "data_transactions=pd.merge(data_transactions_list,data_transactions_units_sales,on=['location_id','transaction_dt','transaction_id','customer_id_hashed'],how=\"left\")\n",
    "data_transactions['basket_str']=data_transactions['basket_list'].apply(lambda x: sorted(x)).astype(str)\n",
    "data_transactions['transactin_id_given']=[x for x in range(1,len(data_transactions)+1)]\n",
    "data_transactions['types']=data_transactions['basket_list'].apply(lambda x: len(x))\n",
    "\n",
    "# To save\n",
    "\n",
    "\n",
    "data=pd.merge(data,data_transactions,on=[\"location_id\",\"transaction_dt\",\"transaction_id\",\"customer_id_hashed\"],how=\"left\")\n",
    "apply_func={\"subclass_transaction_units\":\"sum\",\"transactin_id_given\":\"count\"}\n",
    "\n",
    "single_prod_df=data.groupby(['product_comb'])['subclass_transaction_units','transactin_id_given'].agg(apply_func).reset_index().rename(columns={\"subclass_transaction_units\":\"Total_Units\",\"transactin_id_given\":\"Total_Trans\"})\n",
    "total_unit=single_prod_df['Total_Units'].sum()\n",
    "total_trans=len(data_transactions)\n",
    "\n",
    "single_prod_df['prob_unit']=single_prod_df['Total_Units']/total_unit\n",
    "single_prod_df['prob_tran']=single_prod_df['Total_Trans']/total_trans\n",
    "\n",
    "dict_single_prod_unit=single_prod_df.set_index(['product_comb'])['prob_unit'].to_dict()\n",
    "dict_single_prod_tran=single_prod_df.set_index(['product_comb'])['prob_tran'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_basket=data_transactions.groupby(['basket_str'])['total_item_units','total_item_revenue','transactin_id_given'].agg(\n",
    "            {\"total_item_units\":\"sum\",\"total_item_revenue\":\"sum\",\"transactin_id_given\":\"count\"}).reset_index().rename(columns={\"transactin_id_given\":\"trans_count\"})\n",
    "data_basket['basket_list']=data_basket['basket_str'].apply(eval)\n",
    "data_basket['item_types']=data_basket['basket_list'].apply(len)\n",
    "data_basket=data_basket.sort_values(['item_types','basket_str'])\n",
    "\n",
    "data_basket=data_basket.reset_index()\n",
    "del data_basket['index']\n",
    "\n",
    "unique_id_by_basket=data_transactions.groupby(['basket_str'])['customer_id_hashed'].apply(lambda x: len(set(x))).to_frame().reset_index().rename(columns={'customer_id_hashed':\"unique_ids\"})\n",
    "data_basket=pd.merge(data_basket,unique_id_by_basket,on=\"basket_str\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3725826570719857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_prod_df['prob_tran'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2019-01-28 10:19:46.130650\n",
      "26934\n",
      "3 2019-01-28 10:20:13.121029\n",
      "127884\n",
      "4 2019-01-28 10:20:29.550753\n",
      "305168\n",
      "5 2019-01-28 10:20:45.327337\n",
      "780196\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "def findsubsets(total_set,item_counts):\n",
    "    return list(set(combinations(total_set, item_counts)))\n",
    "\n",
    "for i in range(2,6):\n",
    "    locals()['set_'+str(i)+\"_comb\"]=[]\n",
    "    output_1_basket_str_list_i=sorted(data_basket[data_basket['item_types']==i]['basket_str'].unique().tolist())\n",
    "    output_2_basket_str_list_i_plus=[]\n",
    "    basket_str_list_i_plus=data_basket[data_basket['item_types']>i]['basket_str'].unique().tolist()\n",
    "    \n",
    "    \n",
    "    for set_str in basket_str_list_i_plus:\n",
    "        set_list=eval(set_str)\n",
    "        output_2_basket_str_list_i_plus=list(set(output_2_basket_str_list_i_plus+[str(list(x)) for x in findsubsets(set_list,i)]))\n",
    "        \n",
    "    locals()['set_'+str(i)+\"_comb\"]=sorted(list(set(output_1_basket_str_list_i+output_2_basket_str_list_i_plus)))\n",
    "    print(i, datetime.datetime.now())\n",
    "    print(len(locals()['set_'+str(i)+\"_comb\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basket_transaction_2_plus=data_basket[data_basket['item_types']>=2][['basket_str','trans_count']]\n",
    "basket_transaction_3_plus=data_basket[data_basket['item_types']>=3][['basket_str','trans_count']]\n",
    "basket_transaction_4_plus=data_basket[data_basket['item_types']>=4][['basket_str','trans_count']]\n",
    "basket_transaction_5_plus=data_basket[data_basket['item_types']>=5][['basket_str','trans_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609615"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_basket['total_item_units'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFast to run because of only trans measurement\\n\\n# Starting with transction BAI only first\\n# Approach 1 for the speed test: from the basket level data\\nall_trans_list=data_transactions['basket_str'].tolist()\\ndict_basket_2_support={}\\ndict_basket_2_prob={}\\ni_counter=0\\nfor basket_2 in set_2_comb:\\n    basket_item_1=eval(basket_2)[0]\\n    basket_item_2=eval(basket_2)[1]\\n    \\n    df=basket_transaction_2_plus[basket_transaction_2_plus['basket_str'].apply(lambda x: (basket_item_1 in x) and (basket_item_2 in x))]\\n    \\n    trans_basket_2=df['trans_count'].sum()\\n    dict_basket_2_support.update({basket_2:trans_basket_2})\\n    \\n    prob_basket_2=(trans_basket_2/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2])\\n    dict_basket_2_prob.update({basket_2:prob_basket_2})\\n    i_counter+=1\\n    if i_counter%1000==20:\\n        print(datetime.datetime.now(),i_counter)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Fast to run because of only trans measurement\n",
    "\n",
    "# Starting with transction BAI only first\n",
    "# Approach 1 for the speed test: from the basket level data\n",
    "all_trans_list=data_transactions['basket_str'].tolist()\n",
    "dict_basket_2_support={}\n",
    "dict_basket_2_prob={}\n",
    "i_counter=0\n",
    "for basket_2 in set_2_comb:\n",
    "    basket_item_1=eval(basket_2)[0]\n",
    "    basket_item_2=eval(basket_2)[1]\n",
    "    \n",
    "    df=basket_transaction_2_plus[basket_transaction_2_plus['basket_str'].apply(lambda x: (basket_item_1 in x) and (basket_item_2 in x))]\n",
    "    \n",
    "    trans_basket_2=df['trans_count'].sum()\n",
    "    dict_basket_2_support.update({basket_2:trans_basket_2})\n",
    "    \n",
    "    prob_basket_2=(trans_basket_2/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2])\n",
    "    dict_basket_2_prob.update({basket_2:prob_basket_2})\n",
    "    i_counter+=1\n",
    "    if i_counter%1000==20:\n",
    "        print(datetime.datetime.now(),i_counter)\n",
    "'''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with transction BAI only first\n",
    "# Approach 2 for the speed test: from the detailed raw data level (> $10 only)\n",
    "dict_basket_2_support_trans={}\n",
    "dict_basket_2_BAI_trans={}\n",
    "\n",
    "dict_basket_2_support_items={}\n",
    "dict_basket_2_BAI_items={}\n",
    "\n",
    "i_counter=0\n",
    "for basket_2 in set_2_comb:\n",
    "    basket_item_1=eval(basket_2)[0]\n",
    "    basket_item_2=eval(basket_2)[1]\n",
    "    \n",
    "    df=data[(data['product_comb']==basket_item_1) | (data['product_comb']==basket_item_2)]\n",
    "    \n",
    "    trans_basket_2=len(df['transactin_id_given'].unique())\n",
    "    items_basket_2=df['subclass_transaction_units'].sum()\n",
    "    \n",
    "    dict_basket_2_support_trans.update({basket_2:trans_basket_2})\n",
    "    dict_basket_2_support_items.update({basket_2:items_basket_2})\n",
    "    \n",
    "    \n",
    "    BAI_basket_2_trans=(trans_basket_2/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2])\n",
    "    BAI_basket_2_items=(items_basket_2/total_unit)/(dict_single_prod_unit[basket_item_1]*dict_single_prod_unit[basket_item_2])\n",
    "    \n",
    "    dict_basket_2_BAI_trans.update({basket_2:BAI_basket_2_trans})\n",
    "    dict_basket_2_BAI_items.update({basket_2:BAI_basket_2_items})\n",
    "    \n",
    "    \n",
    "    i_counter+=1\n",
    "    if i_counter%10000==20:\n",
    "        logging.info(str(datetime.datetime.now())+\"|\"+str(i_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "dict_basket_3_support_trans={}\n",
    "dict_basket_3_BAI_trans={}\n",
    "\n",
    "dict_basket_3_support_items={}\n",
    "dict_basket_3_BAI_items={}\n",
    "\n",
    "i_counter=0\n",
    "for basket_3 in set_3_comb:\n",
    "    \n",
    "    \n",
    "    basket_list=eval(basket_3)\n",
    "    basket_item_1=basket_list[0]\n",
    "    basket_item_2=basket_list[1]\n",
    "    basket_item_3=basket_list[2]\n",
    "    \n",
    "    df=data[data['product_comb'].isin(basket_list)]\n",
    "    ####\n",
    "    trans_basket_3=len(df['transactin_id_given'].unique())\n",
    "    items_basket_3=df['subclass_transaction_units'].sum()\n",
    "    \n",
    "    dict_basket_3_support_trans.update({basket_3:trans_basket_3})\n",
    "    dict_basket_3_support_items.update({basket_3:items_basket_3})\n",
    "    ####\n",
    "    \n",
    "    BAI_basket_3_trans=(trans_basket_3/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2]*dict_single_prod_tran[basket_item_3])\n",
    "    BAI_basket_3_items=(items_basket_3/total_unit)/(dict_single_prod_unit[basket_item_1]*dict_single_prod_unit[basket_item_2]*dict_single_prod_unit[basket_item_3])\n",
    "    \n",
    "    dict_basket_3_BAI_trans.update({basket_3:BAI_basket_3_trans})\n",
    "    dict_basket_3_BAI_items.update({basket_3:BAI_basket_3_items})\n",
    "    \n",
    "    \n",
    "    i_counter+=1\n",
    "    if i_counter%10000==20:\n",
    "        logging.info(str(datetime.datetime.now())+\"|\"+str(i_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# 4\n",
    "\n",
    "dict_basket_4_support_trans={}\n",
    "dict_basket_4_BAI_trans={}\n",
    "\n",
    "dict_basket_4_support_items={}\n",
    "dict_basket_4_BAI_items={}\n",
    "\n",
    "i_counter=0\n",
    "for basket_4 in set_4_comb:\n",
    "    \n",
    "    \n",
    "    basket_list=eval(basket_4)\n",
    "    basket_item_1=basket_list[0]\n",
    "    basket_item_2=basket_list[1]\n",
    "    basket_item_3=basket_list[2]\n",
    "    basket_item_4=basket_list[3]\n",
    "    \n",
    "    df=data[data['product_comb'].isin(basket_list)]\n",
    "    ####\n",
    "    trans_basket_4=len(df['transactin_id_given'].unique())\n",
    "    items_basket_4=df['subclass_transaction_units'].sum()\n",
    "    \n",
    "    dict_basket_4_support_trans.update({basket_4:trans_basket_4})\n",
    "    dict_basket_4_support_items.update({basket_4:items_basket_4})\n",
    "    ####\n",
    "    \n",
    "    BAI_basket_4_trans=(trans_basket_4/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2]*dict_single_prod_tran[basket_item_3]*dict_single_prod_tran[basket_item_4])\n",
    "    BAI_basket_4_items=(items_basket_4/total_unit)/(dict_single_prod_unit[basket_item_1]*dict_single_prod_unit[basket_item_2]*dict_single_prod_unit[basket_item_3]*dict_single_prod_unit[basket_item_4])\n",
    "    \n",
    "    dict_basket_4_BAI_trans.update({basket_4:BAI_basket_4_trans})\n",
    "    dict_basket_4_BAI_items.update({basket_4:BAI_basket_4_items})\n",
    "    \n",
    "    \n",
    "    i_counter+=1\n",
    "    if i_counter%10000==20:\n",
    "        logging.info(str(datetime.datetime.now())+\"|\"+str(i_counter))\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# 5\n",
    "\n",
    "dict_basket_5_support_trans={}\n",
    "dict_basket_5_BAI_trans={}\n",
    "\n",
    "dict_basket_5_support_items={}\n",
    "dict_basket_5_BAI_items={}\n",
    "\n",
    "i_counter=0\n",
    "for basket_5 in set_5_comb:\n",
    "    \n",
    "    \n",
    "    basket_list=eval(basket_5)\n",
    "    basket_item_1=basket_list[0]\n",
    "    basket_item_2=basket_list[1]\n",
    "    basket_item_3=basket_list[2]\n",
    "    basket_item_4=basket_list[3]\n",
    "    basket_item_5=basket_list[4]\n",
    "    df=data[data['product_comb'].isin(basket_list)]\n",
    "    ####\n",
    "    trans_basket_5=len(df['transactin_id_given'].unique())\n",
    "    items_basket_5=df['subclass_transaction_units'].sum()\n",
    "    \n",
    "    dict_basket_5_support_trans.update({basket_5:trans_basket_5})\n",
    "    dict_basket_5_support_items.update({basket_5:items_basket_5})\n",
    "    ####\n",
    "    \n",
    "    BAI_basket_5_trans=(trans_basket_5/total_trans)/(dict_single_prod_tran[basket_item_1]*dict_single_prod_tran[basket_item_2]*dict_single_prod_tran[basket_item_3]*dict_single_prod_tran[basket_item_4]*dict_single_prod_tran[basket_item_5])\n",
    "    BAI_basket_5_items=(items_basket_5/total_unit)/(dict_single_prod_unit[basket_item_1]*dict_single_prod_unit[basket_item_2]*dict_single_prod_unit[basket_item_3]*dict_single_prod_unit[basket_item_4]*dict_single_prod_unit[basket_item_5])\n",
    "    \n",
    "    dict_basket_5_BAI_trans.update({basket_5:BAI_basket_5_trans})\n",
    "    dict_basket_5_BAI_items.update({basket_5:BAI_basket_5_items})\n",
    "    \n",
    "    \n",
    "    i_counter+=1\n",
    "    if i_counter%10000==20:\n",
    "        logging.info(str(datetime.datetime.now())+\"|\"+str(i_counter))\n",
    "'''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAI_2345_trans=dict_basket_2_BAI_trans # 23 only\n",
    "BAI_2345_items=dict_basket_2_BAI_items\n",
    "\n",
    "BAI_2345_trans.update(dict_basket_3_BAI_trans)\n",
    "# BAI_2345_trans.update(dict_basket_4_BAI_trans)\n",
    "# BAI_2345_trans.update(dict_basket_5_BAI_trans)\n",
    "\n",
    "BAI_2345_items.update(dict_basket_3_BAI_items)\n",
    "# BAI_2345_items.update(dict_basket_4_BAI_items)\n",
    "# BAI_2345_items.update(dict_basket_5_BAI_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "output_1=data_basket[data_basket['item_types']==1]\n",
    "output_2=data_basket[data_basket['item_types'].isin([2,3])] # 2, 3 only\n",
    "output_3=data_basket[data_basket['item_types']>3]\n",
    "\n",
    "output_1['BAI_trans']=1\n",
    "output_1['BAI_items']=1\n",
    "\n",
    "output_2['BAI_trans']=output_2['basket_str'].apply(lambda x: BAI_2345_trans[x])\n",
    "output_2['BAI_items']=output_2['basket_str'].apply(lambda x: BAI_2345_items[x])\n",
    "\n",
    "output_basket=output_1.append(output_2).append(output_3) # To add those only in multiple item trans\n",
    "#E.g. [a,b,c,d] [a,c] doesn't exsit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(BAI_2345_trans,index=['BAI_Trans']).T.reset_index().rename(columns={\"index\":\"basket_str\"})\n",
    "df2=pd.DataFrame(BAI_2345_items,index=['BAI_Items']).T.reset_index().rename(columns={\"index\":\"basket_str\"})\n",
    "output_all_available=pd.merge(df1,df2,on='basket_str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter('/home/jian/Projects/Big_Lots/Analysis/2018_Q4/Product_Basket/BL_DBasket_Version2_JL_'+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "# output=output[['basket_str','basket_list','BAI_trans','BAI_units','item_types','total_item_revenue','total_item_units','trans_count','unique_ids','price_list']]\n",
    "output_all_available.to_excel(writer,\"BAI_2_to_3\",index=False)\n",
    "output_basket.to_excel(writer,\"basket_only\",index=False)\n",
    "writer.save()\n",
    "\n",
    "logging.info(\"Done: \"+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
