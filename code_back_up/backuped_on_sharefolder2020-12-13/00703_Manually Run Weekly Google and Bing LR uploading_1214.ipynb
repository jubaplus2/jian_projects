{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Upload to LiveRamp-Bing\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import gc\n",
    "\n",
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thismonday 2019-12-16\n",
      "Good to load\n"
     ]
    }
   ],
   "source": [
    "thismonday=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday())\n",
    "# thismonday=datetime.date(2019,3,25)\n",
    "print(\"thismonday\", thismonday)\n",
    "\n",
    "last_week_end_saturday=thismonday-datetime.timedelta(days=2)\n",
    "\n",
    "writer_pather=\"/home/jian/celery/Bing_LiveRamp/output/\"\n",
    "\n",
    "posibble_recent_folder=\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/\"\n",
    "daily_files_recent=[x for x in list(recursive_file_gen(posibble_recent_folder)) if \"Daily\" in x]\n",
    "\n",
    "list_1_after_201806_2019=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"aily\" in x) & (\".txt\" in x) ]\n",
    "list_1_after_201806_2019=[x for x in list_1_after_201806_2019 if str(last_week_end_saturday) in x]\n",
    "\n",
    "\n",
    "daily_files_last_week=daily_files_recent+list_1_after_201806_2019\n",
    "if len(daily_files_last_week)==1:\n",
    "    daily_file_last_week=daily_files_last_week[0]\n",
    "    print(\"Good to load\")\n",
    "else:\n",
    "    daily_file_last_week=np.nan\n",
    "    print(\"Last week daily data not avaiable\")\n",
    "\n",
    "\n",
    "\n",
    "qc_weekly=pd.read_table(daily_file_last_week,sep=\"|\",dtype=str)\n",
    "qc_weekly=qc_weekly[qc_weekly['location_id']!=\"6990\"]\n",
    "qc_weekly['item_transaction_amt']=qc_weekly['item_transaction_amt'].astype(float)\n",
    "qc_weekly_sales=qc_weekly.groupby(['location_id'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales_from_Daily\"})\n",
    "qc_weekly_trans=qc_weekly[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates()\n",
    "qc_weekly_trans=qc_weekly_trans.groupby(['location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans_from_Daily\"})\n",
    "\n",
    "qc_weekly_from_daily=pd.merge(qc_weekly_sales,qc_weekly_trans,on=\"location_id\",how=\"outer\")\n",
    "qc_weekly_from_daily.shape\n",
    "\n",
    "\n",
    "import glob\n",
    "weekly_data_path=glob.glob(\"/home/jian/BigLots/2019_by_weeks/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "if len(weekly_data_path)==0:    \n",
    "    weekly_data_path=glob.glob(\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "    weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "\n",
    "if len(weekly_data_path)==1:\n",
    "    weekly_data_path=weekly_data_path[0]\n",
    "\n",
    "else:\n",
    "    print(\"Checking the new weekly data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thismonday 2019-12-16\n",
      "Good to load\n",
      "1% store sales variances: 2\n",
      "4% store trans variances: 200\n"
     ]
    }
   ],
   "source": [
    "thismonday=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday())\n",
    "# thismonday=datetime.date(2019,3,25)\n",
    "print(\"thismonday\", thismonday)\n",
    "\n",
    "last_week_end_saturday=thismonday-datetime.timedelta(days=2)\n",
    "\n",
    "writer_pather=\"/home/jian/celery/Bing_LiveRamp/output/\"\n",
    "\n",
    "posibble_recent_folder=\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/\"\n",
    "daily_files_recent=[x for x in list(recursive_file_gen(posibble_recent_folder)) if \"Daily\" in x]\n",
    "\n",
    "list_1_after_201806_2019=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"aily\" in x) & (\".txt\" in x) ]\n",
    "list_1_after_201806_2019=[x for x in list_1_after_201806_2019 if str(last_week_end_saturday) in x]\n",
    "\n",
    "\n",
    "daily_files_last_week=daily_files_recent+list_1_after_201806_2019\n",
    "if len(daily_files_last_week)==1:\n",
    "    daily_file_last_week=daily_files_last_week[0]\n",
    "    print(\"Good to load\")\n",
    "else:\n",
    "    daily_file_last_week=np.nan\n",
    "    print(\"Last week daily data not avaiable\")\n",
    "\n",
    "\n",
    "\n",
    "qc_weekly=pd.read_table(daily_file_last_week,sep=\"|\",dtype=str)\n",
    "qc_weekly=qc_weekly[qc_weekly['location_id']!=\"6990\"]\n",
    "qc_weekly['item_transaction_amt']=qc_weekly['item_transaction_amt'].astype(float)\n",
    "qc_weekly_sales=qc_weekly.groupby(['location_id'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales_from_Daily\"})\n",
    "qc_weekly_trans=qc_weekly[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates()\n",
    "qc_weekly_trans=qc_weekly_trans.groupby(['location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans_from_Daily\"})\n",
    "\n",
    "qc_weekly_from_daily=pd.merge(qc_weekly_sales,qc_weekly_trans,on=\"location_id\",how=\"outer\")\n",
    "qc_weekly_from_daily.shape\n",
    "\n",
    "\n",
    "import glob\n",
    "weekly_data_path=glob.glob(\"/home/jian/BigLots/2019_by_weeks/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "if len(weekly_data_path)==0:    \n",
    "    weekly_data_path=glob.glob(\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "    weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "\n",
    "if len(weekly_data_path)==1:\n",
    "    weekly_data_path=weekly_data_path[0]\n",
    "\n",
    "else:\n",
    "    print(\"Checking the new weekly data\")\n",
    "\n",
    "\n",
    "Weekly_Data=pd.read_table(weekly_data_path,dtype=str,sep=\"|\",usecols=[\"location_id\",'week_end_dt','gross_sales_amt','gross_transaction_cnt'])\n",
    "Weekly_Data=Weekly_Data[Weekly_Data['location_id']!=\"6990\"]\n",
    "Weekly_Data=Weekly_Data.drop_duplicates()\n",
    "Weekly_Data['gross_sales_amt']=Weekly_Data['gross_sales_amt'].astype(float)\n",
    "Weekly_Data['gross_transaction_cnt']=Weekly_Data['gross_transaction_cnt'].astype(int)\n",
    "\n",
    "\n",
    "QC_df=pd.merge(Weekly_Data,qc_weekly_from_daily,on=\"location_id\",how=\"outer\")\n",
    "\n",
    "\n",
    "QC_df['Sales_Diff']=(QC_df['gross_sales_amt']-QC_df['sales_from_Daily'])/QC_df['sales_from_Daily']\n",
    "QC_df['Trans_Diff']=(QC_df['gross_transaction_cnt']-QC_df['trans_from_Daily'])/QC_df['gross_transaction_cnt']\n",
    "\n",
    "print(\"1% store sales variances: \"+str(QC_df[(QC_df['Sales_Diff'].apply(lambda x: np.abs(x)>0.01))].shape[0]))\n",
    "print(\"4% store trans variances: \"+str(QC_df[(QC_df['Trans_Diff'].apply(lambda x: np.abs(x)>0.04))].shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to load\n"
     ]
    }
   ],
   "source": [
    "sales_daily_lastweek=pd.read_table(daily_file_last_week,sep=\"|\",dtype=str,usecols=['location_id','customer_id_hashed','transaction_dt','item_transaction_amt'])\n",
    "sales_daily_lastweek=sales_daily_lastweek[~pd.isnull(sales_daily_lastweek['customer_id_hashed'])]\n",
    "sales_daily_lastweek=sales_daily_lastweek[sales_daily_lastweek['location_id']!=\"6990\"]\n",
    "sales_daily_lastweek['item_transaction_amt']=sales_daily_lastweek['item_transaction_amt'].astype(float)\n",
    "sales_daily_lastweek_agg=sales_daily_lastweek.groupby(['customer_id_hashed','transaction_dt'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "sales_daily_lastweek_agg=sales_daily_lastweek_agg.rename(columns={\"transaction_dt\":\"Timestamp\",\"item_transaction_amt\":\"Conversion_Amount\"})\n",
    "sales_daily_lastweek_agg['Timestamp']=sales_daily_lastweek_agg['Timestamp'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "\n",
    "\n",
    "posibble_recent_Master=\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/\"\n",
    "master_files_recent=[x for x in list(recursive_file_gen(posibble_recent_Master)) if \"ster\" in x]\n",
    "\n",
    "if len(master_files_recent)==1:\n",
    "    master_files_recent=master_files_recent[0]\n",
    "    print(\"Good to load\")\n",
    "else:\n",
    "    master_files_recent=np.nan\n",
    "    print(\"Last week Master file not avaiable, Check the cell below if already in\")\n",
    "\n",
    "recent_date=last_week_end_saturday\n",
    "\n",
    "Master_2019_weekly=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"aster\" in x) & (\".txt\" in x) ]\n",
    "\n",
    "\n",
    "weekly_df=pd.DataFrame({\"file_path\":Master_2019_weekly})\n",
    "weekly_df['date']=weekly_df['file_path'].apply(lambda x: x.split(\"_by_weeks/MediaStorm_\")[1][:10])\n",
    "\n",
    "weekly_df['date']=weekly_df['date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "weekly_df=weekly_df[weekly_df['date']>=datetime.date(2019,6,8)]\n",
    "\n",
    "weekly_df=weekly_df.sort_values(\"date\",ascending=False).reset_index()\n",
    "del weekly_df['index']\n",
    "\n",
    "if pd.notnull(master_files_recent):\n",
    "    weekly_df=pd.DataFrame({\"file_path\":master_files_recent,\"date\":recent_date},index=[0]).append(weekly_df)\n",
    "weekly_df=weekly_df.drop_duplicates().reset_index()\n",
    "del weekly_df['index'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7255226, 3)\n",
      "(5759051, 3)\n",
      "(659127, 3)\n",
      "(7888562, 3)\n",
      "(461199, 3)\n",
      "(32125306, 3)\n",
      "32125202\n",
      "Null Email rows excluded: 130133\n",
      "16.783728954223754\n"
     ]
    }
   ],
   "source": [
    "data_0=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip\",\n",
    "    dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code']).drop_duplicates()\n",
    "data_1=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStormCustTot-hashed-email.txt\",\n",
    "    dtype=str,header=None,usecols=[0,1,5])\n",
    "data_1.columns=['customer_id_hashed','email_address_hash','customer_zip_code']\n",
    "data_1['customer_id_hashed']=data_1['customer_id_hashed'].apply(lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest())\n",
    "data_2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'MediaStormCustomerTransactionTotals_2018-01-09_2018-03-31.txt',\n",
    "    sep = ',',dtype = str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "print(data_2.shape)\n",
    "data_3 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'Existing Reward Member Master as of 2018-06-05.txt',\n",
    "    dtype = str,sep = '|',usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "print(data_3.shape)\n",
    "data_4 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'New Reward Member Master as of 2018-06-05.txt',\n",
    "    dtype = str,sep = '|',usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "print(data_4.shape)\n",
    "\n",
    "data_5 = pd.read_csv('/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/All Rewards Members 2018-02-04 - 2019-05-04.zip',\n",
    "    dtype = str,sep = '|',compression=\"zip\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "print(data_5.shape)\n",
    "\n",
    "data_6 = pd.read_csv('/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/MediaStorm Rewards Master P4 2019 - no transaction info.zip',\n",
    "    dtype = str,sep = '|',usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "print(data_6.shape)\n",
    "\n",
    "master_old=data_6.append(data_5).append(data_4).append(data_3).append(data_2).append(data_1).append(data_0).drop_duplicates(\"customer_id_hashed\")\n",
    "del data_6\n",
    "del data_5\n",
    "del data_4\n",
    "del data_3\n",
    "del data_2\n",
    "del data_1\n",
    "del data_0\n",
    "gc.collect()\n",
    "\n",
    "all_weekly_biweekly_master_file=pd.DataFrame()\n",
    "for file_path in weekly_df['file_path'].tolist():\n",
    "    df=pd.read_table(file_path,dtype=str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'],sep=\"|\")\n",
    "    all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.append(df)\n",
    "\n",
    "all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.append(master_old)\n",
    "del master_old\n",
    "gc.collect()\n",
    "\n",
    "print(all_weekly_biweekly_master_file.shape)\n",
    "print(len(all_weekly_biweekly_master_file['customer_id_hashed'].unique()))\n",
    "\n",
    "all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.drop_duplicates('customer_id_hashed')\n",
    "\n",
    "sales_daily_lastweek_agg=pd.merge(sales_daily_lastweek_agg,all_weekly_biweekly_master_file,on=\"customer_id_hashed\",how=\"left\").rename(columns={\"email_address_hash\":\"Email_1\",\"customer_zip_code\":\"Zip\"})\n",
    "sales_daily_lastweek_agg.head(2)\n",
    "\n",
    "\n",
    "print(\"Null Email rows excluded: \"+str(sales_daily_lastweek_agg[pd.isnull(sales_daily_lastweek_agg['Email_1'])].shape[0]))\n",
    "print(sales_daily_lastweek_agg.shape[0]/sales_daily_lastweek_agg[pd.isnull(sales_daily_lastweek_agg['Email_1'])].shape[0])\n",
    "\n",
    "sales_daily_lastweek_agg=sales_daily_lastweek_agg[~pd.isnull(sales_daily_lastweek_agg['Email_1'])]\n",
    "del sales_daily_lastweek_agg['customer_id_hashed']\n",
    "\n",
    "sales_daily_lastweek_agg=sales_daily_lastweek_agg[[\"Email_1\",\"Zip\",\"Timestamp\", \"Conversion_Amount\"]]\n",
    "sales_daily_lastweek_agg['Conversion_Amount']=sales_daily_lastweek_agg['Conversion_Amount'].apply(lambda x: np.round(x,2)).astype(str)\n",
    "sales_daily_lastweek_agg['Conversion_Amount']=sales_daily_lastweek_agg['Conversion_Amount'].apply(lambda x: x.split(\".\")[0]+\".\"+x.split(\".\")[1].ljust(2,\"0\"))\n",
    "sales_daily_lastweek_agg['Product_Group']=\"In_Store\"\n",
    "\n",
    "sales_daily_lastweek_agg['Zip']=\"00000\"\n",
    "\n",
    "data_max_date=sales_daily_lastweek_agg['Timestamp'].max()\n",
    "data_max_date\n",
    "\n",
    "data_min_date=sales_daily_lastweek_agg['Timestamp'].min()\n",
    "data_min_date\n",
    "\n",
    "local_path=writer_pather+\"/BL_LR_BingStoreSales_\"+str(data_min_date)+\"_\"+str(data_max_date)+\"_JL_\"+str(datetime.datetime.now().date())+\".txt\"\n",
    "\n",
    "sales_daily_lastweek_agg.to_csv(local_path,index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully sent email\n",
      "done celery: 2019-12-17 22:06:01.741503\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "\n",
    "host = \"files.liveramp.com\" #hard-coded\n",
    "port = 22\n",
    "transport = paramiko.Transport((host, port))\n",
    "\n",
    "password = \"Jubaplus2019!\" #hard-coded\n",
    "username = \"bing-big-lots\" #hard-coded\n",
    "transport.connect(username = username, password = password)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "# local_path defined above before saving the local txt\n",
    "remote_path=\"/uploads/\"+os.path.basename(local_path)\n",
    "sftp.put(local_path,remote_path)\n",
    "sftp.close()\n",
    "transport.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Google below from the output of Bing\n",
    "df_google=sales_daily_lastweek_agg.rename(columns={\"Zip\":\"Zip_Code\",\n",
    "  \"Timestamp\":\"transaction_timestamp\",\n",
    "  \"Product_Group\":\"transaction_category\",\n",
    "  \"Conversion_Amount\":\"transaction_amount\"})\n",
    "\n",
    "df_google=df_google[['Zip_Code','Email_1','transaction_category','transaction_timestamp','transaction_amount']]\n",
    "\n",
    "local_path_google=\"/home/jian/celery/Google_LiveRamp/output/BL_LR_GoogleStoreSales_\"+str(data_min_date)+\"_\"+str(data_max_date)+\"_JL_\"+str(datetime.datetime.now().date())+\".txt\"\n",
    "df_google.to_csv(local_path_google,index=False,sep=\"|\")\n",
    "\n",
    "import paramiko\n",
    "\n",
    "host = \"files.liveramp.com\" #hard-coded\n",
    "port = 22\n",
    "transport = paramiko.Transport((host, port))\n",
    "\n",
    "password = \"Jubaplus2019!\" #hard-coded\n",
    "username = \"big-lots-ga-aw\" #hard-coded\n",
    "transport.connect(username = username, password = password)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "remote_path=\"/uploads/\"+os.path.basename(local_path_google)\n",
    "sftp.put(local_path_google,remote_path)\n",
    "sftp.close()\n",
    "transport.close()\n",
    "\n",
    "import smtplib\n",
    "message = \"\"\"From: Juba <jubapluscc@gmail.com>\n",
    "To: Jian <jian@jubaplus.com>, Mike Mahler <mmahler@mediastorm.biz>, Maggie Chiu <mchiu@mediastorm.biz>, Naja Aldefri <naldefri@mediastorm.biz>, Daniela Balboni <dbalboni@mediastorm.biz>, Zhenya Brisker <zbrisker@mediastorm.biz>, John Thomas <jthomas@mediastorm.biz>, Simeng Sun <ssun@mediastorm.biz>, Mohammed Uddin <muddin@mediastorm.biz>\n",
    "MIME-Version: 1.0\n",
    "Content-type: text\n",
    "Subject: Big Lots Rewards Sales in Store uploaded to LiveRamp \n",
    "\n",
    "Hi Mike,\n",
    "\n",
    "The last week Big Lots Rewards Sales in Store uploaded to LiveRamp Bing & Google.\n",
    "\n",
    "Thanks,\n",
    "Jian\n",
    "\"\"\"\n",
    "smtpObj = smtplib.SMTP('smtp.gmail.com',587)\n",
    "smtpObj.ehlo()\n",
    "smtpObj.starttls()\n",
    "smtpObj.login('jubapluscc@gmail.com','mfppxsfikqmazbqj')\n",
    "\n",
    "\n",
    "sender=\"jubapluscc@gmail.com\"\n",
    "receivers=['jian@jubaplus.com','mmahler@mediastorm.biz','mchiu@mediastorm.biz', 'naldefri@mediastorm.biz', 'dbalboni@mediastorm.biz', 'zbrisker@mediastorm.biz', 'jthomas@mediastorm.biz', 'ssun@mediastorm.biz', 'muddin@mediastorm.biz','GAbouJaoude@mediastorm.biz']\n",
    "try:\n",
    "    smtpObj.sendmail(sender, receivers, message)         \n",
    "    print(\"Successfully sent email\")\n",
    "except:\n",
    "    print(\"Error: unable to send email\")\n",
    "print(\"done celery: \"+ str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
