{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapsed Start on: 2017-12-31\n",
      "Active Start on: 2018-07-01\n",
      "Store Allocation Starting on: 2017-12-31\n",
      "Last Saturday: 2019-06-29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upto 20190629\n",
    "# merge email first\n",
    "# then calculate decile\n",
    "# Remove 18+\n",
    "# Added the 18+, no quadrants\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='BL_CRM_LR_Quarterly_upto20190629_2019Q3_'+str(datetime.datetime.now().date())+'.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "\n",
    "lastdate_date = datetime.date(2019,6,29) # Recent Saturday\n",
    "active_Sunday = str(lastdate_date-datetime.timedelta(days=52*7-1))\n",
    "\n",
    "lapsed_Sunday = str(lastdate_date-datetime.timedelta(days=52*7*1.5-1))\n",
    "Beginning_18_months_ago=str(lastdate_date-datetime.timedelta(days=52*7*1.5-1))\n",
    "\n",
    "lastdate=str(lastdate_date)\n",
    "print(\"Lapsed Start on: \"+lapsed_Sunday) #>=\n",
    "print(\"Active Start on: \"+active_Sunday) #>=\n",
    "print(\"Store Allocation Starting on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Last Saturday: \"+lastdate) #<=\n",
    "\n",
    "def recursive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q3/output_'+str(datetime.datetime.now().date())+'/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-07-20 10:45:10.762833\n",
      "2 2019-07-20 10:46:58.529379\n",
      "3 2019-07-20 10:48:44.412085\n",
      "4 2019-07-20 10:50:50.697999\n",
      "5 2019-07-20 10:52:51.187061\n",
      "6 2019-07-20 10:54:58.927663\n",
      "7 2019-07-20 10:57:05.110811\n",
      "8 2019-07-20 10:59:11.551565\n",
      "9 2019-07-20 11:01:02.761387\n",
      "10 2019-07-20 11:03:06.716849\n",
      "11 2019-07-20 11:03:55.548122\n",
      "12 2019-07-20 11:09:48.039840\n",
      "13 2019-07-20 11:11:12.485388\n",
      "14 2019-07-20 11:13:26.476073\n",
      "15 2019-07-20 11:14:51.690950\n",
      "Earliest Date:2016-06-26\n",
      "Latest Date:2018-09-22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize_num = 10**7\n",
    "filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "dftrans_before_20180922=pd.DataFrame()\n",
    "count_i=0\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str,usecols=['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_amt'],nrows=samplerows): #Add back the transaction info,,\n",
    "    chunk['total_transaction_amt']=chunk['total_transaction_amt'].astype(float)\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    \n",
    "    dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "    count_i+=1\n",
    "    print(count_i,datetime.datetime.now())\n",
    "\n",
    "\n",
    "del chunk\n",
    "print(\"Earliest Date:\" + str(dftrans_before_20180922['transaction_date'].min()))\n",
    "print(\"Latest Date:\" + str(dftrans_before_20180922['transaction_date'].max()))\n",
    "dftrans_before_20180922=dftrans_before_20180922.drop_duplicates()\n",
    "\n",
    "logging.info(\"Deduped: \"+str(datetime.datetime.now()))\n",
    "del dftrans_before_20180922['transaction_time']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2018-09-29\n",
      "Max_Date: 2019-02-09\n"
     ]
    }
   ],
   "source": [
    "# Up to 2019-06-29\n",
    "# All item level data, weekly and the 1-time transfered historical data\n",
    "historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "historical_daily_data_list=list(recursive_file_gen(historical_daily_data_folder))\n",
    "historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1])\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y%m%d\").date())\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']<=datetime.date(2019,2,12)]\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']>datetime.date(2018,9,22)]\n",
    "print(historical_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(historical_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(historical_daily_df['week_end_dt'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_daily_df['file_path'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2019-02-16\n",
      "Max_Date: 2019-06-29\n"
     ]
    }
   ],
   "source": [
    "new_daily_data_folder=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "new_daily_data_list=list(recursive_file_gen(new_daily_data_folder))\n",
    "new_daily_data_list=[x for x in new_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "new_daily_data_list=[x for x in new_daily_data_list if \"hist\" not in x]\n",
    "\n",
    "new_daily_df=pd.DataFrame({\"file_path\":new_daily_data_list})\n",
    "\n",
    "new_daily_df['week_end_dt']=new_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2019_by_weeks/MediaStorm_\")[1][:10])\n",
    "new_daily_df['week_end_dt']=new_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']<=lastdate_date]\n",
    "print(new_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(new_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(new_daily_df['week_end_dt'].max()))\n",
    "\n",
    "daily_df_file_after_20180922=historical_daily_df.append(new_daily_df)\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 40\n",
      "1 done 2019-07-20 11:23:45.766165\n",
      "2 done 2019-07-20 11:24:33.000708\n",
      "3 done 2019-07-20 11:25:09.251107\n",
      "4 done 2019-07-20 11:25:46.190610\n",
      "5 done 2019-07-20 11:26:26.089717\n",
      "6 done 2019-07-20 11:27:07.750756\n",
      "7 done 2019-07-20 11:27:52.804896\n",
      "8 done 2019-07-20 11:28:44.392066\n",
      "9 done 2019-07-20 11:29:58.328503\n",
      "10 done 2019-07-20 11:31:43.524216\n",
      "11 done 2019-07-20 11:33:55.663935\n",
      "12 done 2019-07-20 11:37:32.892162\n",
      "13 done 2019-07-20 11:39:33.001181\n",
      "14 done 2019-07-20 11:40:41.064506\n",
      "15 done 2019-07-20 11:41:30.255223\n",
      "16 done 2019-07-20 11:42:19.054430\n",
      "17 done 2019-07-20 11:43:17.140180\n",
      "18 done 2019-07-20 11:44:13.781181\n",
      "19 done 2019-07-20 11:45:02.113582\n",
      "20 done 2019-07-20 11:45:51.175755\n",
      "21 done 2019-07-20 11:46:42.521128\n",
      "22 done 2019-07-20 11:47:34.138530\n",
      "23 done 2019-07-20 11:48:32.110443\n",
      "24 done 2019-07-20 11:49:29.636375\n",
      "25 done 2019-07-20 11:50:26.147599\n",
      "26 done 2019-07-20 11:51:27.048758\n",
      "27 done 2019-07-20 11:52:27.295986\n",
      "28 done 2019-07-20 11:53:45.164824\n",
      "29 done 2019-07-20 11:55:11.060653\n",
      "30 done 2019-07-20 11:56:13.949355\n",
      "31 done 2019-07-20 11:57:09.210473\n",
      "32 done 2019-07-20 11:58:09.494224\n",
      "33 done 2019-07-20 11:59:05.881146\n",
      "34 done 2019-07-20 12:00:00.305738\n",
      "35 done 2019-07-20 12:01:01.580624\n",
      "36 done 2019-07-20 12:02:04.583674\n",
      "37 done 2019-07-20 12:03:07.942710\n",
      "38 done 2019-07-20 12:04:13.633632\n",
      "39 done 2019-07-20 12:05:16.339381\n",
      "40 done 2019-07-20 12:06:18.463052\n"
     ]
    }
   ],
   "source": [
    "combined_rewards_transaction_after_20180922_agg=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.drop_duplicates()\n",
    "\n",
    "    \n",
    "    combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "gc.collect()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"transaction_dt\":\"transaction_date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.groupby(['customer_id_hashed','transaction_date','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"item_transaction_amt\":\"total_transaction_amt\"})\n",
    "\n",
    "all_rewards_since_201606=dftrans_before_20180922.append(combined_rewards_transaction_after_20180922_agg)\n",
    "\n",
    "del dftrans_before_20180922\n",
    "del combined_rewards_transaction_after_20180922_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing takes ~ 1 Hour, hashed to save time\n",
    "\n",
    "# all_rewards_since_201606.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q3/BL_Rewards_Transactions_20160626_to_20190629.csv\",index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_rewards_since_201606.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q3/BL_Rewards_Transactions_20160626_to_20190629.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28365769, 4)\n",
      "(20821581, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the store for an id\n",
    "\n",
    "frequently_visit_stores_18_months=all_rewards_since_201606[all_rewards_since_201606['transaction_date']>=Beginning_18_months_ago]\n",
    "\n",
    "frequently_visit_stores_2=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['total_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "\n",
    "frequently_visit_stores_18_months=pd.merge(frequently_visit_stores_18_months,frequently_visit_stores_2,on=['customer_id_hashed','location_id'],how=\"outer\")\n",
    "del frequently_visit_stores_2\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.sort_values(['customer_id_hashed','trans','sales'],ascending=[True,False,False])\n",
    "\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months[['customer_id_hashed','location_id']].drop_duplicates(\"customer_id_hashed\")\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months.to_csv(folder_write+\"frequently_visit_stores_18_months.csv\",index=False)\n",
    "del frequently_visit_stores_18_months\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_rewards_since_201606['transaction_id']\n",
    "del all_rewards_since_201606['location_id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_rewards_since_201606.shape (201292675, 3)\n",
      "all_rewards_since_201606['customer_id_hashed'].nunique() 27232239\n",
      "all_rewards_since_201606['transaction_date'].nunique() 1097\n",
      "all_rewards_since_201606['transaction_date'].min() 2016-06-26\n",
      "all_rewards_since_201606['transaction_date'].max() 2019-06-29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>total_transaction_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc45d2747297addd8bcad579da77756b007d27cd634c71...</td>\n",
       "      <td>2018-03-20</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f627e56e080cb699864d9c488345aee82c98fa66ebc2f2...</td>\n",
       "      <td>2018-03-10</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed transaction_date  \\\n",
       "0  bc45d2747297addd8bcad579da77756b007d27cd634c71...       2018-03-20   \n",
       "1  f627e56e080cb699864d9c488345aee82c98fa66ebc2f2...       2018-03-10   \n",
       "\n",
       "   total_transaction_amt  \n",
       "0                  22.25  \n",
       "1                  16.00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data here\n",
    "print(\"all_rewards_since_201606.shape\",all_rewards_since_201606.shape)\n",
    "print(\"all_rewards_since_201606['customer_id_hashed'].nunique()\",all_rewards_since_201606['customer_id_hashed'].nunique())\n",
    "print(\"all_rewards_since_201606['transaction_date'].nunique()\",all_rewards_since_201606['transaction_date'].nunique())\n",
    "print(\"all_rewards_since_201606['transaction_date'].min()\",all_rewards_since_201606['transaction_date'].min())\n",
    "print(\"all_rewards_since_201606['transaction_date'].max()\",all_rewards_since_201606['transaction_date'].max())\n",
    "all_rewards_since_201606.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098 days, 0:00:00\n",
      "1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2017-12-31'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lastdate_date-datetime.date(2016,6,26))\n",
    "print(365*4)\n",
    "Beginning_18_months_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_1 2019-07-20 13:52:42.787281\n",
      "check_point_2 2019-07-20 13:53:11.252405\n",
      "check_point_3 2019-07-20 13:53:24.697257\n",
      "check_point_4 2019-07-20 14:11:33.059977\n"
     ]
    }
   ],
   "source": [
    "# Within 18 months only for the dataframe all_rewards_since_201606\n",
    "df_18_plus_ids=all_rewards_since_201606[all_rewards_since_201606['transaction_date']<Beginning_18_months_ago]\n",
    "print(\"check_point_1 \"+str(datetime.datetime.now()))\n",
    "all_rewards_since_201606=all_rewards_since_201606[all_rewards_since_201606['transaction_date']>=Beginning_18_months_ago]\n",
    "print(\"check_point_2 \"+str(datetime.datetime.now()))\n",
    "gc.collect()\n",
    "print(\"check_point_3 \"+str(datetime.datetime.now()))\n",
    "\n",
    "df_18_plus_ids=df_18_plus_ids[~df_18_plus_ids['customer_id_hashed'].isin(all_rewards_since_201606['customer_id_hashed'].unique().tolist())]\n",
    "df_18_plus_ids=df_18_plus_ids[['customer_id_hashed']].drop_duplicates()\n",
    "print(\"check_point_4 \"+str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-31\n",
      "2019-06-29\n"
     ]
    }
   ],
   "source": [
    "###get recency\n",
    "dfrecency=all_rewards_since_201606[['customer_id_hashed','transaction_date']].sort_values(\"transaction_date\",ascending=False).drop_duplicates()#Allready combined\n",
    "\n",
    "print (min(dfrecency['transaction_date']))\n",
    "print (max(dfrecency['transaction_date']))\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20821581, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrecency['transaction_date'] = pd.to_datetime(dfrecency['transaction_date'])\n",
    "dfrecency['recency'] =  lastdate_date - dfrecency['transaction_date']\n",
    "dfrecency['recency'] = dfrecency['recency'].apply(lambda x:x.days)\n",
    "dfrecency['recency'] = np.ceil((dfrecency['recency']+1)/30)\n",
    "\n",
    "dfrecency = dfrecency[['customer_id_hashed','recency']]\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency2.csv',index = False)\n",
    "\n",
    "dfrecency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606['transactions'] = 1\n",
    "dftotal = all_rewards_since_201606[['customer_id_hashed','total_transaction_amt','transactions']].groupby(['customer_id_hashed']).sum().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency,on = 'customer_id_hashed',how='outer')\n",
    "del dfrecency\n",
    "del all_rewards_since_201606\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal = dftotal.sort_values(['transactions','recency','sales'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Transindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['sales','recency','transactions'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Amtindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['recency','transactions','sales'],ascending = [1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'recencyindex'})\n",
    "\n",
    "c_ids = len(dftotal.index)\n",
    "logging.info('total customers from transaction and amt: ')\n",
    "logging.info(c_ids)\n",
    "c_ids = np.ceil(c_ids/5.0)\n",
    "\n",
    "dftotal['Transindex'] = np.ceil((dftotal['Transindex']+1)/c_ids)\n",
    "dftotal['Amtindex'] = np.ceil((dftotal['Amtindex']+1)/c_ids)\n",
    "dftotal['recencyindex'] = np.ceil((dftotal['recencyindex']+1)/c_ids)\n",
    "\n",
    "dftotal['RFM'] = dftotal['recencyindex']*100 + dftotal['Transindex']*10 + dftotal['Amtindex']\n",
    "\n",
    "\n",
    "dftotal = dftotal[['customer_id_hashed','RFM','recency','transactions','sales']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfiddetail.shape (28200021, 3)\n"
     ]
    }
   ],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "###\n",
    "master_file_2018Q1_2019Q1=pd.read_table(\"/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/All Rewards Members 2018-02-04 - 2019-05-04.zip\",\n",
    "                                             compression=\"zip\",dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail=master_file_2018Q1_2019Q1.append(dfiddetail).drop_duplicates(\"customer_id_hashed\")\n",
    "del master_file_2018Q1_2019Q1\n",
    "###\n",
    "master_file_gap=pd.read_table(\"/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/MediaStorm Rewards Master P4 2019 - no transaction info.zip\",\n",
    "                                             compression=\"zip\",dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail=master_file_gap.append(dfiddetail).drop_duplicates(\"customer_id_hashed\")\n",
    "del master_file_gap\n",
    "###\n",
    "\n",
    "all_master_files_after_201906=list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "all_master_files_after_201906=[x for x in all_master_files_after_201906 if (\"aster\" in x) & (\".txt\" in x)]\n",
    "all_master_files_after_201906=[x for x in all_master_files_after_201906 if x.split(\"ts/2019_by_weeks/MediaStorm_\")[1][:10]>\"2019-06-01\"]\n",
    "df_master_all_later_after_201906=pd.DataFrame()\n",
    "for file in all_master_files_after_201906:\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "    df_master_all_later_after_201906=df_master_all_later_after_201906.append(df)\n",
    "\n",
    "dfiddetail=df_master_all_later_after_201906.append(dfiddetail).drop_duplicates(\"customer_id_hashed\")\n",
    "del df_master_all_later_after_201906\n",
    "\n",
    "dfiddetail = dfiddetail.drop_duplicates('email_address_hash')\n",
    "\n",
    "#dropped both email & id duplicated\n",
    "###\n",
    "gc.collect()\n",
    "print(\"dfiddetail.shape\",dfiddetail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:45:54.025712\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active', 'lapsed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrecency = pd.read_csv(folder_write + 'dfrecency.csv')\n",
    "dfrecency['active'] = np.where(dfrecency['transaction_date']>=active_Sunday,'active',\n",
    "                               np.where(dfrecency['transaction_date']>=lapsed_Sunday,'lapsed','other')\n",
    "                              )\n",
    "\n",
    "print(dfrecency['active'].unique().tolist())\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency[['customer_id_hashed','active']],on = 'customer_id_hashed')\n",
    "\n",
    "logging.info(str(dfrecency['active'].unique().tolist()))\n",
    "del dfrecency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28200021\n",
      "totalids_trans: 20821581\n",
      "totalids_trans_mergewithmaster: 18936098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].astype('str')\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].str[0:5]\n",
    "dfiddetail['customer_zip_code'].fillna('00000',inplace = True)\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].apply(lambda x:x.zfill(5))\n",
    "print(len(dfiddetail.index))\n",
    "\n",
    "\n",
    "print(\"totalids_trans:\",len(dftotal.index))\n",
    "dftotal = pd.merge(dftotal,dfiddetail,on = 'customer_id_hashed')\n",
    "print(\"totalids_trans_mergewithmaster:\",len(dftotal.index))\n",
    "\n",
    "del dfiddetail['customer_zip_code']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1873409, 3)\n",
      "(1873954, 3)\n",
      "(1873409, 3)\n",
      "(1352150, 3)\n"
     ]
    }
   ],
   "source": [
    "lapsed_trans=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed'],dtype=str).drop_duplicates() # Doesn't go to score at all, so no need to read all columns\n",
    "\n",
    "lapsed_master=pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "\n",
    "lapsed_master=lapsed_master.drop_duplicates(\"customer_id_hashed\")\n",
    "print(lapsed_master.shape)\n",
    "\n",
    "lapsed_master=pd.merge(lapsed_master,lapsed_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "print(lapsed_master.shape)\n",
    "lapsed_master=lapsed_master[~pd.isnull(lapsed_master['email_address_hash'])]\n",
    "print(lapsed_master.shape)\n",
    "\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(dftotal['customer_id_hashed'].tolist())]\n",
    "print(lapsed_master.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_18_plus_ids=pd.merge(df_18_plus_ids,dfiddetail,on = 'customer_id_hashed')\n",
    "\n",
    "df_18_plus_ids=df_18_plus_ids.append(lapsed_master)\n",
    "df_18_plus_ids=df_18_plus_ids.drop_duplicates(\"customer_id_hashed\").drop_duplicates(\"email_address_hash\")\n",
    "df_18_plus_ids.to_csv(folder_write+\"df_ids_email_18_plus_shoppers.csv\",index=False)\n",
    "\n",
    "del dfiddetail\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the RFM scoring, 2019-07-20 14:59:53.383055\n"
     ]
    }
   ],
   "source": [
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "print(\"Done with the RFM scoring, \"+str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the primary stores for each member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequently_visit_stores_18_months=pd.read_csv(folder_write+\"frequently_visit_stores_18_months.csv\",dtype=str)\n",
    "register_stores=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/output_sing_up_location/BL_id_by_register_store_JL_2019-07-12.csv\",\n",
    "                            dtype=str,nrows=samplerows,usecols=['customer_id_hashed','sign_up_location'])\n",
    "register_stores=register_stores.rename(columns={\"sign_up_location\":\"location_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30668013, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_for_ids=frequently_visit_stores_18_months.append(register_stores)\n",
    "store_for_ids=store_for_ids.drop_duplicates(\"customer_id_hashed\")\n",
    "store_for_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del frequently_visit_stores_18_months\n",
    "del register_stores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal=pd.merge(dftotal,store_for_ids,on=\"customer_id_hashed\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del store_for_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1355, 2)\n",
      "1355\n"
     ]
    }
   ],
   "source": [
    "# read the quadrant by store for 2019 Q1\n",
    "\n",
    "Q1_2019_store_quadrant=pd.read_excel(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q3/Excel_BL_2019_Q1_post_quadrants_JL_2019-05-15.xlsx\",\n",
    "                                dtype=str,sheetname=\"2019_Q1_Store_Quad_Defination\",usecols=['location_id','Quadrant'])\n",
    "print(Q1_2019_store_quadrant.shape)\n",
    "print(len(Q1_2019_store_quadrant['location_id'].unique()))\n",
    "\n",
    "dftotal=pd.merge(dftotal,Q1_2019_store_quadrant,on=\"location_id\",how=\"left\")\n",
    "dftotal['Quadrant']=dftotal['Quadrant'].fillna(\"Quadrant III\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "dftotal['customer_zip_code']=dftotal['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x:\"D\"+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zip_PST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipmap=pd.read_excel('/home/jian/Projects/Big_Lots/New_TA/TA_created_in_201906/final_output_20190718/BL_final_TA_updated_JL_2019-07-18.xlsx',\n",
    "                     dtype = 'str',sheetname=\"unique_zips_full_footprint\")\n",
    "zipmap=zipmap.rename(columns={\"zip_type\":\"zipcodegroup\",\"zip_cd\":\"customer_zip_code\"})\n",
    "\n",
    "zipmap['zipcodegroup']=zipmap['zipcodegroup'].replace(\"trans_P\",\"P\")\n",
    "zipmap['zipcodegroup']=zipmap['zipcodegroup'].replace(\"trans_S\",\"S\")\n",
    "zipmap['zipcodegroup']=zipmap['zipcodegroup'].replace(\"zips_10\",\"S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P' nan 'S']\n",
      "['P' 'T' 'S']\n",
      "Final wemailcsv: (18936098, 12)\n"
     ]
    }
   ],
   "source": [
    "dftotal = pd.merge(dftotal,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "dftotal['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_wemail.csv',index = False)\n",
    "print(\"Final wemailcsv:\",dftotal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18936098, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38377"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_H=pd.DataFrame({\"frmindex\":['D01','D02','D03','D04']})\n",
    "df_H['HML_Group']=\"H\"\n",
    "\n",
    "df_M=pd.DataFrame({\"frmindex\":['D05','D06','D07']})\n",
    "df_M['HML_Group']=\"M\"\n",
    "\n",
    "df_L=pd.DataFrame({\"frmindex\":['D08','D09','D10']})\n",
    "df_L['HML_Group']=\"L\"\n",
    "\n",
    "df_HML=df_H.append(df_M).append(df_L)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal=pd.merge(dftotal,df_HML,on='frmindex',how=\"left\")\n",
    "\n",
    "\n",
    "dftotal['segment_Decile']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['frmindex']+\"_2019Q3\"\n",
    "dftotal['segment_2019Q3']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['HML_Group']+\"_2019Q3\"\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_final_details_wemail_ziplabel_StoreQuad.csv',index = False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
