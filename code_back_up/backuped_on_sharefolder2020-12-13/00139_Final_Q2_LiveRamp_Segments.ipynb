{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapsed Start on: 2017-10-01\n",
      "Active Start on: 2018-04-01\n",
      "Store Allocation Starting on: 2017-10-01\n",
      "Last Saturday: 2019-03-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V5\n",
    "# upto 20190330\n",
    "# merge email first\n",
    "# then calculate decile\n",
    "# Remove 18+\n",
    "# Added the P,L,18+, no quadrants\n",
    "\n",
    "# In other words: all the same, except for the time range\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='BL_CRM_LR_Quarterly_upto20190330_Final_20190422.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "\n",
    "lastdate = datetime.date(2019,3,30) # Recent Saturday\n",
    "active_Sunday = str(lastdate-datetime.timedelta(days=52*7-1))\n",
    "# active_Sunday = \"2017-12-29\"\n",
    "lapsed_Sunday = str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "# lapsed_Sunday = \"2017-06-29\"\n",
    "Beginning_18_months_ago=str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "# Beginning_18_months_ago = \"2017-06-29\"\n",
    "\n",
    "lastdate=str(lastdate)\n",
    "print(\"Lapsed Start on: \"+lapsed_Sunday) #>=\n",
    "print(\"Active Start on: \"+active_Sunday) #>=\n",
    "print(\"Store Allocation Starting on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Last Saturday: \"+lastdate) #<=\n",
    "\n",
    "def recrusive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/output_final_20190422/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.166666666666668"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datetime.date(2019,3,30)-datetime.date(2017,10,1)).days/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.56666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datetime.date(2019,3,30)-datetime.date(2016,6,26)).days/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-04-23 00:06:51.236255\n",
      "Earliest Date:2018-01-09\n",
      "Latest Date:2018-03-31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize_num = 10**7\n",
    "filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "dftrans_before_20180922=pd.DataFrame()\n",
    "count_i=0\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str,usecols=['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_amt'],nrows=samplerows): #Add back the transaction info,,\n",
    "    chunk['total_transaction_amt']=chunk['total_transaction_amt'].astype(float)\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    \n",
    "    dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "    count_i+=1\n",
    "    print(count_i,datetime.datetime.now())\n",
    "\n",
    "\n",
    "del chunk\n",
    "print(\"Earliest Date:\" + str(dftrans_before_20180922['transaction_date'].min()))\n",
    "print(\"Latest Date:\" + str(dftrans_before_20180922['transaction_date'].max()))\n",
    "dftrans_before_20180922=dftrans_before_20180922.drop_duplicates()\n",
    "\n",
    "logging.info(\"Deduped: \"+str(datetime.datetime.now()))\n",
    "del dftrans_before_20180922['transaction_time']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2018-09-29\n",
      "Max_Date: 2019-02-09\n"
     ]
    }
   ],
   "source": [
    "# Up to 2019-03-30\n",
    "# All item level data, weekly and the 1-time transfered historical data\n",
    "historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "historical_daily_data_list=list(recrusive_file_gen(historical_daily_data_folder))\n",
    "historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1])\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y%m%d\").date())\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']<=datetime.date(2019,2,12)]\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']>datetime.date(2018,9,22)]\n",
    "print(historical_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(historical_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(historical_daily_df['week_end_dt'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n",
      "Min_Date: 2019-02-16\n",
      "Max_Date: 2019-03-30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_daily_data_folder=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "new_daily_data_list=list(recrusive_file_gen(new_daily_data_folder))\n",
    "new_daily_data_list=[x for x in new_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "new_daily_data_list=[x for x in new_daily_data_list if \"hist\" not in x]\n",
    "\n",
    "new_daily_df=pd.DataFrame({\"file_path\":new_daily_data_list})\n",
    "\n",
    "new_daily_df['week_end_dt']=new_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2019_by_weeks/MediaStorm_\")[1][:10])\n",
    "new_daily_df['week_end_dt']=new_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']<=datetime.date(2019,3,30)]\n",
    "print(new_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(new_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(new_daily_df['week_end_dt'].max()))\n",
    "\n",
    "daily_df_file_after_20180922=historical_daily_df.append(new_daily_df)\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 27\n",
      "1 done 2019-04-23 00:06:51.924776\n",
      "2 done 2019-04-23 00:06:52.383171\n",
      "3 done 2019-04-23 00:06:52.784031\n",
      "4 done 2019-04-23 00:06:53.212637\n",
      "5 done 2019-04-23 00:06:57.500925\n",
      "6 done 2019-04-23 00:07:00.087684\n",
      "7 done 2019-04-23 00:07:00.612664\n",
      "8 done 2019-04-23 00:07:02.907821\n",
      "9 done 2019-04-23 00:07:03.852511\n",
      "10 done 2019-04-23 00:07:07.016840\n",
      "11 done 2019-04-23 00:07:07.494110\n",
      "12 done 2019-04-23 00:07:08.895941\n",
      "13 done 2019-04-23 00:07:11.031276\n",
      "14 done 2019-04-23 00:07:11.505329\n",
      "15 done 2019-04-23 00:07:13.161954\n",
      "16 done 2019-04-23 00:07:14.145912\n",
      "17 done 2019-04-23 00:07:16.391694\n",
      "18 done 2019-04-23 00:07:17.829914\n",
      "19 done 2019-04-23 00:07:18.825561\n",
      "20 done 2019-04-23 00:07:19.935040\n",
      "21 done 2019-04-23 00:07:21.331683\n",
      "22 done 2019-04-23 00:07:22.268257\n",
      "23 done 2019-04-23 00:07:23.734525\n",
      "24 done 2019-04-23 00:07:24.870905\n",
      "25 done 2019-04-23 00:07:26.726858\n",
      "26 done 2019-04-23 00:07:28.360196\n",
      "27 done 2019-04-23 00:07:30.582634\n"
     ]
    }
   ],
   "source": [
    "combined_rewards_transaction_after_20180922_agg=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.drop_duplicates()\n",
    "\n",
    "    \n",
    "    combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "gc.collect()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"transaction_dt\":\"transaction_date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.groupby(['customer_id_hashed','transaction_date','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"item_transaction_amt\":\"total_transaction_amt\"})\n",
    "\n",
    "all_rewards_since_201606=dftrans_before_20180922.append(combined_rewards_transaction_after_20180922_agg)\n",
    "\n",
    "del dftrans_before_20180922\n",
    "del combined_rewards_transaction_after_20180922_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/BL_Rewards_Transactions_20160626_to_20190330.csv\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1390733, 4)\n",
      "(1363578, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the store for an id\n",
    "\n",
    "frequently_visit_stores_18_months=all_rewards_since_201606[all_rewards_since_201606['transaction_date']>=Beginning_18_months_ago]\n",
    "\n",
    "frequently_visit_stores_2=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['total_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "\n",
    "frequently_visit_stores_18_months=pd.merge(frequently_visit_stores_18_months,frequently_visit_stores_2,on=['customer_id_hashed','location_id'],how=\"outer\")\n",
    "del frequently_visit_stores_2\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.sort_values(['customer_id_hashed','trans','sales'],ascending=[True,False,False])\n",
    "\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months[['customer_id_hashed','location_id']].drop_duplicates(\"customer_id_hashed\")\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months.to_csv(folder_write+\"frequently_visit_stores_18_months.csv\",index=False)\n",
    "del frequently_visit_stores_18_months\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_rewards_since_201606['transaction_id']\n",
    "del all_rewards_since_201606['location_id']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-09\n",
      "2019-03-30\n"
     ]
    }
   ],
   "source": [
    "###get recency\n",
    "dfrecency=all_rewards_since_201606[['customer_id_hashed','transaction_date']].sort_values(\"transaction_date\",ascending=False).drop_duplicates()#Allready combined\n",
    "\n",
    "print (min(dfrecency['transaction_date']))\n",
    "print (max(dfrecency['transaction_date']))\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1363578, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrecency['transaction_date'] = pd.to_datetime(dfrecency['transaction_date'])\n",
    "dfrecency['recency'] =  datetime.datetime.strptime(str(lastdate), '%Y-%m-%d').date() - dfrecency['transaction_date']\n",
    "dfrecency['recency'] = dfrecency['recency'].apply(lambda x:x.days)\n",
    "dfrecency['recency'] = np.ceil((dfrecency['recency']+1)/30)\n",
    "\n",
    "dfrecency = dfrecency[['customer_id_hashed','recency']]\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency2.csv',index = False)\n",
    "\n",
    "dfrecency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606['transactions'] = 1\n",
    "dftotal = all_rewards_since_201606[['customer_id_hashed','total_transaction_amt','transactions']].groupby(['customer_id_hashed']).sum().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency,on = 'customer_id_hashed',how='outer')\n",
    "del dfrecency\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal = dftotal.sort_values(['transactions','recency','sales'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Transindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['sales','recency','transactions'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Amtindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['recency','transactions','sales'],ascending = [1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'recencyindex'})\n",
    "\n",
    "c_ids = len(dftotal.index)\n",
    "logging.info('total customers from transaction and amt: ')\n",
    "logging.info(c_ids)\n",
    "c_ids = np.ceil(c_ids/5.0)\n",
    "\n",
    "dftotal['Transindex'] = np.ceil((dftotal['Transindex']+1)/c_ids)\n",
    "dftotal['Amtindex'] = np.ceil((dftotal['Amtindex']+1)/c_ids)\n",
    "dftotal['recencyindex'] = np.ceil((dftotal['recencyindex']+1)/c_ids)\n",
    "\n",
    "dftotal['RFM'] = dftotal['recencyindex']*100 + dftotal['Transindex']*10 + dftotal['Amtindex']\n",
    "'''\n",
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm.csv',index = False)\n",
    "\n",
    "dftotal = pd.read_csv(folder_write + 'dfrfm.csv')\n",
    "'''\n",
    "dftotal = dftotal[['customer_id_hashed','RFM','recency','transactions','sales']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 00:09:50.657401 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-12/MediaStormMasterBiWeekly20190115-132855-055.txt\n",
      "2019-04-23 00:09:51.051446 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-26/MediaStormMasterBiWeekly20190129-130902-016.txt\n",
      "2019-04-23 00:09:51.369555 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-02/MediaStormMasterWeekly20190205-111610-675.txt\n",
      "2019-04-23 00:09:51.710620 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-09/MediaStormMasterWeekly20190212-122428-267.txt\n",
      "2019-04-23 00:09:52.069931 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-16/MediaStormMasterWeekly20190219-113650-867.txt\n",
      "2019-04-23 00:09:52.458077 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-23/MediaStormMasterWeekly20190226-112921-061.txt\n",
      "2019-04-23 00:09:52.955605 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-02/MediaStormMasterWeekly20190305-112945-302.txt\n",
      "2019-04-23 00:09:53.334140 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-09/MediaStormMasterWeekly20190312-121512-232.txt\n",
      "2019-04-23 00:09:53.710383 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-16/MediaStormMasterWeekly20190319-112932-415.txt\n",
      "2019-04-23 00:09:54.105292 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-23/MediaStormMasterWeekly20190326-113052-887.txt\n",
      "2019-04-23 00:09:54.520753 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-30/MediaStormMasterWeekly20190402-113131-172.txt\n"
     ]
    }
   ],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "#########\n",
    "\n",
    "new_sign_ups_2019_list=list(recrusive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "new_sign_ups_2019_list=sorted([x for x in new_sign_ups_2019_list if \"ster\" in x])\n",
    "\n",
    "new_sign_ups_2019_df=pd.DataFrame({\"file_path\":new_sign_ups_2019_list})\n",
    "new_sign_ups_2019_df['Date']=new_sign_ups_2019_df['file_path'].apply(lambda x: x.split(\"MediaStorm_\")[1][:10])\n",
    "new_sign_ups_2019_df=new_sign_ups_2019_df[new_sign_ups_2019_df['Date']<=lastdate]\n",
    "\n",
    "for file_new_signups in new_sign_ups_2019_df['file_path'].tolist():\n",
    "    df=pd.read_table(file_new_signups,dtype=str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'],sep=\"|\",nrows=samplerows)\n",
    "    dfiddetail=df.append(dfiddetail) # Already sorted and newest kept on the top\n",
    "    print(datetime.datetime.now(),file_new_signups)\n",
    "dfiddetail=dfiddetail.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "######\n",
    "dfiddetail2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail = dfiddetail.append(dfiddetail2,ignore_index = True)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "dfiddetail = dfiddetail.drop_duplicates('email_address_hash')\n",
    "\n",
    "del dfiddetail2\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "logging.info(\"CheckingPoint2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active', 'lapsed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfrecency = pd.read_csv(folder_write + 'dfrecency.csv')\n",
    "dfrecency['active'] = np.where(dfrecency['transaction_date']>=active_Sunday,'active',\n",
    "                               np.where(dfrecency['transaction_date']>=lapsed_Sunday,'lapsed','other')\n",
    "                              )\n",
    "\n",
    "print(dfrecency['active'].unique().tolist())\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency[['customer_id_hashed','active']],on = 'customer_id_hashed')\n",
    "\n",
    "logging.info(str(dfrecency['active'].unique().tolist()))\n",
    "del dfrecency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"CheckingPoint1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122067\n",
      "totalids_trans: 1363578\n",
      "totalids_trans_mergewithmaster: 61553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].astype('str')\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].str[0:5]\n",
    "dfiddetail['customer_zip_code'].fillna('00000',inplace = True)\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].apply(lambda x:x.zfill(5))\n",
    "print(len(dfiddetail.index))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "print(\"totalids_trans:\",len(dftotal.index))\n",
    "dftotal = pd.merge(dftotal,dfiddetail,on = 'customer_id_hashed')\n",
    "print(\"totalids_trans_mergewithmaster:\",len(dftotal.index))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "del dfiddetail\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P' 'S' nan 'T']\n",
      "['P' 'S' 'T']\n",
      "Final wemailcsv: (61553, 10)\n"
     ]
    }
   ],
   "source": [
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "\n",
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "dftotal = pd.merge(dftotal,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "dftotal['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_wemail.csv',index = False)\n",
    "print(\"Final wemailcsv:\",dftotal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_P_L_18_plus rest: (0, 10)\n",
      "df_other_18_plus all: (0, 11)\n",
      "df_other_18_plus rest: (0, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra segment for P|L|18+\n",
    "# Remove the ids don't have transactions within 18 months\n",
    "seg_P_L_18_plus=dftotal[(dftotal['frmindex']>7.5) & (dftotal['zipcodegroup']==\"P\") & (dftotal['active']==\"other\")]\n",
    "print(\"seg_P_L_18_plus rest:\", seg_P_L_18_plus.shape)\n",
    "\n",
    "df_other_18_plus=dftotal[dftotal['active']==\"other\"]\n",
    "df_other_18_plus['segment']=\"18_months_plus_back_201606\"\n",
    "print(\"df_other_18_plus all:\", df_other_18_plus.shape)\n",
    "\n",
    "dftotal=dftotal[dftotal['active']!=\"other\"]\n",
    "\n",
    "df_other_18_plus=df_other_18_plus[(df_other_18_plus['frmindex']<7.5) | (df_other_18_plus['zipcodegroup']!=\"P\") | (df_other_18_plus['active']!=\"other\")]\n",
    "print(\"df_other_18_plus rest:\", df_other_18_plus.shape)\n",
    "\n",
    "# dftotal=dftotal.append(seg_P_L_18_plus) treat the new sperately\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_other_18_plus.to_csv(folder_write+\"others_not_new_seg_PLother.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the primary stores for each member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequently_visit_stores_18_months=pd.read_csv(folder_write+\"frequently_visit_stores_18_months.csv\",dtype=str)\n",
    "register_stores=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/output_sing_up_location/BL_id_by_register_store_JL_2019-04-09.csv\",\n",
    "                            dtype=str,nrows=samplerows)\n",
    "register_stores=register_stores[['customer_id_hashed','sign_up_location']].rename(columns={\"sign_up_location\":\"location_id\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459564, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_for_ids=frequently_visit_stores_18_months.append(register_stores)\n",
    "store_for_ids=store_for_ids.drop_duplicates(\"customer_id_hashed\")\n",
    "store_for_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del frequently_visit_stores_18_months\n",
    "del register_stores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal=pd.merge(dftotal,store_for_ids,on=\"customer_id_hashed\",how=\"left\")\n",
    "seg_P_L_18_plus=pd.merge(seg_P_L_18_plus,store_for_ids,on=\"customer_id_hashed\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1361, 2)\n",
      "1361\n"
     ]
    }
   ],
   "source": [
    "# read the quadrant by store for 2018 Q4\n",
    "\n",
    "Q4_store_quadrant=pd.read_excel(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Excel_BL_2018_Q4_post_YoY_small_JL_2019-03-04.xlsx\",\n",
    "                                dtype=str,sheetname=\"Q4_Store_Quadrant_Defination\",usecols=['location_id','Quadrant'])\n",
    "print(Q4_store_quadrant.shape)\n",
    "print(len(Q4_store_quadrant['location_id'].unique()))\n",
    "\n",
    "dftotal=pd.merge(dftotal,Q4_store_quadrant,on=\"location_id\",how=\"left\")\n",
    "dftotal['Quadrant']=dftotal['Quadrant'].fillna(\"Quadrant III\")\n",
    "\n",
    "seg_P_L_18_plus=pd.merge(seg_P_L_18_plus,Q4_store_quadrant,on=\"location_id\",how=\"left\")\n",
    "seg_P_L_18_plus['Quadrant']=seg_P_L_18_plus['Quadrant'].fillna(\"Quadrant III\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del store_for_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "dftotal['customer_zip_code']=dftotal['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x:\"D\"+x)\n",
    "\n",
    "seg_P_L_18_plus['frmindex']=seg_P_L_18_plus['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "seg_P_L_18_plus['customer_zip_code']=seg_P_L_18_plus['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "seg_P_L_18_plus['frmindex']=seg_P_L_18_plus['frmindex'].apply(lambda x:\"D\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal.to_csv(folder_write + 'dfrfm_final_details_wemail_zip_StoreQuad.csv',index = False)\n",
    "seg_P_L_18_plus.to_csv(folder_write + 'seg_P_L_18_plus_details_wemail_zip_StoreQuad.csv',index = False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Stopped below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "df_H=pd.DataFrame({\"frmindex\":['D01','D02','D03','D04']})\n",
    "df_H['HML_Group']=\"H\"\n",
    "\n",
    "df_M=pd.DataFrame({\"frmindex\":['D05','D06','D07']})\n",
    "df_M['HML_Group']=\"M\"\n",
    "\n",
    "df_L=pd.DataFrame({\"frmindex\":['D08','D09','D10']})\n",
    "df_L['HML_Group']=\"L\"\n",
    "\n",
    "df_HML=df_H.append(df_M).append(df_L)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "dftotal=pd.merge(dftotal,df_HML,on='frmindex') # both 10\n",
    "\n",
    "dftotal['segment_Decile']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['frmindex']+\"_2019Q2\"\n",
    "dftotal['segment_2019Q2']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['HML_Group']+\"_2019Q2\"\n",
    "\n",
    "\n",
    "seg_P_L_18_plus=pd.merge(seg_P_L_18_plus,df_HML,on='frmindex') # both 3\n",
    "\n",
    "seg_P_L_18_plus['segment_Decile']=seg_P_L_18_plus['Quadrant']+\"_\"+seg_P_L_18_plus['zipcodegroup']+\"_\"+seg_P_L_18_plus['frmindex']+\"_2019Q2PL18\"\n",
    "seg_P_L_18_plus['segment_2019Q2']=seg_P_L_18_plus['Quadrant']+\"_\"+seg_P_L_18_plus['zipcodegroup']+\"_\"+seg_P_L_18_plus['HML_Group']+\"_2019Q2PL18\"\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-04-23 00:10:25.109831\n",
      "2 2019-04-23 00:10:25.235331\n",
      "3 2019-04-23 00:10:25.330099\n",
      "4 2019-04-23 00:10:25.443544\n",
      "5 2019-04-23 00:10:25.580195\n",
      "6 2019-04-23 00:10:25.732865\n",
      "7 2019-04-23 00:10:25.844350\n",
      "8 2019-04-23 00:10:26.014759\n",
      "9 2019-04-23 00:10:26.131515\n",
      "10 2019-04-23 00:10:26.243511\n",
      "11 2019-04-23 00:10:26.254379\n",
      "12 2019-04-23 00:10:26.265142\n",
      "13 2019-04-23 00:10:26.275746\n",
      "14 2019-04-23 00:10:26.287161\n",
      "15 2019-04-23 00:10:26.298749\n",
      "16 2019-04-23 00:10:26.309328\n",
      "17 2019-04-23 00:10:26.319859\n",
      "18 2019-04-23 00:10:26.330659\n",
      "19 2019-04-23 00:10:26.344310\n",
      "20 2019-04-23 00:10:26.356458\n",
      "21 2019-04-23 00:10:26.367081\n",
      "22 2019-04-23 00:10:26.377515\n",
      "23 2019-04-23 00:10:26.388277\n",
      "24 2019-04-23 00:10:26.401651\n",
      "25 2019-04-23 00:10:26.418477\n",
      "26 2019-04-23 00:10:26.433058\n",
      "27 2019-04-23 00:10:26.452314\n",
      "28 2019-04-23 00:10:26.468142\n",
      "29 2019-04-23 00:10:26.481983\n",
      "30 2019-04-23 00:10:26.495673\n",
      "31 2019-04-23 00:10:26.616992\n",
      "32 2019-04-23 00:10:26.764178\n",
      "33 2019-04-23 00:10:26.928156\n",
      "34 2019-04-23 00:10:27.070153\n",
      "35 2019-04-23 00:10:27.187117\n",
      "36 2019-04-23 00:10:27.337639\n",
      "37 2019-04-23 00:10:27.449141\n",
      "38 2019-04-23 00:10:27.569887\n",
      "39 2019-04-23 00:10:27.712783\n",
      "40 2019-04-23 00:10:27.859145\n",
      "41 2019-04-23 00:10:27.876498\n",
      "42 2019-04-23 00:10:27.895480\n",
      "43 2019-04-23 00:10:27.911869\n",
      "44 2019-04-23 00:10:27.927358\n",
      "45 2019-04-23 00:10:27.941365\n",
      "46 2019-04-23 00:10:27.957540\n",
      "47 2019-04-23 00:10:27.971996\n",
      "48 2019-04-23 00:10:27.986266\n",
      "49 2019-04-23 00:10:27.999887\n",
      "50 2019-04-23 00:10:28.018893\n",
      "51 2019-04-23 00:10:28.036508\n",
      "52 2019-04-23 00:10:28.050752\n",
      "53 2019-04-23 00:10:28.067021\n",
      "54 2019-04-23 00:10:28.081480\n",
      "55 2019-04-23 00:10:28.100528\n",
      "56 2019-04-23 00:10:28.116726\n",
      "57 2019-04-23 00:10:28.130834\n",
      "58 2019-04-23 00:10:28.147707\n",
      "59 2019-04-23 00:10:28.162346\n",
      "60 2019-04-23 00:10:28.180039\n",
      "61 2019-04-23 00:10:28.336322\n",
      "62 2019-04-23 00:10:28.481551\n",
      "63 2019-04-23 00:10:28.615653\n",
      "64 2019-04-23 00:10:28.763085\n",
      "65 2019-04-23 00:10:28.939907\n",
      "66 2019-04-23 00:10:29.121173\n",
      "67 2019-04-23 00:10:29.252872\n",
      "68 2019-04-23 00:10:29.368359\n",
      "69 2019-04-23 00:10:29.528980\n",
      "70 2019-04-23 00:10:29.704464\n",
      "71 2019-04-23 00:10:29.721522\n",
      "72 2019-04-23 00:10:29.747065\n",
      "73 2019-04-23 00:10:29.768651\n",
      "74 2019-04-23 00:10:29.789876\n",
      "75 2019-04-23 00:10:29.812688\n",
      "76 2019-04-23 00:10:29.841702\n",
      "77 2019-04-23 00:10:29.861217\n",
      "78 2019-04-23 00:10:29.880930\n",
      "79 2019-04-23 00:10:29.899733\n",
      "80 2019-04-23 00:10:29.922112\n",
      "81 2019-04-23 00:10:29.940770\n",
      "82 2019-04-23 00:10:29.959200\n",
      "83 2019-04-23 00:10:29.980400\n",
      "84 2019-04-23 00:10:29.997581\n",
      "85 2019-04-23 00:10:30.014370\n",
      "86 2019-04-23 00:10:30.031688\n",
      "87 2019-04-23 00:10:30.050805\n",
      "88 2019-04-23 00:10:30.067512\n",
      "89 2019-04-23 00:10:30.084720\n",
      "90 2019-04-23 00:10:30.103527\n",
      "91 2019-04-23 00:10:30.220709\n",
      "92 2019-04-23 00:10:30.411716\n",
      "93 2019-04-23 00:10:30.556931\n",
      "94 2019-04-23 00:10:30.716679\n",
      "95 2019-04-23 00:10:30.886558\n",
      "96 2019-04-23 00:10:31.038330\n",
      "97 2019-04-23 00:10:31.190925\n",
      "98 2019-04-23 00:10:31.337404\n",
      "99 2019-04-23 00:10:31.493750\n",
      "100 2019-04-23 00:10:31.652943\n",
      "101 2019-04-23 00:10:31.688582\n",
      "102 2019-04-23 00:10:31.725782\n",
      "103 2019-04-23 00:10:31.761065\n",
      "104 2019-04-23 00:10:31.794242\n",
      "105 2019-04-23 00:10:31.827081\n",
      "106 2019-04-23 00:10:31.861747\n",
      "107 2019-04-23 00:10:31.893914\n",
      "108 2019-04-23 00:10:31.925663\n",
      "109 2019-04-23 00:10:31.959240\n",
      "110 2019-04-23 00:10:31.997453\n",
      "111 2019-04-23 00:10:32.038162\n",
      "112 2019-04-23 00:10:32.074236\n",
      "113 2019-04-23 00:10:32.108805\n",
      "114 2019-04-23 00:10:32.145408\n",
      "115 2019-04-23 00:10:32.179860\n",
      "116 2019-04-23 00:10:32.212422\n",
      "117 2019-04-23 00:10:32.245606\n",
      "118 2019-04-23 00:10:32.278166\n",
      "119 2019-04-23 00:10:32.312772\n",
      "120 2019-04-23 00:10:32.349784\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "logging.info(\"Checking Point before splitting T/C\")\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "total_rows=len(dftotal)\n",
    "\n",
    "test_all_df=pd.DataFrame()\n",
    "control_all_df=pd.DataFrame()\n",
    "\n",
    "i_counter=0\n",
    "\n",
    "dftotal=dftotal[['segment_Decile','segment_2019Q2','customer_id_hashed','email_address_hash','active']]\n",
    "\n",
    "for seg,group in dftotal.groupby(['segment_Decile']):\n",
    "    random_list=random.sample(range(len(group)), int(np.round(len(group)/total_rows*500000)))\n",
    "\n",
    "    group=group.reset_index()\n",
    "    del group['index']\n",
    "    group=group.reset_index()\n",
    "    df_control=group[group['index'].isin(random_list)]\n",
    "    df_test=group[~group['index'].isin(random_list)]\n",
    "    \n",
    "    df_control['segment_Decile']=\"C_\"+df_control['segment_Decile']\n",
    "    df_test['segment_Decile']=\"T_\"+df_test['segment_Decile']\n",
    "    \n",
    "    df_control['segment_2019Q2']=\"C_\"+df_control['segment_2019Q2']\n",
    "    df_test['segment_2019Q2']=\"T_\"+df_test['segment_2019Q2']\n",
    "    test_all_df=test_all_df.append(df_test)\n",
    "    control_all_df=control_all_df.append(df_control)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())\n",
    "    \n",
    "logging.info(\"Checking Point after splitting T/C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "del dftotal\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "logging.info(\"Checking Point before splitting T/C\")\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "total_rows=len(seg_P_L_18_plus)\n",
    "\n",
    "test_seg_P_L_18_plus=pd.DataFrame()\n",
    "control_seg_P_L_18_plus=pd.DataFrame()\n",
    "\n",
    "i_counter=0\n",
    "\n",
    "seg_P_L_18_plus=seg_P_L_18_plus[['segment_Decile','segment_2019Q2','customer_id_hashed','email_address_hash','active']]\n",
    "\n",
    "for seg,group in seg_P_L_18_plus.groupby(['segment_Decile']):\n",
    "    random_list=random.sample(range(len(group)), int(np.round(len(group)/total_rows*100000)))\n",
    "\n",
    "    group=group.reset_index()\n",
    "    del group['index']\n",
    "    group=group.reset_index()\n",
    "    df_control=group[group['index'].isin(random_list)]\n",
    "    df_test=group[~group['index'].isin(random_list)]\n",
    "    \n",
    "    df_control['segment_Decile']=\"C_\"+df_control['segment_Decile']\n",
    "    df_test['segment_Decile']=\"T_\"+df_test['segment_Decile']\n",
    "    \n",
    "    df_control['segment_2019Q2']=\"C_\"+df_control['segment_2019Q2']\n",
    "    df_test['segment_2019Q2']=\"T_\"+df_test['segment_2019Q2']\n",
    "    test_seg_P_L_18_plus=test_seg_P_L_18_plus.append(df_test)\n",
    "    control_seg_P_L_18_plus=control_seg_P_L_18_plus.append(df_control)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())\n",
    "    \n",
    "logging.info(\"Checking Point after splitting T/C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "del seg_P_L_18_plus\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/output_final_20190422/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "folder_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "test_all_df.to_csv(folder_write+\"all_test.csv\",index=False)\n",
    "control_all_df.to_csv(folder_write+\"all_control.csv\",index=False)\n",
    "\n",
    "test_seg_P_L_18_plus.to_csv(folder_write+\"test_seg_P_L_18_plus.csv\",index=False)\n",
    "control_seg_P_L_18_plus.to_csv(folder_write+\"control_seg_P_L_18_plus.csv\",index=False)\n",
    "\n",
    "folder_write_inner_decile = folder_write+'by_decile_group/'\n",
    "try:\n",
    "    os.stat(folder_write_inner_decile)\n",
    "except:\n",
    "    os.mkdir(folder_write_inner_decile)\n",
    "    \n",
    "folder_write_inner_HML = folder_write+'by_HML_group/'\n",
    "try:\n",
    "    os.stat(folder_write_inner_HML)\n",
    "except:\n",
    "    os.mkdir(folder_write_inner_HML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 T_Quadrant III_P_D01_2019Q2 2019-04-23 00:11:27.875768\n",
      "2 T_Quadrant III_P_D02_2019Q2 2019-04-23 00:11:27.882508\n",
      "3 T_Quadrant III_P_D03_2019Q2 2019-04-23 00:11:27.889023\n",
      "4 T_Quadrant III_P_D04_2019Q2 2019-04-23 00:11:27.895443\n",
      "5 T_Quadrant III_P_D05_2019Q2 2019-04-23 00:11:27.902567\n",
      "6 T_Quadrant III_P_D06_2019Q2 2019-04-23 00:11:27.908713\n",
      "7 T_Quadrant III_P_D07_2019Q2 2019-04-23 00:11:27.915154\n",
      "8 T_Quadrant III_P_D08_2019Q2 2019-04-23 00:11:27.921633\n",
      "9 T_Quadrant III_P_D09_2019Q2 2019-04-23 00:11:27.927657\n",
      "10 T_Quadrant III_P_D10_2019Q2 2019-04-23 00:11:27.933820\n",
      "11 T_Quadrant III_S_D01_2019Q2 2019-04-23 00:11:27.936937\n",
      "12 T_Quadrant III_S_D02_2019Q2 2019-04-23 00:11:27.939839\n",
      "13 T_Quadrant III_S_D03_2019Q2 2019-04-23 00:11:27.942814\n",
      "14 T_Quadrant III_S_D04_2019Q2 2019-04-23 00:11:27.945780\n",
      "15 T_Quadrant III_S_D05_2019Q2 2019-04-23 00:11:27.948792\n",
      "16 T_Quadrant III_S_D06_2019Q2 2019-04-23 00:11:27.951955\n",
      "17 T_Quadrant III_S_D07_2019Q2 2019-04-23 00:11:27.955505\n",
      "18 T_Quadrant III_S_D08_2019Q2 2019-04-23 00:11:27.959037\n",
      "19 T_Quadrant III_S_D09_2019Q2 2019-04-23 00:11:27.962881\n",
      "20 T_Quadrant III_S_D10_2019Q2 2019-04-23 00:11:27.966380\n",
      "21 T_Quadrant III_T_D01_2019Q2 2019-04-23 00:11:27.969369\n",
      "22 T_Quadrant III_T_D02_2019Q2 2019-04-23 00:11:27.972156\n",
      "23 T_Quadrant III_T_D03_2019Q2 2019-04-23 00:11:27.974974\n",
      "24 T_Quadrant III_T_D04_2019Q2 2019-04-23 00:11:27.978021\n",
      "25 T_Quadrant III_T_D05_2019Q2 2019-04-23 00:11:27.980415\n",
      "26 T_Quadrant III_T_D06_2019Q2 2019-04-23 00:11:27.983036\n",
      "27 T_Quadrant III_T_D07_2019Q2 2019-04-23 00:11:27.985404\n",
      "28 T_Quadrant III_T_D08_2019Q2 2019-04-23 00:11:27.987760\n",
      "29 T_Quadrant III_T_D09_2019Q2 2019-04-23 00:11:27.990447\n",
      "30 T_Quadrant III_T_D10_2019Q2 2019-04-23 00:11:27.992934\n",
      "31 T_Quadrant II_P_D01_2019Q2 2019-04-23 00:11:27.998949\n",
      "32 T_Quadrant II_P_D02_2019Q2 2019-04-23 00:11:28.005335\n",
      "33 T_Quadrant II_P_D03_2019Q2 2019-04-23 00:11:28.011570\n",
      "34 T_Quadrant II_P_D04_2019Q2 2019-04-23 00:11:28.017993\n",
      "35 T_Quadrant II_P_D05_2019Q2 2019-04-23 00:11:28.023873\n",
      "36 T_Quadrant II_P_D06_2019Q2 2019-04-23 00:11:28.031423\n",
      "37 T_Quadrant II_P_D07_2019Q2 2019-04-23 00:11:28.037808\n",
      "38 T_Quadrant II_P_D08_2019Q2 2019-04-23 00:11:28.044769\n",
      "39 T_Quadrant II_P_D09_2019Q2 2019-04-23 00:11:28.050957\n",
      "40 T_Quadrant II_P_D10_2019Q2 2019-04-23 00:11:28.057246\n",
      "41 T_Quadrant II_S_D01_2019Q2 2019-04-23 00:11:28.060122\n",
      "42 T_Quadrant II_S_D02_2019Q2 2019-04-23 00:11:28.062900\n",
      "43 T_Quadrant II_S_D03_2019Q2 2019-04-23 00:11:28.065878\n",
      "44 T_Quadrant II_S_D04_2019Q2 2019-04-23 00:11:28.068568\n",
      "45 T_Quadrant II_S_D05_2019Q2 2019-04-23 00:11:28.071250\n",
      "46 T_Quadrant II_S_D06_2019Q2 2019-04-23 00:11:28.073863\n",
      "47 T_Quadrant II_S_D07_2019Q2 2019-04-23 00:11:28.077414\n",
      "48 T_Quadrant II_S_D08_2019Q2 2019-04-23 00:11:28.080317\n",
      "49 T_Quadrant II_S_D09_2019Q2 2019-04-23 00:11:28.083023\n",
      "50 T_Quadrant II_S_D10_2019Q2 2019-04-23 00:11:28.085918\n",
      "51 T_Quadrant II_T_D01_2019Q2 2019-04-23 00:11:28.088275\n",
      "52 T_Quadrant II_T_D02_2019Q2 2019-04-23 00:11:28.090615\n",
      "53 T_Quadrant II_T_D03_2019Q2 2019-04-23 00:11:28.092812\n",
      "54 T_Quadrant II_T_D04_2019Q2 2019-04-23 00:11:28.094881\n",
      "55 T_Quadrant II_T_D05_2019Q2 2019-04-23 00:11:28.097053\n",
      "56 T_Quadrant II_T_D06_2019Q2 2019-04-23 00:11:28.099293\n",
      "57 T_Quadrant II_T_D07_2019Q2 2019-04-23 00:11:28.101584\n",
      "58 T_Quadrant II_T_D08_2019Q2 2019-04-23 00:11:28.104120\n",
      "59 T_Quadrant II_T_D09_2019Q2 2019-04-23 00:11:28.106462\n",
      "60 T_Quadrant II_T_D10_2019Q2 2019-04-23 00:11:28.108742\n",
      "61 T_Quadrant IV_P_D01_2019Q2 2019-04-23 00:11:28.114403\n",
      "62 T_Quadrant IV_P_D02_2019Q2 2019-04-23 00:11:28.120342\n",
      "63 T_Quadrant IV_P_D03_2019Q2 2019-04-23 00:11:28.126187\n",
      "64 T_Quadrant IV_P_D04_2019Q2 2019-04-23 00:11:28.132232\n",
      "65 T_Quadrant IV_P_D05_2019Q2 2019-04-23 00:11:28.138344\n",
      "66 T_Quadrant IV_P_D06_2019Q2 2019-04-23 00:11:28.144399\n",
      "67 T_Quadrant IV_P_D07_2019Q2 2019-04-23 00:11:28.150277\n",
      "68 T_Quadrant IV_P_D08_2019Q2 2019-04-23 00:11:28.156253\n",
      "69 T_Quadrant IV_P_D09_2019Q2 2019-04-23 00:11:28.162025\n",
      "70 T_Quadrant IV_P_D10_2019Q2 2019-04-23 00:11:28.168550\n",
      "71 T_Quadrant IV_S_D01_2019Q2 2019-04-23 00:11:28.171543\n",
      "72 T_Quadrant IV_S_D02_2019Q2 2019-04-23 00:11:28.174481\n",
      "73 T_Quadrant IV_S_D03_2019Q2 2019-04-23 00:11:28.177548\n",
      "74 T_Quadrant IV_S_D04_2019Q2 2019-04-23 00:11:28.180558\n",
      "75 T_Quadrant IV_S_D05_2019Q2 2019-04-23 00:11:28.183506\n",
      "76 T_Quadrant IV_S_D06_2019Q2 2019-04-23 00:11:28.186349\n",
      "77 T_Quadrant IV_S_D07_2019Q2 2019-04-23 00:11:28.189090\n",
      "78 T_Quadrant IV_S_D08_2019Q2 2019-04-23 00:11:28.191944\n",
      "79 T_Quadrant IV_S_D09_2019Q2 2019-04-23 00:11:28.195040\n",
      "80 T_Quadrant IV_S_D10_2019Q2 2019-04-23 00:11:28.198295\n",
      "81 T_Quadrant IV_T_D01_2019Q2 2019-04-23 00:11:28.200789\n",
      "82 T_Quadrant IV_T_D02_2019Q2 2019-04-23 00:11:28.203405\n",
      "83 T_Quadrant IV_T_D03_2019Q2 2019-04-23 00:11:28.205914\n",
      "84 T_Quadrant IV_T_D04_2019Q2 2019-04-23 00:11:28.208325\n",
      "85 T_Quadrant IV_T_D05_2019Q2 2019-04-23 00:11:28.211444\n",
      "86 T_Quadrant IV_T_D06_2019Q2 2019-04-23 00:11:28.214715\n",
      "87 T_Quadrant IV_T_D07_2019Q2 2019-04-23 00:11:28.217254\n",
      "88 T_Quadrant IV_T_D08_2019Q2 2019-04-23 00:11:28.219700\n",
      "89 T_Quadrant IV_T_D09_2019Q2 2019-04-23 00:11:28.222199\n",
      "90 T_Quadrant IV_T_D10_2019Q2 2019-04-23 00:11:28.224619\n",
      "91 T_Quadrant I_P_D01_2019Q2 2019-04-23 00:11:28.240460\n",
      "92 T_Quadrant I_P_D02_2019Q2 2019-04-23 00:11:28.256979\n",
      "93 T_Quadrant I_P_D03_2019Q2 2019-04-23 00:11:28.273262\n",
      "94 T_Quadrant I_P_D04_2019Q2 2019-04-23 00:11:28.290139\n",
      "95 T_Quadrant I_P_D05_2019Q2 2019-04-23 00:11:28.310078\n",
      "96 T_Quadrant I_P_D06_2019Q2 2019-04-23 00:11:28.325477\n",
      "97 T_Quadrant I_P_D07_2019Q2 2019-04-23 00:11:28.341983\n",
      "98 T_Quadrant I_P_D08_2019Q2 2019-04-23 00:11:28.357996\n",
      "99 T_Quadrant I_P_D09_2019Q2 2019-04-23 00:11:28.374415\n",
      "100 T_Quadrant I_P_D10_2019Q2 2019-04-23 00:11:28.390936\n",
      "101 T_Quadrant I_S_D01_2019Q2 2019-04-23 00:11:28.396871\n",
      "102 T_Quadrant I_S_D02_2019Q2 2019-04-23 00:11:28.403774\n",
      "103 T_Quadrant I_S_D03_2019Q2 2019-04-23 00:11:28.409401\n",
      "104 T_Quadrant I_S_D04_2019Q2 2019-04-23 00:11:28.414635\n",
      "105 T_Quadrant I_S_D05_2019Q2 2019-04-23 00:11:28.420104\n",
      "106 T_Quadrant I_S_D06_2019Q2 2019-04-23 00:11:28.425605\n",
      "107 T_Quadrant I_S_D07_2019Q2 2019-04-23 00:11:28.431839\n",
      "108 T_Quadrant I_S_D08_2019Q2 2019-04-23 00:11:28.437363\n",
      "109 T_Quadrant I_S_D09_2019Q2 2019-04-23 00:11:28.442782\n",
      "110 T_Quadrant I_S_D10_2019Q2 2019-04-23 00:11:28.448166\n",
      "111 T_Quadrant I_T_D01_2019Q2 2019-04-23 00:11:28.451854\n",
      "112 T_Quadrant I_T_D02_2019Q2 2019-04-23 00:11:28.455436\n",
      "113 T_Quadrant I_T_D03_2019Q2 2019-04-23 00:11:28.458952\n",
      "114 T_Quadrant I_T_D04_2019Q2 2019-04-23 00:11:28.462628\n",
      "115 T_Quadrant I_T_D05_2019Q2 2019-04-23 00:11:28.466464\n",
      "116 T_Quadrant I_T_D06_2019Q2 2019-04-23 00:11:28.469982\n",
      "117 T_Quadrant I_T_D07_2019Q2 2019-04-23 00:11:28.473202\n",
      "118 T_Quadrant I_T_D08_2019Q2 2019-04-23 00:11:28.476785\n",
      "119 T_Quadrant I_T_D09_2019Q2 2019-04-23 00:11:28.480581\n",
      "120 T_Quadrant I_T_D10_2019Q2 2019-04-23 00:11:28.484164\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "# Test decile\n",
    "\n",
    "i_counter=0\n",
    "for seg,group in test_all_df.groupby(['segment_Decile']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_Decile','segment_2019Q2']]\n",
    "    group.to_csv(folder_write_inner_decile+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())\n",
    "    \n",
    "for seg,group in test_seg_P_L_18_plus.groupby(['segment_Decile']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_Decile','segment_2019Q2']]\n",
    "    group.to_csv(folder_write_inner_decile+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 C_Quadrant III_P_D01_2019Q2 2019-04-23 00:11:35.221452\n",
      "2 C_Quadrant III_P_D02_2019Q2 2019-04-23 00:11:35.223513\n",
      "3 C_Quadrant III_P_D03_2019Q2 2019-04-23 00:11:35.225291\n",
      "4 C_Quadrant III_P_D04_2019Q2 2019-04-23 00:11:35.227770\n",
      "5 C_Quadrant III_P_D05_2019Q2 2019-04-23 00:11:35.229551\n",
      "6 C_Quadrant III_P_D06_2019Q2 2019-04-23 00:11:35.231294\n",
      "7 C_Quadrant III_P_D07_2019Q2 2019-04-23 00:11:35.233031\n",
      "8 C_Quadrant III_P_D08_2019Q2 2019-04-23 00:11:35.234665\n",
      "9 C_Quadrant III_P_D09_2019Q2 2019-04-23 00:11:35.236369\n",
      "10 C_Quadrant III_P_D10_2019Q2 2019-04-23 00:11:35.237954\n",
      "11 C_Quadrant II_P_D01_2019Q2 2019-04-23 00:11:35.239614\n",
      "12 C_Quadrant II_P_D02_2019Q2 2019-04-23 00:11:35.241276\n",
      "13 C_Quadrant II_P_D03_2019Q2 2019-04-23 00:11:35.243418\n",
      "14 C_Quadrant II_P_D04_2019Q2 2019-04-23 00:11:35.245200\n",
      "15 C_Quadrant II_P_D05_2019Q2 2019-04-23 00:11:35.246953\n",
      "16 C_Quadrant II_P_D06_2019Q2 2019-04-23 00:11:35.248728\n",
      "17 C_Quadrant II_P_D07_2019Q2 2019-04-23 00:11:35.250467\n",
      "18 C_Quadrant II_P_D08_2019Q2 2019-04-23 00:11:35.252197\n",
      "19 C_Quadrant II_P_D09_2019Q2 2019-04-23 00:11:35.253930\n",
      "20 C_Quadrant II_P_D10_2019Q2 2019-04-23 00:11:35.255549\n",
      "21 C_Quadrant IV_P_D01_2019Q2 2019-04-23 00:11:35.257314\n",
      "22 C_Quadrant IV_P_D02_2019Q2 2019-04-23 00:11:35.259327\n",
      "23 C_Quadrant IV_P_D03_2019Q2 2019-04-23 00:11:35.260967\n",
      "24 C_Quadrant IV_P_D04_2019Q2 2019-04-23 00:11:35.271789\n",
      "25 C_Quadrant IV_P_D05_2019Q2 2019-04-23 00:11:35.273937\n",
      "26 C_Quadrant IV_P_D06_2019Q2 2019-04-23 00:11:35.275766\n",
      "27 C_Quadrant IV_P_D07_2019Q2 2019-04-23 00:11:35.277522\n",
      "28 C_Quadrant IV_P_D08_2019Q2 2019-04-23 00:11:35.279465\n",
      "29 C_Quadrant IV_P_D09_2019Q2 2019-04-23 00:11:35.281200\n",
      "30 C_Quadrant IV_P_D10_2019Q2 2019-04-23 00:11:35.282941\n",
      "31 C_Quadrant I_P_D01_2019Q2 2019-04-23 00:11:35.284963\n",
      "32 C_Quadrant I_P_D02_2019Q2 2019-04-23 00:11:35.286679\n",
      "33 C_Quadrant I_P_D03_2019Q2 2019-04-23 00:11:35.288337\n",
      "34 C_Quadrant I_P_D04_2019Q2 2019-04-23 00:11:35.290720\n",
      "35 C_Quadrant I_P_D05_2019Q2 2019-04-23 00:11:35.292391\n",
      "36 C_Quadrant I_P_D06_2019Q2 2019-04-23 00:11:35.294018\n",
      "37 C_Quadrant I_P_D07_2019Q2 2019-04-23 00:11:35.295773\n",
      "38 C_Quadrant I_P_D08_2019Q2 2019-04-23 00:11:35.297543\n",
      "39 C_Quadrant I_P_D09_2019Q2 2019-04-23 00:11:35.299303\n",
      "40 C_Quadrant I_P_D10_2019Q2 2019-04-23 00:11:35.301059\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "# Control decile\n",
    "\n",
    "i_counter=0\n",
    "for seg,group in control_all_df.groupby(['segment_Decile']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_Decile','segment_2019Q2']]\n",
    "    group.to_csv(folder_write_inner_decile+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())\n",
    "    \n",
    "for seg,group in control_seg_P_L_18_plus.groupby(['segment_Decile']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_Decile','segment_2019Q2']]\n",
    "    group.to_csv(folder_write_inner_decile+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 T_Quadrant III_P_H_2019Q2 2019-04-23 00:13:10.009753\n",
      "2 T_Quadrant III_P_L_2019Q2 2019-04-23 00:13:10.024680\n",
      "3 T_Quadrant III_P_M_2019Q2 2019-04-23 00:13:10.040382\n",
      "4 T_Quadrant III_S_H_2019Q2 2019-04-23 00:13:10.047494\n",
      "5 T_Quadrant III_S_L_2019Q2 2019-04-23 00:13:10.053897\n",
      "6 T_Quadrant III_S_M_2019Q2 2019-04-23 00:13:10.059760\n",
      "7 T_Quadrant III_T_H_2019Q2 2019-04-23 00:13:10.064451\n",
      "8 T_Quadrant III_T_L_2019Q2 2019-04-23 00:13:10.069536\n",
      "9 T_Quadrant III_T_M_2019Q2 2019-04-23 00:13:10.073983\n",
      "10 T_Quadrant II_P_H_2019Q2 2019-04-23 00:13:10.092472\n",
      "11 T_Quadrant II_P_L_2019Q2 2019-04-23 00:13:10.111905\n",
      "12 T_Quadrant II_P_M_2019Q2 2019-04-23 00:13:10.131278\n",
      "13 T_Quadrant II_S_H_2019Q2 2019-04-23 00:13:10.139139\n",
      "14 T_Quadrant II_S_L_2019Q2 2019-04-23 00:13:10.144688\n",
      "15 T_Quadrant II_S_M_2019Q2 2019-04-23 00:13:10.150242\n",
      "16 T_Quadrant II_T_H_2019Q2 2019-04-23 00:13:10.154329\n",
      "17 T_Quadrant II_T_L_2019Q2 2019-04-23 00:13:10.158360\n",
      "18 T_Quadrant II_T_M_2019Q2 2019-04-23 00:13:10.162446\n",
      "19 T_Quadrant IV_P_H_2019Q2 2019-04-23 00:13:10.180794\n",
      "20 T_Quadrant IV_P_L_2019Q2 2019-04-23 00:13:10.195221\n",
      "21 T_Quadrant IV_P_M_2019Q2 2019-04-23 00:13:10.208979\n",
      "22 T_Quadrant IV_S_H_2019Q2 2019-04-23 00:13:10.217158\n",
      "23 T_Quadrant IV_S_L_2019Q2 2019-04-23 00:13:10.222713\n",
      "24 T_Quadrant IV_S_M_2019Q2 2019-04-23 00:13:10.228029\n",
      "25 T_Quadrant IV_T_H_2019Q2 2019-04-23 00:13:10.233083\n",
      "26 T_Quadrant IV_T_L_2019Q2 2019-04-23 00:13:10.237004\n",
      "27 T_Quadrant IV_T_M_2019Q2 2019-04-23 00:13:10.241686\n",
      "28 T_Quadrant I_P_H_2019Q2 2019-04-23 00:13:10.297476\n",
      "29 T_Quadrant I_P_L_2019Q2 2019-04-23 00:13:10.341519\n",
      "30 T_Quadrant I_P_M_2019Q2 2019-04-23 00:13:10.383645\n",
      "31 T_Quadrant I_S_H_2019Q2 2019-04-23 00:13:10.402066\n",
      "32 T_Quadrant I_S_L_2019Q2 2019-04-23 00:13:10.416901\n",
      "33 T_Quadrant I_S_M_2019Q2 2019-04-23 00:13:10.429617\n",
      "34 T_Quadrant I_T_H_2019Q2 2019-04-23 00:13:10.438246\n",
      "35 T_Quadrant I_T_L_2019Q2 2019-04-23 00:13:10.445884\n",
      "36 T_Quadrant I_T_M_2019Q2 2019-04-23 00:13:10.453328\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "i_counter=0\n",
    "for seg,group in test_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner_HML+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())\n",
    "\n",
    "    \n",
    "for seg,group in test_seg_P_L_18_plus.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner_HML+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 C_Quadrant III_P_H_2019Q2 2019-04-23 00:13:17.259958\n",
      "2 C_Quadrant III_P_L_2019Q2 2019-04-23 00:13:17.262503\n",
      "3 C_Quadrant III_P_M_2019Q2 2019-04-23 00:13:17.264756\n",
      "4 C_Quadrant II_P_H_2019Q2 2019-04-23 00:13:17.266980\n",
      "5 C_Quadrant II_P_L_2019Q2 2019-04-23 00:13:17.269181\n",
      "6 C_Quadrant II_P_M_2019Q2 2019-04-23 00:13:17.271379\n",
      "7 C_Quadrant IV_P_H_2019Q2 2019-04-23 00:13:17.274202\n",
      "8 C_Quadrant IV_P_L_2019Q2 2019-04-23 00:13:17.276412\n",
      "9 C_Quadrant IV_P_M_2019Q2 2019-04-23 00:13:17.278610\n",
      "10 C_Quadrant I_P_H_2019Q2 2019-04-23 00:13:17.280661\n",
      "11 C_Quadrant I_P_L_2019Q2 2019-04-23 00:13:17.282728\n",
      "12 C_Quadrant I_P_M_2019Q2 2019-04-23 00:13:17.284798\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "i_counter=0\n",
    "for seg,group in control_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner_HML+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())\n",
    "    \n",
    "for seg,group in control_seg_P_L_18_plus.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner_HML+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99871, 3)\n",
      "(1871049, 4)\n"
     ]
    }
   ],
   "source": [
    "# No use\n",
    "lapsed_trans=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed'],dtype=str).drop_duplicates() # Doesn't go to score at all, so no need to read all columns\n",
    "lapsed_trans['lapsed_trans']=True\n",
    "\n",
    "lapsed_master=pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "\n",
    "lapsed_master=lapsed_master.drop_duplicates(\"customer_id_hashed\")\n",
    "print(lapsed_master.shape)\n",
    "\n",
    "lapsed_master=pd.merge(lapsed_master,lapsed_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "print(lapsed_master.shape)\n",
    "lapsed_master=lapsed_master[~pd.isnull(lapsed_master['email_address_hash'])]\n",
    "\n",
    "# remove the non-match email ids at the end and no calculation for the WD here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "summary_18_plus_back_20160626=df_other_18_plus.groupby(['segment','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97864, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No use\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(test_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(control_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(test_all_df['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(control_all_df['email_address_hash'])]\n",
    "\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(test_seg_P_L_18_plus['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(control_seg_P_L_18_plus['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(test_seg_P_L_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(control_seg_P_L_18_plus['email_address_hash'])]\n",
    "\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(df_other_18_plus['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "\n",
    "\n",
    "lapsed_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "lapsed_master=lapsed_master[['customer_id_hashed','email_address_hash']]\n",
    "lapsed_master['segment']=\"WalkingDead_2019Q2\"\n",
    "lapsed_master['active']=\"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "lapsed_master.to_csv(folder_write_inner_decile+\"WalkingDead_Group_before_20160626.csv\",index=False)\n",
    "lapsed_master.to_csv(folder_write_inner_HML+\"WalkingDead_Group_before_20160626.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "summary_test=test_all_df.groupby(['segment_2019Q2','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_control=control_all_df.groupby(['segment_2019Q2','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "\n",
    "summary_test_seg_P_L_18=test_seg_P_L_18_plus.groupby(['segment_2019Q2','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_control_seg_P_L_18=control_seg_P_L_18_plus.groupby(['segment_2019Q2','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "\n",
    "\n",
    "summary_WD=lapsed_master.groupby(['segment','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "\n",
    "summary_overll=summary_test.append(summary_control).append(summary_WD).append(summary_18_plus_back_20160626).append(summary_test_seg_P_L_18).append(summary_control_seg_P_L_18)\n",
    "\n",
    "\n",
    "summary_overll.to_csv(folder_write_inner_HML+\"test_control_groups_summary_HLM_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No use\n",
    "summary_test=test_all_df.groupby(['segment_Decile','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_Decile\":\"segment\"})\n",
    "summary_control=control_all_df.groupby(['segment_Decile','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_Decile\":\"segment\"})\n",
    "\n",
    "summary_test_seg_P_L_18=test_seg_P_L_18_plus.groupby(['segment_Decile','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_Decile\":\"segment\"})\n",
    "summary_control_seg_P_L_18=control_seg_P_L_18_plus.groupby(['segment_Decile','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_Decile\":\"segment\"})\n",
    "\n",
    "\n",
    "summary_WD=lapsed_master.groupby(['segment','active'])['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "\n",
    "summary_overll=summary_test.append(summary_control).append(summary_WD).append(summary_18_plus_back_20160626).append(summary_test_seg_P_L_18).append(summary_control_seg_P_L_18)\n",
    "\n",
    "\n",
    "summary_overll.to_csv(folder_write_inner_decile+\"test_control_groups_summary_decile_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
