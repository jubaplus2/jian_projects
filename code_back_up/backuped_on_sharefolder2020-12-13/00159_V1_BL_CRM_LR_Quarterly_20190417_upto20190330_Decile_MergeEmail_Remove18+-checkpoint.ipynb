{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapsed Start on: 2017-10-01\n",
      "Active Start on: 2018-04-01\n",
      "Store Allocation Starting on: 2017-10-01\n",
      "Last Saturday: 2019-03-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V1\n",
    "# upto 20190330\n",
    "# calculate decile first\n",
    "# then merge email\n",
    "# then remove the 18+\n",
    "\n",
    "\n",
    "# In other words: all the same, except for the time range\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='V1_BL_CRM_LR_Quarterly_20190417_upto20190330_Decile_MergeEmail_Remove18+.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "\n",
    "lastdate = datetime.date(2019,3,30) # Recent Saturday\n",
    "active_Sunday = str(lastdate-datetime.timedelta(days=52*7-1))\n",
    "lapsed_Sunday = str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "Beginning_18_months_ago=str(lastdate-datetime.timedelta(days=52*7*1.5-1))\n",
    "lastdate=str(lastdate)\n",
    "print(\"Lapsed Start on: \"+lapsed_Sunday) #>=\n",
    "print(\"Active Start on: \"+active_Sunday) #>=\n",
    "print(\"Store Allocation Starting on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Last Saturday: \"+lastdate) #<=\n",
    "\n",
    "def recrusive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/output_upto20190330_Decile_MergeEmail_Remove_'+str(datetime.datetime.now().date())+'/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-04-18 10:25:41.701321\n",
      "2 2019-04-18 10:26:22.846903\n",
      "3 2019-04-18 10:26:59.080279\n",
      "4 2019-04-18 10:27:40.476683\n",
      "5 2019-04-18 10:28:22.466453\n",
      "6 2019-04-18 10:29:13.805466\n",
      "7 2019-04-18 10:30:06.917008\n",
      "8 2019-04-18 10:30:56.661353\n",
      "9 2019-04-18 10:31:53.278814\n",
      "10 2019-04-18 10:32:47.684026\n",
      "11 2019-04-18 10:34:02.601595\n",
      "12 2019-04-18 10:35:49.971883\n",
      "13 2019-04-18 10:37:40.310354\n",
      "14 2019-04-18 10:40:19.548358\n",
      "15 2019-04-18 10:42:02.192818\n",
      "Earliest Date:2016-06-26\n",
      "Deduped 2019-04-18 11:03:39.755948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize_num = 10**7\n",
    "filename='/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q1/crm_newscore_0922/combinedtransactions_0922.csv'\n",
    "dftrans_before_20180922=pd.DataFrame()\n",
    "count_i=0\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize_num,dtype=str,usecols=['customer_id_hashed','transaction_date','transaction_time',\n",
    "                   'transaction_id','location_id','total_transaction_amt'],nrows=samplerows): #Add back the transaction info\n",
    "    chunk['total_transaction_amt']=chunk['total_transaction_amt'].astype(float)\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    \n",
    "    dftrans_before_20180922=dftrans_before_20180922.append(chunk)\n",
    "    count_i+=1\n",
    "    print(count_i,datetime.datetime.now())\n",
    "\n",
    "\n",
    "del chunk\n",
    "print(\"Earliest Date:\" + str(dftrans_before_20180922['transaction_date'].min()))\n",
    "\n",
    "dftrans_before_20180922=dftrans_before_20180922.sort_values([\"customer_id_hashed\",\"transaction_date\"],ascending=[True,False])\n",
    "dftrans_before_20180922=dftrans_before_20180922.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "print(\"Deduped\",datetime.datetime.now())\n",
    "\n",
    "\n",
    "logging.info(\"Deduped: \"+str(datetime.datetime.now()))\n",
    "del dftrans_before_20180922['transaction_time']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2018-09-29\n",
      "Max_Date: 2019-02-09\n"
     ]
    }
   ],
   "source": [
    "# Up to 2019-03-30\n",
    "# All item level data, weekly and the 1-time transfered historical data\n",
    "historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "historical_daily_data_list=list(recrusive_file_gen(historical_daily_data_folder))\n",
    "historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1])\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y%m%d\").date())\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']<=datetime.date(2019,3,30)]\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']>datetime.date(2018,9,22)]\n",
    "print(historical_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(historical_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(historical_daily_df['week_end_dt'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n",
      "Min_Date: 2019-02-16\n",
      "Max_Date: 2019-03-30\n"
     ]
    }
   ],
   "source": [
    "new_daily_data_folder=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "new_daily_data_list=list(recrusive_file_gen(new_daily_data_folder))\n",
    "new_daily_data_list=[x for x in new_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "new_daily_data_list=[x for x in new_daily_data_list if \"hist\" not in x]\n",
    "\n",
    "new_daily_df=pd.DataFrame({\"file_path\":new_daily_data_list})\n",
    "\n",
    "new_daily_df['week_end_dt']=new_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2019_by_weeks/MediaStorm_\")[1][:10])\n",
    "new_daily_df['week_end_dt']=new_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']<=datetime.date(2019,3,30)]\n",
    "print(new_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(new_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(new_daily_df['week_end_dt'].max()))\n",
    "\n",
    "daily_df_file_after_20180922=historical_daily_df.append(new_daily_df)\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 27\n",
      "1 done 2019-04-18 11:05:16.877621\n",
      "2 done 2019-04-18 11:06:11.960628\n",
      "3 done 2019-04-18 11:06:54.733328\n",
      "4 done 2019-04-18 11:07:38.984404\n",
      "5 done 2019-04-18 11:08:26.194611\n",
      "6 done 2019-04-18 11:09:15.877165\n",
      "7 done 2019-04-18 11:10:09.378583\n",
      "8 done 2019-04-18 11:11:10.817855\n",
      "9 done 2019-04-18 11:12:34.454194\n",
      "10 done 2019-04-18 11:13:57.131669\n",
      "11 done 2019-04-18 11:15:28.239436\n",
      "12 done 2019-04-18 11:17:10.125789\n",
      "13 done 2019-04-18 11:19:04.433835\n",
      "14 done 2019-04-18 11:20:25.728640\n",
      "15 done 2019-04-18 11:21:28.107393\n",
      "16 done 2019-04-18 11:22:29.820762\n",
      "17 done 2019-04-18 11:23:43.350256\n",
      "18 done 2019-04-18 11:24:52.671074\n",
      "19 done 2019-04-18 11:25:52.723832\n",
      "20 done 2019-04-18 11:26:55.690475\n",
      "21 done 2019-04-18 11:28:00.009239\n",
      "22 done 2019-04-18 11:29:05.379335\n",
      "23 done 2019-04-18 11:30:15.974040\n",
      "24 done 2019-04-18 11:31:26.693073\n",
      "25 done 2019-04-18 11:32:35.221665\n",
      "26 done 2019-04-18 11:33:45.232130\n",
      "27 done 2019-04-18 11:34:58.946155\n"
     ]
    }
   ],
   "source": [
    "combined_rewards_transaction_after_20180922_agg=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df=df.drop_duplicates()\n",
    "\n",
    "    \n",
    "    combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "gc.collect()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"transaction_dt\":\"transaction_date\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.groupby(['customer_id_hashed','transaction_date','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "combined_rewards_transaction_after_20180922_agg=combined_rewards_transaction_after_20180922_agg.rename(columns={\"item_transaction_amt\":\"total_transaction_amt\"})\n",
    "\n",
    "all_rewards_since_201606=dftrans_before_20180922.append(combined_rewards_transaction_after_20180922_agg)\n",
    "\n",
    "del dftrans_before_20180922\n",
    "del combined_rewards_transaction_after_20180922_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606.to_csv(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/BL_Rewards_Transactions_20160626_to_20190330.csv\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Lapsed data\\n\\nlapsed=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\\n                     sep=\",\",usecols=[\\'customer_id_hashed\\',\\'transaction_date\\'],dtype=str) # Doesn\\'t go to score at all, so no need to read all columns\\n\\nprint(\"Lapsed earliest:\" + lapsed[\\'transaction_date\\'].min())\\nprint(\"Lapsed newest:\" + lapsed[\\'transaction_date\\'].max())\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Lapsed data\n",
    "\n",
    "lapsed=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed','transaction_date'],dtype=str) # Doesn't go to score at all, so no need to read all columns\n",
    "\n",
    "print(\"Lapsed earliest:\" + lapsed['transaction_date'].min())\n",
    "print(\"Lapsed newest:\" + lapsed['transaction_date'].max())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24905063, 4)\n",
      "(20569010, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the store for an id\n",
    "\n",
    "frequently_visit_stores_18_months=all_rewards_since_201606[all_rewards_since_201606['transaction_date']>=Beginning_18_months_ago]\n",
    "\n",
    "frequently_visit_stores_2=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['total_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.groupby(['customer_id_hashed','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "\n",
    "frequently_visit_stores_18_months=pd.merge(frequently_visit_stores_18_months,frequently_visit_stores_2,on=['customer_id_hashed','location_id'],how=\"outer\")\n",
    "del frequently_visit_stores_2\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months.sort_values(['customer_id_hashed','trans','sales'],ascending=[True,False,False])\n",
    "\n",
    "frequently_visit_stores_18_months=frequently_visit_stores_18_months[['customer_id_hashed','location_id']].drop_duplicates(\"customer_id_hashed\")\n",
    "print(frequently_visit_stores_18_months.shape)\n",
    "frequently_visit_stores_18_months.to_csv(folder_write+\"frequently_visit_stores_18_months.csv\",index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-26\n",
      "2019-03-30\n"
     ]
    }
   ],
   "source": [
    "###get recency\n",
    "dfrecency=all_rewards_since_201606[['customer_id_hashed','transaction_date']].sort_values(\"transaction_date\",ascending=False).drop_duplicates()#Allready combined\n",
    "\n",
    "print (min(dfrecency['transaction_date']))\n",
    "print (max(dfrecency['transaction_date']))\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25770895, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrecency['transaction_date'] = pd.to_datetime(dfrecency['transaction_date'])\n",
    "dfrecency['recency'] =  datetime.datetime.strptime(str(lastdate), '%Y-%m-%d').date() - dfrecency['transaction_date']\n",
    "dfrecency['recency'] = dfrecency['recency'].apply(lambda x:x.days)\n",
    "dfrecency['recency'] = np.ceil((dfrecency['recency']+1)/30)\n",
    "\n",
    "dfrecency = dfrecency[['customer_id_hashed','recency']]\n",
    "dfrecency = dfrecency.drop_duplicates('customer_id_hashed')\n",
    "dfrecency.to_csv(folder_write + 'dfrecency2.csv',index = False)\n",
    "\n",
    "dfrecency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-18 12:25:59.447855 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-12/MediaStormMasterBiWeekly20190115-132855-055.txt\n",
      "2019-04-18 12:26:06.287462 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-01-26/MediaStormMasterBiWeekly20190129-130902-016.txt\n",
      "2019-04-18 12:26:12.048389 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-02/MediaStormMasterWeekly20190205-111610-675.txt\n",
      "2019-04-18 12:26:21.199527 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-09/MediaStormMasterWeekly20190212-122428-267.txt\n",
      "2019-04-18 12:26:27.664936 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-16/MediaStormMasterWeekly20190219-113650-867.txt\n",
      "2019-04-18 12:26:35.004173 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-02-23/MediaStormMasterWeekly20190226-112921-061.txt\n",
      "2019-04-18 12:26:41.333909 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-02/MediaStormMasterWeekly20190305-112945-302.txt\n",
      "2019-04-18 12:26:48.209184 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-09/MediaStormMasterWeekly20190312-121512-232.txt\n",
      "2019-04-18 12:26:54.299150 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-16/MediaStormMasterWeekly20190319-112932-415.txt\n",
      "2019-04-18 12:27:01.229374 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-23/MediaStormMasterWeekly20190326-113052-887.txt\n",
      "2019-04-18 12:27:07.372785 /home/jian/BigLots/2019_by_weeks/MediaStorm_2019-03-30/MediaStormMasterWeekly20190402-113131-172.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "#########\n",
    "new_sign_ups_2019_list=list(recrusive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "new_sign_ups_2019_list=sorted([x for x in new_sign_ups_2019_list if \"ster\" in x])\n",
    "\n",
    "new_sign_ups_2019_df=pd.DataFrame({\"file_path\":new_sign_ups_2019_list})\n",
    "new_sign_ups_2019_df['Date']=new_sign_ups_2019_df['file_path'].apply(lambda x: x.split(\"MediaStorm_\")[1][:10])\n",
    "new_sign_ups_2019_df=new_sign_ups_2019_df[new_sign_ups_2019_df['Date']<=lastdate]\n",
    "\n",
    "for file_new_signups in new_sign_ups_2019_df['file_path'].tolist():\n",
    "    df=pd.read_table(file_new_signups,dtype=str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'],sep=\"|\",nrows=samplerows)\n",
    "    dfiddetail=df.append(dfiddetail) # Already sorted and newest kept on the top\n",
    "    print(datetime.datetime.now(),file_new_signups)\n",
    "dfiddetail=dfiddetail.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "######\n",
    "dfiddetail2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "dfiddetail = dfiddetail.append(dfiddetail2,ignore_index = True)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "dfiddetail = dfiddetail.drop_duplicates('email_address_hash')\n",
    "\n",
    "del dfiddetail2\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards_since_201606['transactions'] = 1\n",
    "dftotal = all_rewards_since_201606[['customer_id_hashed','total_transaction_amt','transactions']].groupby(['customer_id_hashed']).sum().reset_index().rename(columns={\"total_transaction_amt\":\"sales\"})\n",
    "\n",
    "dftotal = pd.merge(dftotal,dfrecency,on = 'customer_id_hashed',how='outer')\n",
    "del dfrecency\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>sales</th>\n",
       "      <th>transactions</th>\n",
       "      <th>recency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>244.58</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001dadc0265bf9d250566d74e0006323f18b5826641...</td>\n",
       "      <td>143.25</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed   sales  transactions  \\\n",
       "0  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...  244.58             4   \n",
       "1  000001dadc0265bf9d250566d74e0006323f18b5826641...  143.25             3   \n",
       "\n",
       "   recency  \n",
       "0      6.0  \n",
       "1      4.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal = dftotal.sort_values(['transactions','recency','sales'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Transindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['sales','recency','transactions'],ascending = [0,1,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'Amtindex'})\n",
    "\n",
    "dftotal = dftotal.sort_values(['recency','transactions','sales'],ascending = [1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'recencyindex'})\n",
    "\n",
    "c_ids = len(dftotal.index)\n",
    "logging.info('total customers from transaction and amt: ')\n",
    "logging.info(c_ids)\n",
    "c_ids = np.ceil(c_ids/5.0)\n",
    "\n",
    "dftotal['Transindex'] = np.ceil((dftotal['Transindex']+1)/c_ids)\n",
    "dftotal['Amtindex'] = np.ceil((dftotal['Amtindex']+1)/c_ids)\n",
    "dftotal['recencyindex'] = np.ceil((dftotal['recencyindex']+1)/c_ids)\n",
    "\n",
    "dftotal['RFM'] = dftotal['recencyindex']*100 + dftotal['Transindex']*10 + dftotal['Amtindex']\n",
    "dftotal = dftotal.sort_values(['RFM','recency','transactions',\n",
    "                               'sales'],ascending = [1,1,0,0])\n",
    "dftotal.reset_index(drop = True, inplace = True)\n",
    "dftotal.reset_index(inplace = True)\n",
    "dftotal = dftotal.rename(columns = {'index':'frmindex'})\n",
    "c_ids = len(dftotal.index)\n",
    "c_ids = np.ceil(c_ids/10.0)\n",
    "dftotal['frmindex'] = np.ceil((dftotal['frmindex']+1)/c_ids)\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm.csv',index = False)\n",
    "\n",
    "dftotal = pd.read_csv(folder_write + 'dfrfm.csv')\n",
    "\n",
    "dftotal = dftotal[['customer_id_hashed','frmindex']]\n",
    "\n",
    "dfrecency = pd.read_csv(folder_write + 'dfrecency.csv')\n",
    "dfrecency['active'] = np.where(dfrecency['transaction_date']>=active_Sunday,'active',\n",
    "                               np.where(dfrecency['transaction_date']>=lapsed_Sunday,'lapsed','other')\n",
    "                              )\n",
    "\n",
    "print(dfrecency['active'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24959949\n",
      "totalids_trans: 25770895\n",
      "totalids_trans_mergewithmaster: 24077571\n",
      "['P' 'S' nan 'T']\n",
      "['P' 'S' 'T']\n",
      "Final wemailcsv: (24077571, 6)\n"
     ]
    }
   ],
   "source": [
    "dftotal = pd.merge(dftotal,dfrecency[['customer_id_hashed','active']],on = 'customer_id_hashed')\n",
    "\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].astype('str')\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].str[0:5]\n",
    "dfiddetail['customer_zip_code'].fillna('00000',inplace = True)\n",
    "dfiddetail['customer_zip_code'] = dfiddetail['customer_zip_code'].apply(lambda x:x.zfill(5))\n",
    "print(len(dfiddetail.index))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "print(\"totalids_trans:\",len(dftotal.index))\n",
    "dftotal = pd.merge(dftotal,dfiddetail,on = 'customer_id_hashed')\n",
    "print(\"totalids_trans_mergewithmaster:\",len(dftotal.index))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "zipmap = pd.read_csv('/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/zip_with_ta_dma.csv',dtype = 'str')\n",
    "zipmap['zipcodegroup'] = zipmap['revenue_flag']\n",
    "zipmap = zipmap[['zip','zipcodegroup']].drop_duplicates('zip')\n",
    "zipmap.columns = ['customer_zip_code','zipcodegroup']\n",
    "dftotal = pd.merge(dftotal,zipmap,on ='customer_zip_code',how = 'left' )\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "dftotal['zipcodegroup'].fillna('T',inplace = True)\n",
    "print(dftotal['zipcodegroup'].unique())\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "dftotal.to_csv(folder_write + 'dfrfm_wemail.csv',index = False)\n",
    "print(\"Final wemailcsv:\",dftotal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the ids don't have transactions within 18 months\n",
    "df_other_18_plus=dftotal[dftotal['active']==\"other\"][['customer_id_hashed','email_address_hash']]\n",
    "df_other_18_plus['segment']=\"18_months_plus_back_201606\"\n",
    "\n",
    "dftotal=dftotal[dftotal['active']!=\"other\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the primary stores for each member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequently_visit_stores_18_months=pd.read_csv(folder_write+\"frequently_visit_stores_18_months.csv\",dtype=str)\n",
    "register_stores=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/output_sing_up_location/BL_id_by_register_store_JL_2019-04-09.csv\",\n",
    "                            dtype=str,nrows=samplerows)\n",
    "register_stores=register_stores[['customer_id_hashed','sign_up_location']].rename(columns={\"sign_up_location\":\"location_id\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26754444, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_for_ids=frequently_visit_stores_18_months.append(register_stores)\n",
    "store_for_ids=store_for_ids.drop_duplicates(\"customer_id_hashed\")\n",
    "store_for_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del frequently_visit_stores_18_months\n",
    "del register_stores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal=pd.merge(dftotal,store_for_ids,on=\"customer_id_hashed\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1361, 2)\n",
      "1361\n"
     ]
    }
   ],
   "source": [
    "# read the quadrant by store for 2018 Q4\n",
    "\n",
    "Q4_store_quadrant=pd.read_excel(\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Excel_BL_2018_Q4_post_YoY_small_JL_2019-03-04.xlsx\",\n",
    "                                dtype=str,sheetname=\"Q4_Store_Quadrant_Defination\",usecols=['location_id','Quadrant'])\n",
    "print(Q4_store_quadrant.shape)\n",
    "print(len(Q4_store_quadrant['location_id'].unique()))\n",
    "\n",
    "dftotal=pd.merge(dftotal,Q4_store_quadrant,on=\"location_id\",how=\"left\")\n",
    "dftotal['Quadrant']=dftotal['Quadrant'].fillna(\"Quadrant III\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89172"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x: str(int(float(x))).zfill(2))\n",
    "dftotal['customer_zip_code']=dftotal['customer_zip_code'].apply(lambda x: x.zfill(5))\n",
    "dftotal['frmindex']=dftotal['frmindex'].apply(lambda x:\"D\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftotal.to_csv(folder_write + 'dfrfm_final_details_wemail_zip_StoreQuad.csv',index = False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotal['HML_Group']=np.where(dftotal['frmindex'].isin(['D01','D02','D03','D04']),\"H\",\n",
    "                                        np.where(dftotal['frmindex'].isin(['D05','D06','D07']),\"M\",\"L\"))\n",
    "\n",
    "dftotal['segment_2019Q2']=dftotal['Quadrant']+\"_\"+dftotal['zipcodegroup']+\"_\"+dftotal['HML_Group']+\"_2019Q2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-04-18 13:11:55.288597\n",
      "2 2019-04-18 13:11:58.887805\n",
      "3 2019-04-18 13:12:03.737250\n",
      "4 2019-04-18 13:12:08.171955\n",
      "5 2019-04-18 13:12:11.833088\n",
      "6 2019-04-18 13:12:15.843128\n",
      "7 2019-04-18 13:12:19.957492\n",
      "8 2019-04-18 13:12:23.754446\n",
      "9 2019-04-18 13:12:27.681960\n",
      "10 2019-04-18 13:12:35.053241\n",
      "11 2019-04-18 13:12:41.786924\n",
      "12 2019-04-18 13:12:49.556440\n",
      "13 2019-04-18 13:12:57.326375\n",
      "14 2019-04-18 13:13:04.191628\n",
      "15 2019-04-18 13:13:11.315941\n",
      "16 2019-04-18 13:13:18.576994\n",
      "17 2019-04-18 13:13:25.249444\n",
      "18 2019-04-18 13:13:32.358540\n",
      "19 2019-04-18 13:13:43.361885\n",
      "20 2019-04-18 13:13:53.983144\n",
      "21 2019-04-18 13:14:05.906142\n",
      "22 2019-04-18 13:14:17.492290\n",
      "23 2019-04-18 13:14:28.145348\n",
      "24 2019-04-18 13:14:38.892505\n",
      "25 2019-04-18 13:14:50.089314\n",
      "26 2019-04-18 13:15:01.040312\n",
      "27 2019-04-18 13:15:12.187270\n",
      "28 2019-04-18 13:15:35.940502\n",
      "29 2019-04-18 13:15:56.428978\n",
      "30 2019-04-18 13:16:26.128334\n",
      "31 2019-04-18 13:16:50.483983\n",
      "32 2019-04-18 13:17:14.140514\n",
      "33 2019-04-18 13:17:36.346531\n",
      "34 2019-04-18 13:18:00.714299\n",
      "35 2019-04-18 13:18:22.733379\n",
      "36 2019-04-18 13:18:47.785974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "total_rows=len(dftotal)\n",
    "\n",
    "test_all_df=pd.DataFrame()\n",
    "control_all_df=pd.DataFrame()\n",
    "\n",
    "i_counter=0\n",
    "\n",
    "for seg,group in dftotal.groupby(['segment_2019Q2']):\n",
    "    random_list=random.sample(range(len(group)), int(np.round(len(group)/total_rows*500000)))\n",
    "\n",
    "    group=group.reset_index()\n",
    "    del group['index']\n",
    "    group=group.reset_index()\n",
    "    df_control=group[group['index'].isin(random_list)]\n",
    "    df_test=group[~group['index'].isin(random_list)]\n",
    "    \n",
    "    df_control['segment_2019Q2']=\"C_\"+df_control['segment_2019Q2']\n",
    "    df_test['segment_2019Q2']=\"T_\"+df_test['segment_2019Q2']\n",
    "    test_all_df=test_all_df.append(df_test)\n",
    "    control_all_df=control_all_df.append(df_control)\n",
    "    i_counter+=1\n",
    "    print(i_counter,datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dftotal\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q2/Investigation_of_differnt_versions/output_upto20190330_Decile_MergeEmail_Remove_2019-04-18/'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all_df.to_csv(folder_write+\"all_test.csv\",index=False)\n",
    "control_all_df.to_csv(folder_write+\"all_control.csv\",index=False)\n",
    "\n",
    "folder_write_inner = folder_write+'by_group/'\n",
    "try:\n",
    "    os.stat(folder_write_inner)\n",
    "except:\n",
    "    os.mkdir(folder_write_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 T_Quadrant III_P_H_2019Q2 2019-04-18 13:22:40.027737\n",
      "2 T_Quadrant III_P_L_2019Q2 2019-04-18 13:22:43.000501\n",
      "3 T_Quadrant III_P_M_2019Q2 2019-04-18 13:22:49.364981\n",
      "4 T_Quadrant III_S_H_2019Q2 2019-04-18 13:22:51.699709\n",
      "5 T_Quadrant III_S_L_2019Q2 2019-04-18 13:22:52.430964\n",
      "6 T_Quadrant III_S_M_2019Q2 2019-04-18 13:22:54.220080\n",
      "7 T_Quadrant III_T_H_2019Q2 2019-04-18 13:22:55.289936\n",
      "8 T_Quadrant III_T_L_2019Q2 2019-04-18 13:22:55.656271\n",
      "9 T_Quadrant III_T_M_2019Q2 2019-04-18 13:22:56.645103\n",
      "10 T_Quadrant II_P_H_2019Q2 2019-04-18 13:23:04.724115\n",
      "11 T_Quadrant II_P_L_2019Q2 2019-04-18 13:23:07.106268\n",
      "12 T_Quadrant II_P_M_2019Q2 2019-04-18 13:23:12.280964\n",
      "13 T_Quadrant II_S_H_2019Q2 2019-04-18 13:23:13.991405\n",
      "14 T_Quadrant II_S_L_2019Q2 2019-04-18 13:23:14.457246\n",
      "15 T_Quadrant II_S_M_2019Q2 2019-04-18 13:23:15.643045\n",
      "16 T_Quadrant II_T_H_2019Q2 2019-04-18 13:23:16.420959\n",
      "17 T_Quadrant II_T_L_2019Q2 2019-04-18 13:23:16.654190\n",
      "18 T_Quadrant II_T_M_2019Q2 2019-04-18 13:23:17.260092\n",
      "19 T_Quadrant IV_P_H_2019Q2 2019-04-18 13:23:26.035807\n",
      "20 T_Quadrant IV_P_L_2019Q2 2019-04-18 13:23:29.026305\n",
      "21 T_Quadrant IV_P_M_2019Q2 2019-04-18 13:23:35.204566\n",
      "22 T_Quadrant IV_S_H_2019Q2 2019-04-18 13:23:37.857058\n",
      "23 T_Quadrant IV_S_L_2019Q2 2019-04-18 13:23:38.668879\n",
      "24 T_Quadrant IV_S_M_2019Q2 2019-04-18 13:23:40.564041\n",
      "25 T_Quadrant IV_T_H_2019Q2 2019-04-18 13:23:41.796557\n",
      "26 T_Quadrant IV_T_L_2019Q2 2019-04-18 13:23:42.264426\n",
      "27 T_Quadrant IV_T_M_2019Q2 2019-04-18 13:23:43.420531\n",
      "28 T_Quadrant I_P_H_2019Q2 2019-04-18 13:24:09.756560\n",
      "29 T_Quadrant I_P_L_2019Q2 2019-04-18 13:24:17.370459\n",
      "30 T_Quadrant I_P_M_2019Q2 2019-04-18 13:24:34.623319\n",
      "31 T_Quadrant I_S_H_2019Q2 2019-04-18 13:24:41.630942\n",
      "32 T_Quadrant I_S_L_2019Q2 2019-04-18 13:24:43.593526\n",
      "33 T_Quadrant I_S_M_2019Q2 2019-04-18 13:24:48.265968\n",
      "34 T_Quadrant I_T_H_2019Q2 2019-04-18 13:24:51.196675\n",
      "35 T_Quadrant I_T_L_2019Q2 2019-04-18 13:24:52.196553\n",
      "36 T_Quadrant I_T_M_2019Q2 2019-04-18 13:24:54.585169\n"
     ]
    }
   ],
   "source": [
    "i_counter=0\n",
    "for seg,group in test_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 C_Quadrant III_P_H_2019Q2 2019-04-18 13:24:55.912765\n",
      "2 C_Quadrant III_P_L_2019Q2 2019-04-18 13:24:55.984570\n",
      "3 C_Quadrant III_P_M_2019Q2 2019-04-18 13:24:56.144131\n",
      "4 C_Quadrant III_S_H_2019Q2 2019-04-18 13:24:56.197887\n",
      "5 C_Quadrant III_S_L_2019Q2 2019-04-18 13:24:56.216406\n",
      "6 C_Quadrant III_S_M_2019Q2 2019-04-18 13:24:56.258639\n",
      "7 C_Quadrant III_T_H_2019Q2 2019-04-18 13:24:56.283318\n",
      "8 C_Quadrant III_T_L_2019Q2 2019-04-18 13:24:56.294117\n",
      "9 C_Quadrant III_T_M_2019Q2 2019-04-18 13:24:56.317809\n",
      "10 C_Quadrant II_P_H_2019Q2 2019-04-18 13:24:56.515946\n",
      "11 C_Quadrant II_P_L_2019Q2 2019-04-18 13:24:56.573293\n",
      "12 C_Quadrant II_P_M_2019Q2 2019-04-18 13:24:56.703064\n",
      "13 C_Quadrant II_S_H_2019Q2 2019-04-18 13:24:56.744206\n",
      "14 C_Quadrant II_S_L_2019Q2 2019-04-18 13:24:56.757557\n",
      "15 C_Quadrant II_S_M_2019Q2 2019-04-18 13:24:56.786794\n",
      "16 C_Quadrant II_T_H_2019Q2 2019-04-18 13:24:56.806610\n",
      "17 C_Quadrant II_T_L_2019Q2 2019-04-18 13:24:56.815051\n",
      "18 C_Quadrant II_T_M_2019Q2 2019-04-18 13:24:56.831517\n",
      "19 C_Quadrant IV_P_H_2019Q2 2019-04-18 13:24:57.064671\n",
      "20 C_Quadrant IV_P_L_2019Q2 2019-04-18 13:24:57.135937\n",
      "21 C_Quadrant IV_P_M_2019Q2 2019-04-18 13:24:57.290701\n",
      "22 C_Quadrant IV_S_H_2019Q2 2019-04-18 13:24:57.350244\n",
      "23 C_Quadrant IV_S_L_2019Q2 2019-04-18 13:24:57.370207\n",
      "24 C_Quadrant IV_S_M_2019Q2 2019-04-18 13:24:57.414141\n",
      "25 C_Quadrant IV_T_H_2019Q2 2019-04-18 13:24:57.441217\n",
      "26 C_Quadrant IV_T_L_2019Q2 2019-04-18 13:24:57.452706\n",
      "27 C_Quadrant IV_T_M_2019Q2 2019-04-18 13:24:57.476421\n",
      "28 C_Quadrant I_P_H_2019Q2 2019-04-18 13:24:58.316264\n",
      "29 C_Quadrant I_P_L_2019Q2 2019-04-18 13:24:58.521577\n",
      "30 C_Quadrant I_P_M_2019Q2 2019-04-18 13:24:59.054141\n",
      "31 C_Quadrant I_S_H_2019Q2 2019-04-18 13:24:59.224771\n",
      "32 C_Quadrant I_S_L_2019Q2 2019-04-18 13:24:59.272503\n",
      "33 C_Quadrant I_S_M_2019Q2 2019-04-18 13:24:59.389591\n",
      "34 C_Quadrant I_T_H_2019Q2 2019-04-18 13:24:59.459404\n",
      "35 C_Quadrant I_T_L_2019Q2 2019-04-18 13:24:59.485372\n",
      "36 C_Quadrant I_T_M_2019Q2 2019-04-18 13:24:59.547027\n"
     ]
    }
   ],
   "source": [
    "i_counter=0\n",
    "for seg,group in control_all_df.groupby(['segment_2019Q2']):\n",
    "    group=group[['customer_id_hashed','email_address_hash','segment_2019Q2']].rename(columns={\"segment_2019Q2\":\"segment\"})\n",
    "    group.to_csv(folder_write_inner+seg+\".csv\",index=False)\n",
    "    i_counter+=1\n",
    "    print(i_counter,seg,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1873409, 3)\n",
      "(1873954, 4)\n"
     ]
    }
   ],
   "source": [
    "lapsed_trans=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_sales_data/lapsed20140826_20170226/MediaStormLapsedCustDtl.txt\",\n",
    "                     sep=\",\",usecols=['customer_id_hashed'],dtype=str).drop_duplicates() # Doesn't go to score at all, so no need to read all columns\n",
    "lapsed_trans['lapsed_trans']=True\n",
    "\n",
    "lapsed_master=pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip',\n",
    "                     nrows = samplerows,dtype = 'str',sep = '|',\n",
    "                       usecols = ['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "\n",
    "lapsed_master=lapsed_master.drop_duplicates(\"customer_id_hashed\")\n",
    "print(lapsed_master.shape)\n",
    "\n",
    "lapsed_master=pd.merge(lapsed_master,lapsed_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "print(lapsed_master.shape)\n",
    "lapsed_master=lapsed_master[~pd.isnull(lapsed_master['email_address_hash'])]\n",
    "\n",
    "# remove the non-match email ids at the end and no calculation for the WD here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_18_plus_back_20160626=df_other_18_plus.groupby('segment')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832243, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(test_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['customer_id_hashed'].isin(control_all_df['customer_id_hashed'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(test_all_df['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(control_all_df['email_address_hash'])]\n",
    "\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "lapsed_master=lapsed_master[~lapsed_master['email_address_hash'].isin(df_other_18_plus['email_address_hash'])]\n",
    "\n",
    "lapsed_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_master=lapsed_master[['customer_id_hashed','email_address_hash']]\n",
    "lapsed_master['segment']=\"WalkingDead_2019Q2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lapsed_master.to_csv(folder_write_inner+\"WalkingDead_Group_before_20160626.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_test=test_all_df.groupby('segment_2019Q2')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_control=control_all_df.groupby('segment_2019Q2')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\",\"segment_2019Q2\":\"segment\"})\n",
    "summary_WD=lapsed_master.groupby('segment')['customer_id_hashed'].count().to_frame().reset_index().rename(columns={\"customer_id_hashed\":\"id_count\"})\n",
    "\n",
    "summary_overll=summary_test.append(summary_control).append(summary_WD).append(summary_18_plus_back_20160626)\n",
    "\n",
    "\n",
    "summary_overll.to_csv(folder_write_inner+\"test_control_groups_summary_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment</th>\n",
       "      <th>id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T_Quadrant III_P_H_2019Q2</td>\n",
       "      <td>1116280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T_Quadrant III_P_L_2019Q2</td>\n",
       "      <td>338861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T_Quadrant III_P_M_2019Q2</td>\n",
       "      <td>846723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T_Quadrant III_S_H_2019Q2</td>\n",
       "      <td>250381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T_Quadrant III_S_L_2019Q2</td>\n",
       "      <td>78687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T_Quadrant III_S_M_2019Q2</td>\n",
       "      <td>217277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T_Quadrant III_T_H_2019Q2</td>\n",
       "      <td>115058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T_Quadrant III_T_L_2019Q2</td>\n",
       "      <td>41947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T_Quadrant III_T_M_2019Q2</td>\n",
       "      <td>115850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T_Quadrant II_P_H_2019Q2</td>\n",
       "      <td>1032149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T_Quadrant II_P_L_2019Q2</td>\n",
       "      <td>266506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T_Quadrant II_P_M_2019Q2</td>\n",
       "      <td>694612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T_Quadrant II_S_H_2019Q2</td>\n",
       "      <td>186962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T_Quadrant II_S_L_2019Q2</td>\n",
       "      <td>52865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T_Quadrant II_S_M_2019Q2</td>\n",
       "      <td>146561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T_Quadrant II_T_H_2019Q2</td>\n",
       "      <td>89691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T_Quadrant II_T_L_2019Q2</td>\n",
       "      <td>29514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T_Quadrant II_T_M_2019Q2</td>\n",
       "      <td>76154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>T_Quadrant IV_P_H_2019Q2</td>\n",
       "      <td>1193125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>T_Quadrant IV_P_L_2019Q2</td>\n",
       "      <td>336533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>T_Quadrant IV_P_M_2019Q2</td>\n",
       "      <td>822073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>T_Quadrant IV_S_H_2019Q2</td>\n",
       "      <td>285578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>T_Quadrant IV_S_L_2019Q2</td>\n",
       "      <td>85396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>T_Quadrant IV_S_M_2019Q2</td>\n",
       "      <td>226863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>T_Quadrant IV_T_H_2019Q2</td>\n",
       "      <td>128402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>T_Quadrant IV_T_L_2019Q2</td>\n",
       "      <td>45417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>T_Quadrant IV_T_M_2019Q2</td>\n",
       "      <td>115949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>T_Quadrant I_P_H_2019Q2</td>\n",
       "      <td>3708311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>T_Quadrant I_P_L_2019Q2</td>\n",
       "      <td>895825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>T_Quadrant I_P_M_2019Q2</td>\n",
       "      <td>2439725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C_Quadrant III_T_M_2019Q2</td>\n",
       "      <td>3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C_Quadrant II_P_H_2019Q2</td>\n",
       "      <td>28082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C_Quadrant II_P_L_2019Q2</td>\n",
       "      <td>7251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C_Quadrant II_P_M_2019Q2</td>\n",
       "      <td>18899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C_Quadrant II_S_H_2019Q2</td>\n",
       "      <td>5087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C_Quadrant II_S_L_2019Q2</td>\n",
       "      <td>1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C_Quadrant II_S_M_2019Q2</td>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C_Quadrant II_T_H_2019Q2</td>\n",
       "      <td>2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C_Quadrant II_T_L_2019Q2</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C_Quadrant II_T_M_2019Q2</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C_Quadrant IV_P_H_2019Q2</td>\n",
       "      <td>32462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C_Quadrant IV_P_L_2019Q2</td>\n",
       "      <td>9156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>C_Quadrant IV_P_M_2019Q2</td>\n",
       "      <td>22367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C_Quadrant IV_S_H_2019Q2</td>\n",
       "      <td>7770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C_Quadrant IV_S_L_2019Q2</td>\n",
       "      <td>2323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C_Quadrant IV_S_M_2019Q2</td>\n",
       "      <td>6172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>C_Quadrant IV_T_H_2019Q2</td>\n",
       "      <td>3494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>C_Quadrant IV_T_L_2019Q2</td>\n",
       "      <td>1236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>C_Quadrant IV_T_M_2019Q2</td>\n",
       "      <td>3155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C_Quadrant I_P_H_2019Q2</td>\n",
       "      <td>100894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C_Quadrant I_P_L_2019Q2</td>\n",
       "      <td>24373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>C_Quadrant I_P_M_2019Q2</td>\n",
       "      <td>66379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>C_Quadrant I_S_H_2019Q2</td>\n",
       "      <td>21772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>C_Quadrant I_S_L_2019Q2</td>\n",
       "      <td>5818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>C_Quadrant I_S_M_2019Q2</td>\n",
       "      <td>16922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>C_Quadrant I_T_H_2019Q2</td>\n",
       "      <td>9493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>C_Quadrant I_T_L_2019Q2</td>\n",
       "      <td>2981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>C_Quadrant I_T_M_2019Q2</td>\n",
       "      <td>8259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WalkingDead_2019Q2</td>\n",
       "      <td>832243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18_months_plus_back_201606</td>\n",
       "      <td>5200288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       segment  id_count\n",
       "0    T_Quadrant III_P_H_2019Q2   1116280\n",
       "1    T_Quadrant III_P_L_2019Q2    338861\n",
       "2    T_Quadrant III_P_M_2019Q2    846723\n",
       "3    T_Quadrant III_S_H_2019Q2    250381\n",
       "4    T_Quadrant III_S_L_2019Q2     78687\n",
       "5    T_Quadrant III_S_M_2019Q2    217277\n",
       "6    T_Quadrant III_T_H_2019Q2    115058\n",
       "7    T_Quadrant III_T_L_2019Q2     41947\n",
       "8    T_Quadrant III_T_M_2019Q2    115850\n",
       "9     T_Quadrant II_P_H_2019Q2   1032149\n",
       "10    T_Quadrant II_P_L_2019Q2    266506\n",
       "11    T_Quadrant II_P_M_2019Q2    694612\n",
       "12    T_Quadrant II_S_H_2019Q2    186962\n",
       "13    T_Quadrant II_S_L_2019Q2     52865\n",
       "14    T_Quadrant II_S_M_2019Q2    146561\n",
       "15    T_Quadrant II_T_H_2019Q2     89691\n",
       "16    T_Quadrant II_T_L_2019Q2     29514\n",
       "17    T_Quadrant II_T_M_2019Q2     76154\n",
       "18    T_Quadrant IV_P_H_2019Q2   1193125\n",
       "19    T_Quadrant IV_P_L_2019Q2    336533\n",
       "20    T_Quadrant IV_P_M_2019Q2    822073\n",
       "21    T_Quadrant IV_S_H_2019Q2    285578\n",
       "22    T_Quadrant IV_S_L_2019Q2     85396\n",
       "23    T_Quadrant IV_S_M_2019Q2    226863\n",
       "24    T_Quadrant IV_T_H_2019Q2    128402\n",
       "25    T_Quadrant IV_T_L_2019Q2     45417\n",
       "26    T_Quadrant IV_T_M_2019Q2    115949\n",
       "27     T_Quadrant I_P_H_2019Q2   3708311\n",
       "28     T_Quadrant I_P_L_2019Q2    895825\n",
       "29     T_Quadrant I_P_M_2019Q2   2439725\n",
       "..                         ...       ...\n",
       "8    C_Quadrant III_T_M_2019Q2      3152\n",
       "9     C_Quadrant II_P_H_2019Q2     28082\n",
       "10    C_Quadrant II_P_L_2019Q2      7251\n",
       "11    C_Quadrant II_P_M_2019Q2     18899\n",
       "12    C_Quadrant II_S_H_2019Q2      5087\n",
       "13    C_Quadrant II_S_L_2019Q2      1438\n",
       "14    C_Quadrant II_S_M_2019Q2      3988\n",
       "15    C_Quadrant II_T_H_2019Q2      2440\n",
       "16    C_Quadrant II_T_L_2019Q2       803\n",
       "17    C_Quadrant II_T_M_2019Q2      2072\n",
       "18    C_Quadrant IV_P_H_2019Q2     32462\n",
       "19    C_Quadrant IV_P_L_2019Q2      9156\n",
       "20    C_Quadrant IV_P_M_2019Q2     22367\n",
       "21    C_Quadrant IV_S_H_2019Q2      7770\n",
       "22    C_Quadrant IV_S_L_2019Q2      2323\n",
       "23    C_Quadrant IV_S_M_2019Q2      6172\n",
       "24    C_Quadrant IV_T_H_2019Q2      3494\n",
       "25    C_Quadrant IV_T_L_2019Q2      1236\n",
       "26    C_Quadrant IV_T_M_2019Q2      3155\n",
       "27     C_Quadrant I_P_H_2019Q2    100894\n",
       "28     C_Quadrant I_P_L_2019Q2     24373\n",
       "29     C_Quadrant I_P_M_2019Q2     66379\n",
       "30     C_Quadrant I_S_H_2019Q2     21772\n",
       "31     C_Quadrant I_S_L_2019Q2      5818\n",
       "32     C_Quadrant I_S_M_2019Q2     16922\n",
       "33     C_Quadrant I_T_H_2019Q2      9493\n",
       "34     C_Quadrant I_T_L_2019Q2      2981\n",
       "35     C_Quadrant I_T_M_2019Q2      8259\n",
       "0           WalkingDead_2019Q2    832243\n",
       "0   18_months_plus_back_201606   5200288\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_overll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
