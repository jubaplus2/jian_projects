{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start celery\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d12e193e674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# In[7]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdfsales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'combinedsales'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mlastweeksdate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsalespath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jian/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jian/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jian/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jian/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jian/.local/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m    680\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start celery\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import paramiko\n",
    "import glob\n",
    "import logging\n",
    "import gc\n",
    "import smtplib\n",
    "folderpath = '/home/jian/BiglotsCode/outputs/'\n",
    "lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+10))\n",
    "\n",
    "Tuesday_StampDate_Str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1))\n",
    "Tuesday_today_str =Tuesday_StampDate_Str[0:4]+Tuesday_StampDate_Str[5:7]+Tuesday_StampDate_Str[8:10]\n",
    "thisweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3))\n",
    "\n",
    "today_str=str(datetime.datetime.now().date())\n",
    "logging.basicConfig(filename='celery.log', level=logging.INFO)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "recent_weekly_data_folder=\"/home/jian/BigLots/MediaStorm_\"+thisweeksdate+\"/\"\n",
    "Simeng_recent_weekly_data_folder=\"/home/simeng/outputs_\"+thisweeksdate+\"/\"\n",
    "\n",
    "Saturday_str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3-7))\n",
    "\n",
    "if not os.path.exists(\"/home/jian/BiglotsCode/outputs/Output_\"+thisweeksdate+\"/By_Zip_weather_forecast_for_Saturday_\"+Saturday_str+\".csv\"):\n",
    "    del Saturday_str\n",
    "\n",
    "    try:\n",
    "        os.stat(recent_weekly_data_folder)\n",
    "    except:\n",
    "        os.mkdir(recent_weekly_data_folder)\n",
    "\n",
    "    try:\n",
    "        os.stat(Simeng_recent_weekly_data_folder)\n",
    "    except:\n",
    "        os.mkdir(Simeng_recent_weekly_data_folder)\n",
    "\n",
    "    logging.info(str(datetime.datetime.now())+\": Start Running\")\n",
    "    # In[3]:\n",
    "\n",
    "    host = \"64.237.51.251\" #hard-coded\n",
    "    port = 22\n",
    "    transport = paramiko.Transport((host, port))\n",
    "\n",
    "    password = \"bwRi3V6fgZsfJrMl\" #hard-coded\n",
    "    username = \"client\" #hard-coded\n",
    "    transport.connect(username = username, password = password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "\n",
    "    Client_Today_STR=str(datetime.datetime.now().date())\n",
    "    Client_Today_NUM_STR =Client_Today_STR[0:4]+Client_Today_STR[5:7]+Client_Today_STR[8:10]\n",
    "\n",
    "    new_weekly_file_list=sftp.listdir(\"/mnt/drv5/biglots_data/\")\n",
    "    new_weekly_file_list=[\"/mnt/drv5/biglots_data/\"+x for x in new_weekly_file_list if Client_Today_NUM_STR in x]\n",
    "\n",
    "    for new_weekly_file in new_weekly_file_list:\n",
    "        localpath=recent_weekly_data_folder+new_weekly_file.split(\"/\")[len(new_weekly_file.split(\"/\"))-1]\n",
    "        try:\n",
    "            os.stat(localpath)\n",
    "        except:\n",
    "            sftp.get(new_weekly_file,localpath)\n",
    "\n",
    "    sftp.close()\n",
    "    transport.close()\n",
    "    logging.info(str(datetime.datetime.now())+\": Finished copying\")\n",
    "    # In[4]:\n",
    "\n",
    "    newsalespath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormSalesWeekly' in x][0]\n",
    "    newtrafficpath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormTrafficWeekly' in x][0]\n",
    "    newinventorypath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormInventoryWeekly' in x][0]\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "    closed_onlinestorelist = ['6990','145']\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "    df_tradearea_all = pd.read_csv('/home/jian/BiglotsCode/OtherInput/New_TA_info.csv',dtype = 'str')\n",
    "    df_tradearea_all['trade_area_code']=df_tradearea_all['Ta_Info'].apply(lambda x: x.split(\" | \")[0])\n",
    "    df_tradearea_all=df_tradearea_all[['location_id','trade_area_code']]\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "    dfsales = pd.read_csv(folderpath + 'combinedsales'+ lastweeksdate + '.csv',sep = '|',dtype = 'str')\n",
    "    df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "    a = df.columns\n",
    "    print(\"new sales data column header matches:\")\n",
    "    print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "           'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "    logging.info(\"new sales data column header matches:\")\n",
    "    logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "           'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "    '''print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "           'gross_transaction_cnt', 'class_code_id', 'class_gross_sales_amt'])'''\n",
    "    df['subclass_gross_sales_amt']=df['subclass_gross_sales_amt'].astype(float)\n",
    "    df=df.groupby(['location_id','week_end_dt','fiscal_week_nbr','gross_sales_amt','gross_transaction_cnt','class_code_id'])['subclass_gross_sales_amt'].sum().to_frame().reset_index()\n",
    "    df=df.rename(columns={\"subclass_gross_sales_amt\":\"class_gross_sales_amt\"})\n",
    "\n",
    "\n",
    "\n",
    "    dfsales = dfsales.append(df,ignore_index = True)\n",
    "    a = (len(dfsales.index))\n",
    "    dfsales = dfsales.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "           'class_code_id'])\n",
    "    b = (len(dfsales.index))\n",
    "    if a==b:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"last week traffic data duplication deduped\")\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "    recentweek = str(max(dfsales['week_end_dt']).split(\" \")[0])\n",
    "    recentweek\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "    lastweeksdate\n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "    # dfsales.to_csv(folderpath + 'combinedsales'+ recentweek + '.csv',index = False,sep = '|')\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "    dfsales = dfsales[~dfsales['location_id'].isin(closed_onlinestorelist)]\n",
    "\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "    outputpath = folderpath +'Output_' + recentweek +'/'\n",
    "    try:\n",
    "        os.stat(outputpath)\n",
    "    except:\n",
    "        os.mkdir(outputpath)\n",
    "\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "    dfnodata = dfsales[(dfsales['class_gross_sales_amt'] == '?')&                   (dfsales['week_end_dt'] == recentweek)]\n",
    "    # dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "    print(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))\n",
    "    logging.info(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))\n",
    "\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "    dfsales['week_end_dt'] = pd.to_datetime(dfsales['week_end_dt'])\n",
    "    dfsales = dfsales[dfsales['class_gross_sales_amt']!='?']\n",
    "    dfsales = dfsales.reset_index(drop = True)\n",
    "\n",
    "    dfsales['gross_sales_amt'] = dfsales['gross_sales_amt'].astype('float')\n",
    "    dfsales['gross_transaction_cnt'] = dfsales['gross_transaction_cnt'].astype('float')\n",
    "    dfsales['class_gross_sales_amt'] = dfsales['class_gross_sales_amt'].astype('float')\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "    dfweeklist = dfsales[['week_end_dt','fiscal_week_nbr']].drop_duplicates()\n",
    "    dfweeklist = dfweeklist.sort_values('week_end_dt',ascending = False)\n",
    "    dfweeklist.reset_index(drop = True,inplace = True)\n",
    "    dfweeklist.reset_index(inplace = True)\n",
    "\n",
    "    dfweeklist_wow = dfweeklist.copy()\n",
    "    dfweeklist_wow['index'] = dfweeklist_wow['index'] - 1\n",
    "    dfweeklist_wow = dfweeklist_wow[['index','week_end_dt']].rename(columns={\"week_end_dt\":\"weeklastweek\"})\n",
    "    dfweeklist_wow.columns = ['index','weeklastweek']\n",
    "\n",
    "    dfweeklist = dfweeklist[dfweeklist['index']<104]\n",
    "    dfweeklist.reset_index(drop = True,inplace = True)\n",
    "    dfweeklist['year'] = np.ceil((dfweeklist['index'] + 1)/52)\n",
    "\n",
    "    dfweeklist1 = dfweeklist[dfweeklist['year'] == 1]\n",
    "    dfweeklist1 = dfweeklist1[['index', 'week_end_dt', 'fiscal_week_nbr']]\n",
    "    dfweeklist2 = dfweeklist[dfweeklist['year'] == 2]\n",
    "    dfweeklist2 = dfweeklist2[['week_end_dt', 'fiscal_week_nbr']]\n",
    "    dfweeklist2.columns = ['weeklastyear', 'fiscal_week_nbr']\n",
    "    dfweeklist1['rank']=dfweeklist1['week_end_dt'].rank(ascending=True)\n",
    "    dfweeklist2['rank']=dfweeklist2['weeklastyear'].rank(ascending=True)\n",
    "\n",
    "    del dfweeklist2['fiscal_week_nbr']\n",
    "\n",
    "    dfweeklist = pd.merge(dfweeklist1,dfweeklist2,on ='rank' )\n",
    "    dfweeklist = pd.merge(dfweeklist,dfweeklist_wow,on ='index')\n",
    "    del dfweeklist1,dfweeklist2,dfweeklist_wow\n",
    "\n",
    "    dfweeklist.to_csv(outputpath + 'weeklist.csv',index = False)\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "    recentweek_date = max(dfsales['week_end_dt'])\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "    dfcheck = dfsales[dfsales['week_end_dt'] == recentweek_date]\n",
    "\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "    dfcheck_total1 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                             'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "    a = (len(dfcheck_total1.index))\n",
    "    dfcheck_total1 = dfcheck_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "    b = (len(dfcheck_total1.index))\n",
    "    if a==b:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"last week sales multiple gross sales/trasaction in the same store\")\n",
    "\n",
    "    dfcheck_total2 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                             'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "    dfcheck_total2.reset_index(inplace = True)\n",
    "\n",
    "    dfcheck_total = pd.merge(dfcheck_total1,dfcheck_total2,\n",
    "                            on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                            how = 'outer')\n",
    "\n",
    "    del dfcheck_total1,dfcheck_total2\n",
    "\n",
    "    dfcheck_zero = dfcheck_total[(dfcheck_total['class_gross_sales_amt']<=0)|                            (dfcheck_total['gross_transaction_cnt']<=0) ]\n",
    "\n",
    "    dfcheck_zero.to_csv(outputpath + 'zerosales.csv',index = False)\n",
    "    print(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "    logging.info(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "\n",
    "\n",
    "    del dfcheck_zero\n",
    "\n",
    "    dfcheck_total['TotalDiff'] = dfcheck_total['gross_sales_amt']-dfcheck_total['class_gross_sales_amt']\n",
    "    dfcheck_total['TotalDiff'] = dfcheck_total['TotalDiff'].round()\n",
    "    dfcheck_totalnonmatch = dfcheck_total[dfcheck_total['TotalDiff']!=0]\n",
    "    print(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "    logging.info(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "\n",
    "    dfcheck_totalnonmatch.to_csv(outputpath + 'totalnonmatch.csv',index = False)\n",
    "    del dfcheck_totalnonmatch\n",
    "\n",
    "    dfcheck_zeroclass = dfcheck[(dfcheck['class_gross_sales_amt']==0)]\n",
    "    dfcheck_zeroclass.to_csv(outputpath + 'zeroclasssales.csv',index = False)\n",
    "    print(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "    logging.info(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "    del dfcheck_zeroclass\n",
    "    del dfcheck\n",
    "\n",
    "\n",
    "    # In[20]:\n",
    "\n",
    "    dfsales_total1 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr']].drop_duplicates()\n",
    "    # dfsales_total1 = dfsales_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "    dfsales_total2 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                             'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "    dfsales_total2.reset_index(inplace = True)\n",
    "\n",
    "    dfsales_total = pd.merge(dfsales_total1,dfsales_total2,\n",
    "                            on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                            how = 'outer')\n",
    "    del dfsales_total1,dfsales_total2\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "\n",
    "    list_store_files=glob.glob('/home/jian/BigLots/static_files/Store_list/*.txt')\n",
    "    list_store_files.reverse()\n",
    "    list_store_files.append(\"/home/jian/BigLots/static_files/MediaStormStoreList_Nov15.txt\")\n",
    "    dfstore=pd.DataFrame()\n",
    "    for file in list_store_files:\n",
    "        df=pd.read_table(file,sep = '|',dtype = 'str')\n",
    "        if len(dfstore)==0:\n",
    "            dfstore=dfstore.append(df)\n",
    "        else:\n",
    "            df=df[~df['location_id'].isin(dfstore['location_id'].tolist())]\n",
    "            dfstore=dfstore.append(df)\n",
    "        dfstore=dfstore.drop_duplicates()\n",
    "\n",
    "    dfstore['open_dt'] = pd.to_datetime(dfstore['open_dt'])\n",
    "    dfstore['open_dtwd'] = dfstore['open_dt'].dt.dayofweek\n",
    "    dfstore['open_wk'] = np.where(dfstore['open_dtwd']<=5,\n",
    "                           dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(5-x.weekday()))),\n",
    "                           dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(12-x.weekday()))))\n",
    "\n",
    "    dma = pd.read_csv('/home/jian/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')\n",
    "    dfstore_exc = dfstore\n",
    "    dfstore_exc['zip_cd'] = dfstore_exc['zip_cd'].str[0:5]\n",
    "    dfstore_exc = pd.merge(dfstore_exc,dma,on = 'zip_cd',how = 'left')\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "    dfstorematch = dfsales_total[['location_id']].drop_duplicates()\n",
    "    dfstorematch = pd.merge(dfstorematch,dfstore[['location_id','address_line_1']],\n",
    "                            on = 'location_id',how = 'left')\n",
    "    dfstorematch['address_line_1'].fillna('empty',inplace = True)\n",
    "    dfstorematch = dfstorematch[dfstorematch['address_line_1']=='empty']\n",
    "    print(\"stores w/o detailed info: \")\n",
    "    print(dfstorematch['location_id'].unique())\n",
    "    logging.info(\"stores w/o detailed info: \")\n",
    "    logging.info(dfstorematch['location_id'].unique())\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "    len(dfstorematch['location_id'].unique())\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "    old_stores=pd.read_excel(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",sheet_name=\"old_stores\",dtype=str)\n",
    "    new_stores=pd.read_excel(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",sheet_name=\"new_stores\",dtype=str)\n",
    "\n",
    "\n",
    "    last_week_closed_stores=old_stores['closed_store'].tolist()+new_stores['location_id'].tolist()\n",
    "\n",
    "\n",
    "    sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores)\n",
    "    logging.info(\"No New Stores: \"+ str(sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores)))\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "    this_week_new_store_list=[x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores]\n",
    "    logging.info(\"New Stores:\")\n",
    "    logging.info([x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores])\n",
    "\n",
    "    new_stores_df=pd.DataFrame({\"location_id\":this_week_new_store_list,\"first_week_in_data\":[recentweek_date]*len(this_week_new_store_list)},index=[x for x in range(len(this_week_new_store_list))])\n",
    "    new_stores=new_stores.append(new_stores_df)\n",
    "\n",
    "    writer=pd.ExcelWriter(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",engine='xlsxwriter')\n",
    "    old_stores.to_excel(writer,'old_stores',index=False)\n",
    "    new_stores.to_excel(writer,'new_stores',index=False)\n",
    "    writer.save()\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[26]:\n",
    "\n",
    "    len(dfstorematch['location_id'].unique())\n",
    "    del dfstorematch\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "    dfsales_total = pd.merge(dfsales_total,dfstore[['location_id','open_wk']],\n",
    "                            on = 'location_id',how = 'left')\n",
    "    dfsales_total['open_wk'].fillna(datetime.datetime.strptime(str(20300101), '%Y%m%d').date(),inplace = True)\n",
    "\n",
    "\n",
    "    # In[28]:\n",
    "\n",
    "    dftraffic = pd.read_csv(folderpath + 'combinedtraffic'+ lastweeksdate + '.csv',\n",
    "                   sep = '|',dtype = 'str')\n",
    "\n",
    "    df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "    a = df.columns\n",
    "    print(\"new traffic data column header matches:\")\n",
    "    print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "           'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "           'traffic_day_6', 'traffic_day_7'])\n",
    "    logging.info(\"new traffic data column header matches:\")\n",
    "    logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "           'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "           'traffic_day_6', 'traffic_day_7'])\n",
    "\n",
    "    dftraffic = dftraffic.append(df,ignore_index = True)\n",
    "    a = (len(dftraffic.index))\n",
    "    dftraffic = dftraffic.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "    b = (len(dftraffic.index))\n",
    "    if a==b:\n",
    "        print(\"\")\n",
    "        logging.info(\"\")\n",
    "    else:\n",
    "        print(\"last week traffic data duplication deduped\")\n",
    "        logging.info(\"last week traffic data duplication deduped\")\n",
    "    # dftraffic.to_csv(folderpath + 'combinedtraffic'+ recentweek + '.csv',index = False,sep = '|')\n",
    "\n",
    "    dftraffic['traffic_week'] = 0 \n",
    "    for i in ['traffic_day_1','traffic_day_2', 'traffic_day_3', 'traffic_day_4',\n",
    "              'traffic_day_5', 'traffic_day_6', 'traffic_day_7']:\n",
    "        dftraffic[i] = dftraffic[i].astype('float')\n",
    "        dftraffic['traffic_week'] = dftraffic['traffic_week'] +dftraffic[i]\n",
    "    dftraffic['week_end_dt'] = pd.to_datetime(dftraffic['week_end_dt'])\n",
    "\n",
    "\n",
    "    # In[29]:\n",
    "\n",
    "    dfinventory = pd.read_csv(folderpath + 'combinedinventory'+ lastweeksdate + '.csv',\n",
    "                   sep = '|',dtype = 'str')\n",
    "\n",
    "    df = pd.read_csv(newinventorypath,sep = '|',dtype = 'str')\n",
    "    a = df.columns\n",
    "    print(\"new inventory data column header matches:\")\n",
    "    logging.info(\"new inventory data column header matches:\")\n",
    "    print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "    logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "    df.columns = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand']\n",
    "    dfinventory = dfinventory.append(df,ignore_index = True)\n",
    "    a = (len(dfinventory.index))\n",
    "    dfinventory = dfinventory.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id'])\n",
    "    b = (len(dfinventory.index))\n",
    "    if a==b:\n",
    "        print(\"\")\n",
    "        logging.info(\"\")\n",
    "    else:\n",
    "        print(\"last week inventory data duplication deduped\")\n",
    "        logging.info(\"last week inventory data duplication deduped\")\n",
    "\n",
    "\n",
    "    # In[30]:\n",
    "\n",
    "    # dfinventory.to_csv(folderpath + 'combinedinventory'+ recentweek + '.csv',index = False,sep = '|')\n",
    "\n",
    "\n",
    "    # In[31]:\n",
    "\n",
    "    dfinventory['week_end_dt'] = pd.to_datetime(dfinventory['week_end_dt'])\n",
    "    dfinventory['on_hand'] = dfinventory['on_hand'].astype('float')\n",
    "    dfinventory_total = dfinventory.groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "    dfinventory_total.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "    # In[32]:\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total,\n",
    "                                    dfweeklist[['week_end_dt', 'fiscal_week_nbr','weeklastyear','weeklastweek']],\n",
    "                                    on= ['week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "    dfsales_total_lastyear = pd.merge(dfsales_total,\n",
    "                                     dfweeklist[['weeklastyear']],\n",
    "                                     left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "    dfsales_total_lastyear = dfsales_total_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "    dfsales_total_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastyear,\n",
    "                                    on = ['location_id','weeklastyear'],how = 'outer')\n",
    "    dfsales_total_recent.fillna(0,inplace = True)\n",
    "\n",
    "    dict_store_open_date=dfstore.set_index(\"location_id\").to_dict()['open_wk']\n",
    "    def add_open_dt_to_store(x):\n",
    "        if x in dict_store_open_date.keys():\n",
    "            y=dict_store_open_date[x]\n",
    "        else:\n",
    "            y=datetime.date(2030,1,1)\n",
    "        return y\n",
    "    dfsales_total_recent['open_wk']=dfsales_total_recent['location_id'].apply(lambda x: add_open_dt_to_store(x))\n",
    "\n",
    "\n",
    "    dfsales_total_recent['Store_Category'] = np.where(dfsales_total_recent['open_wk']>=dfsales_total_recent['weeklastyear'],'New',\n",
    "                                             np.where((dfsales_total_recent['gross_transaction_cnt_ly']==0)&(dfsales_total_recent['class_gross_sales_amt_ly']==0),\n",
    "                                             'Converted',\n",
    "                                             np.where((dfsales_total_recent['gross_transaction_cnt']==0)&(dfsales_total_recent['class_gross_sales_amt']==0),\n",
    "                                            'Converted','Complete')))\n",
    "\n",
    "    dfsales_total_lastweek = pd.merge(dfsales_total,\n",
    "                                     dfweeklist[['weeklastweek']],\n",
    "                                     left_on= 'week_end_dt',right_on = 'weeklastweek')\n",
    "\n",
    "    dfsales_total_lastweek = dfsales_total_lastweek[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastweek']]\n",
    "    dfsales_total_lastweek.columns = ['location_id','gross_transaction_cnt_lw', 'class_gross_sales_amt_lw','weeklastweek']\n",
    "\n",
    "    dfsales_total_recent['week_end_dt']=dfsales_total_recent['weeklastyear'].apply(lambda x: x+datetime.timedelta(days=52*7))\n",
    "    dfsales_total_recent['weeklastweek']=dfsales_total_recent['weeklastyear'].apply(lambda x: x+datetime.timedelta(days=51*7))\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastweek,\n",
    "                                    on = ['location_id','weeklastweek'],how = 'left')\n",
    "    dfsales_total_recent.fillna(0,inplace = True)\n",
    "\n",
    "    dfsales_total_recent['week_end_dt'] = np.where(dfsales_total_recent['week_end_dt']=='1970-01-01',\n",
    "                                           dfsales_total_recent['weeklastyear'] + pd.DateOffset(364),\n",
    "                                           dfsales_total_recent['week_end_dt'])\n",
    "    dfsales_total_recent['weeklastyear'] = np.where(dfsales_total_recent['weeklastyear']=='1970-01-01',\n",
    "                                           dfsales_total_recent['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                           dfsales_total_recent['weeklastyear'])\n",
    "\n",
    "\n",
    "    # In[33]:\n",
    "\n",
    "    dfallstorelist = dfstore[~dfstore['location_id'].isin(closed_onlinestorelist)]\n",
    "    dfallstorelist.reset_index(drop = True, inplace = True)\n",
    "    dfweeklist2 = dfweeklist.copy()\n",
    "    dfallstorelist['concat'] = 1\n",
    "    dfweeklist2['concat'] = 1\n",
    "    dfallstorelist = pd.merge(dfallstorelist,dfweeklist2,on='concat')\n",
    "    dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "    dfallstorelist = pd.merge(dfallstorelist,dfsales_total_recent,on=['location_id','week_end_dt'],\n",
    "                             how = 'left')\n",
    "\n",
    "    dfallstorelist.fillna(0,inplace = True)\n",
    "    dfallstorelist = dfallstorelist[(dfallstorelist['gross_sales_amt']==0)&                               (dfallstorelist['gross_transaction_cnt']==0)&                               (dfallstorelist['class_gross_sales_amt']==0)&                               (dfallstorelist['gross_transaction_cnt_ly']==0)&                               (dfallstorelist['class_gross_sales_amt_ly']==0)]\n",
    "    dfallstorelist = dfallstorelist.sort_values(['week_end_dt','location_id'],ascending = [0,1])\n",
    "    dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "\n",
    "    x_temp=dfsales_total_recent[['week_end_dt','fiscal_week_nbr']]\n",
    "    y_temp=x_temp.drop_duplicates()\n",
    "    y_temp=y_temp[y_temp['fiscal_week_nbr']!=\"0\"]\n",
    "    y_temp=y_temp[y_temp['fiscal_week_nbr']!=0]\n",
    "    dfallstorelist = pd.merge(dfallstorelist,y_temp,on=\"week_end_dt\",how=\"left\")\n",
    "    dfallstorelist = pd.merge(dfallstorelist,dfstore_exc,on=\"location_id\",how='left')\n",
    "    dfallstorelist = pd.merge(dfallstorelist,df_tradearea_all,on=\"location_id\",how='left')\n",
    "    del dfallstorelist['open_dtwd']\n",
    "    del dfallstorelist['open_wk']\n",
    "\n",
    "    # dfallstorelist.to_csv(outputpath + 'nobothyeardatastores.csv',index = False)\n",
    "    print(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "    logging.info(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "\n",
    "    test = dfallstorelist[dfallstorelist['week_end_dt']==recentweek_date]\n",
    "    print(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "    logging.info(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "\n",
    "    # del test,dfweeklist2,dfallstorelist\n",
    "\n",
    "\n",
    "    # For later use to add index\n",
    "\n",
    "    Recent_52_Week_nbr=dfweeklist[['week_end_dt','index']]\n",
    "    Recent_52_Week_nbr['52_Weeks_nbr']=52-Recent_52_Week_nbr['index']\n",
    "    del Recent_52_Week_nbr['index']\n",
    "\n",
    "    # Recent_52_Week_nbr\n",
    "\n",
    "\n",
    "    # In[34]:\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic,\n",
    "                                    on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "    dftraffic2 = dftraffic[['location_id', 'week_end_dt','traffic_day_1',\n",
    "           'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "           'traffic_day_6', 'traffic_day_7','traffic_week']]\n",
    "    dftraffic2.columns = ['location_id', 'weeklastyear','traffic_day_1_ly',\n",
    "           'traffic_day_2_ly', 'traffic_day_3_ly', 'traffic_day_4_ly', 'traffic_day_5_ly',\n",
    "           'traffic_day_6_ly', 'traffic_day_7_ly','traffic_week_ly']\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic2,\n",
    "                                    on=['location_id', 'weeklastyear'],how = 'left')\n",
    "    del dftraffic2\n",
    "\n",
    "\n",
    "    # In[35]:\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total,\n",
    "                                    on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "    dfinventory_total2 = dfinventory_total[['location_id', 'week_end_dt','on_hand']]\n",
    "    dfinventory_total2.columns = ['location_id', 'weeklastyear','on_hand_ly']\n",
    "\n",
    "    dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total2,\n",
    "                                    on=['location_id', 'weeklastyear'],how = 'left')\n",
    "    del dfinventory_total2\n",
    "\n",
    "\n",
    "    # In[36]:\n",
    "\n",
    "    recentweek_last=datetime.datetime.strptime(recentweek, '%Y-%m-%d').date()\n",
    "    recentweek_last=recentweek_last+datetime.timedelta(days=(-84))\n",
    "\n",
    "\n",
    "    # In[37]:\n",
    "\n",
    "    dfsales_total_recent['yoysales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_ly'] - 1\n",
    "    dfsales_total_recent['yoytrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_ly'] - 1\n",
    "    dfsales_total_recent['wowsales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_lw'] - 1\n",
    "    dfsales_total_recent['wowtrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_lw'] - 1\n",
    "\n",
    "\n",
    "    # In[38]:\n",
    "\n",
    "    dfsales_total_recent_delete = dfsales_total_recent[(dfsales_total_recent['Store_Category']=='Complete')&                                                   ((abs(dfsales_total_recent['yoysales'])>0.2)&                                                   (abs(dfsales_total_recent['yoytrans'])>0.2))]#|\\\n",
    "                                                       #(abs(dfsales_total_recent['wowsales'])>0.2)|\\\n",
    "                                                       #(abs(dfsales_total_recent['wowtrans'])>0.2))]\n",
    "    dfsales_total_recent_delete = dfsales_total_recent_delete.sort_values(['week_end_dt','location_id'],\n",
    "                                                                         ascending = [0,1])\n",
    "\n",
    "    dfsales_total_recent_delete=pd.merge(dfsales_total_recent_delete,dfstore_exc,on=\"location_id\",how='left')\n",
    "    dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,df_tradearea_all,on='location_id',how='left')\n",
    "\n",
    "    del dfsales_total_recent_delete['open_dtwd']\n",
    "    del dfsales_total_recent_delete['open_wk_y']\n",
    "    dfsales_total_recent_delete.rename(index=str, columns={\"open_wk_x\": \"open_wk\"})\n",
    "\n",
    "    dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "    # dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores.csv',index = False)\n",
    "    print(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "    logging.info(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "    test = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "    print(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "    logging.info(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "    del test\n",
    "\n",
    "\n",
    "    # In[39]:\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "    #dfsales_total_recent =  dfsales_total_recent[(dfsales_total_recent['gross_transaction_cnt']!=0)|\\\n",
    "    #                                             (dfsales_total_recent['class_gross_sales_amt']!=0)]\n",
    "    dfsales_total_recent = dfsales_total_recent[(dfsales_total_recent['Store_Category']!='Complete')| ((dfsales_total_recent['Store_Category']=='Complete')&(abs(dfsales_total_recent['yoysales'])<=0.2)|(abs(dfsales_total_recent['yoytrans'])<=0.2))]\n",
    "                                                #(abs(dfsales_total_recent['wowsales'])<=0.2)&\\\n",
    "                                                #(abs(dfsales_total_recent['wowtrans'])<=0.2))]\n",
    "\n",
    "\n",
    "    # In[41]:\n",
    "\n",
    "    dfweeklist2 = dfweeklist[['week_end_dt']]\n",
    "    dfweeklist2['week_end_dt_8w'] = dfweeklist2['week_end_dt']+pd.DateOffset(-84)\n",
    "    # Name \"week_end_dt_8w\" reflects 12 weeks, not 8\n",
    "\n",
    "\n",
    "    # In[42]:\n",
    "\n",
    "    dfweeklist_12plus = dfsales[['week_end_dt']].drop_duplicates()\n",
    "    dfweeklist_12plus = dfweeklist_12plus.sort_values('week_end_dt',ascending = False)\n",
    "    dfweeklist_12plus.reset_index(drop = True,inplace = True)\n",
    "    dfweeklist_12plus.reset_index(inplace = True)\n",
    "    dfweeklist_12plus = dfweeklist_12plus[dfweeklist_12plus['index']<64]\n",
    "    dfweeklist_12plus['weeklastyear'] = dfweeklist_12plus['week_end_dt'] + pd.DateOffset(-364)\n",
    "\n",
    "\n",
    "    # In[43]:\n",
    "\n",
    "    dfsales_12plus = pd.merge(dfsales_total,dfweeklist_12plus,on= ['week_end_dt'])\n",
    "\n",
    "    dfsales_12plus_lastyear = pd.merge(dfsales_total,\n",
    "                                     dfweeklist_12plus[['weeklastyear']],\n",
    "                                     left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "    dfsales_12plus_lastyear = dfsales_12plus_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "    dfsales_12plus_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "    dfsales_12plus = pd.merge(dfsales_12plus,dfsales_12plus_lastyear,\n",
    "                                    on = ['location_id','weeklastyear'],how = 'left')\n",
    "    dfsales_12plus.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
