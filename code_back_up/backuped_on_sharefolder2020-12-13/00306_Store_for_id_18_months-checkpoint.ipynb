{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapsed Start on: 2018-04-15\n",
      "Active Start on: 2018-10-14\n",
      "Store Allocation Starting on: 2018-04-15\n",
      "Last Saturday: 2019-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mapping of each id & associated store first\n",
    "\n",
    "# upto 20191012\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import hashlib\n",
    "import gc\n",
    "import glob\n",
    "logging.basicConfig(filename='BL_CRM_LR_Quarterly_upto20191012_2019Q4_Store_per_ID_'+str(datetime.datetime.now().date())+'.log', level=logging.INFO)\n",
    "logging.info('Started')\n",
    "\n",
    "samplerows = None\n",
    "\n",
    "lastdate_date = datetime.date(2019,10,12) # Recent Saturday\n",
    "active_Sunday = str(lastdate_date-datetime.timedelta(days=52*7-1))\n",
    "\n",
    "lapsed_Sunday = str(lastdate_date-datetime.timedelta(days=52*7*1.5-1))\n",
    "Beginning_18_months_ago=str(lastdate_date-datetime.timedelta(days=52*7*1.5-1))\n",
    "\n",
    "lastdate=str(lastdate_date)\n",
    "print(\"Lapsed Start on: \"+lapsed_Sunday) #>=\n",
    "print(\"Active Start on: \"+active_Sunday) #>=\n",
    "print(\"Store Allocation Starting on: \"+Beginning_18_months_ago) #>=\n",
    "print(\"Last Saturday: \"+lastdate) #<=\n",
    "\n",
    "def recursive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "folder_write = '/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_2019Q4/output_'+str(datetime.datetime.now().date())+'/'\n",
    "try:\n",
    "    os.stat(folder_write)\n",
    "except:\n",
    "    os.mkdir(folder_write)\n",
    "    \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "historical_subclass_files=glob.glob(\"/home/jian/BigLots/hist_daily_data_subclasslevel/*.txt\")\n",
    "historical_item_files=glob.glob(\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/*.txt\")\n",
    "\n",
    "daily_item_files_2019=list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "daily_item_files_2019=[x for x in daily_item_files_2019 if x[-4:]==\".txt\" and \"daily\" in x.lower()]\n",
    "\n",
    "daily_subclass_files_2018=list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\"))\n",
    "daily_subclass_files_2018=[x for x in daily_subclass_files_2018 if x[-4:]==\".txt\" and \"daily\" in x.lower()]\n",
    "\n",
    "historical_subclass_files.sort()\n",
    "historical_item_files.sort()\n",
    "daily_item_files_2019.sort()\n",
    "daily_subclass_files_2018.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf=pd.read_table(historical_subclass_files[-1],dtype=str,sep=\"|\",usecols=[\\'transaction_dt\\'])\\nprint(\"max_date_subclass: \"+str(df[\\'transaction_dt\\'].max()))\\ndel df\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 1: subclass 2018-04-15 to 2018-06-09\n",
    "historical_subclass_files=[x for x in historical_subclass_files if x.split(\"_week_ending_\")[1][:10]>=lapsed_Sunday]\n",
    "'''\n",
    "df=pd.read_table(historical_subclass_files[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_subclass: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# historical subclass -- min_date_subclass: 2018-04-15\n",
    "\n",
    "\n",
    "'''\n",
    "df=pd.read_table(historical_subclass_files[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_subclass: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# historical subclass -- max_date_subclass: 2018-06-09\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf=pd.read_table(daily_subclass_files_2018[-1],dtype=str,sep=\"|\",usecols=[\\'transaction_dt\\'])\\nprint(\"max_date_daily_subclass: \"+str(df[\\'transaction_dt\\'].max()))\\ndel df\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 2: subclass 2018-06-10 to 2018-06-09\n",
    "'''\n",
    "df=pd.read_table(historical_item_files[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_historical_item: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# historical item -- min_date_historical_item: 2018-08-05\n",
    "daily_subclass_files_2018=[x for x in daily_subclass_files_2018 if x.split(\"eks/MediaStorm_\")[1][:10]>\"2018-06-09\"]\n",
    "daily_subclass_files_2018=[x for x in daily_subclass_files_2018 if x.split(\"eks/MediaStorm_\")[1][:10]<\"2018-08-05\"]\n",
    "\n",
    "\n",
    "\n",
    "# QC the daily subclass 2018 data range\n",
    "'''\n",
    "df=pd.read_table(daily_subclass_files_2018[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_daily_subclass: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# min_date_daily_subclass: 2018-06-10\n",
    "\n",
    "'''\n",
    "df=pd.read_table(daily_subclass_files_2018[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_daily_subclass: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# max_date_daily_subclass: 2018-08-04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf=pd.read_table(historical_item_files[-1],dtype=str,sep=\"|\",usecols=[\\'transaction_dt\\'])\\nprint(\"max_date_historical_item: \"+str(df[\\'transaction_dt\\'].max()))\\ndel df\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 3: item 2018-08-05 to 2019-02-09\n",
    "'''\n",
    "df=pd.read_table(historical_item_files[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_historical_item: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# min_date_historical_item: 2018-08-05\n",
    "\n",
    "'''\n",
    "df=pd.read_table(historical_item_files[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_historical_item: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# max_date_historical_item: 2019-02-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf=pd.read_table(daily_item_files_2019[-1],dtype=str,sep=\"|\",usecols=[\\'transaction_dt\\'])\\nprint(\"max_date_daily_item: \"+str(df[\\'transaction_dt\\'].max()))\\ndel df\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Piece 4: item 2019-02-10 to 2019-10-12\n",
    "\n",
    "daily_item_files_2019=[x for x in daily_item_files_2019 if x.split(\"_weeks/MediaStorm_\")[1][:10]>=\"2019-02-10\"]\n",
    "\n",
    "\n",
    "#QC\n",
    "'''\n",
    "df=pd.read_table(daily_item_files_2019[0],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"min_date_daily_item: \"+str(df['transaction_dt'].min()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# min_date_daily_item: 2019-02-10\n",
    "\n",
    "'''\n",
    "df=pd.read_table(daily_item_files_2019[-1],dtype=str,sep=\"|\",usecols=['transaction_dt'])\n",
    "print(\"max_date_daily_item: \"+str(df['transaction_dt'].max()))\n",
    "del df\n",
    "gc.collect()\n",
    "'''\n",
    "# max_date_daily_item: 2019-10-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_all_files=historical_subclass_files+daily_subclass_files_2018+historical_item_files+daily_item_files_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2019-10-20 15:29:58.234552\n",
      "11 2019-10-20 16:03:09.065981\n",
      "21 2019-10-20 16:26:37.555403\n",
      "31 2019-10-20 16:43:29.142892\n",
      "41 2019-10-20 17:44:13.451456\n",
      "51 2019-10-20 18:21:58.480528\n",
      "61 2019-10-20 18:34:36.216344\n",
      "71 2019-10-20 18:47:10.666020\n"
     ]
    }
   ],
   "source": [
    "# removed 6990\n",
    "list_df_transactions_18_months=[]\n",
    "i_counter=0\n",
    "for file in list_all_files:\n",
    "    try:\n",
    "        df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "        df=df.rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "    except:\n",
    "        df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','subclass_transaction_amt'])\n",
    "        df=df.rename(columns={\"subclass_transaction_amt\":\"sales\"})\n",
    "    df=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df=df[df['location_id']!=\"6990\"]\n",
    "    df['sales']=df['sales'].astype(float)\n",
    "    \n",
    "    df_sales=df.groupby(['customer_id_hashed','location_id'])['sales'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_trans=df[['customer_id_hashed','transaction_dt','transaction_id','location_id']].drop_duplicates()\n",
    "    df_trans['trans']=1\n",
    "    df_trans=df_trans.groupby(['customer_id_hashed','location_id'])['trans'].sum().to_frame().reset_index()\n",
    "    df=pd.merge(df_sales,df_trans,on=[\"customer_id_hashed\",'location_id'],how=\"outer\")\n",
    "\n",
    "    list_df_transactions_18_months.append(df)\n",
    "    i_counter+=1\n",
    "    if i_counter%10==1:\n",
    "        print(i_counter,datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_by_id_store_18=pd.concat(list_df_transactions_18_months)\n",
    "del list_df_transactions_18_months\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96816062, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_by_id_store_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>location_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000358fbc9f7162c1acbfc88f1211ebec1245400631f1...</td>\n",
       "      <td>5296</td>\n",
       "      <td>44.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00004eb78a416ee5bf936f1fdb9d8504ea4694dae3aa95...</td>\n",
       "      <td>1385</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed location_id  sales  trans\n",
       "0  0000358fbc9f7162c1acbfc88f1211ebec1245400631f1...        5296   44.4      2\n",
       "1  00004eb78a416ee5bf936f1fdb9d8504ea4694dae3aa95...        1385    3.0      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_by_id_store_18.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sales_by_id_store_18.to_csv(folder_write+\"/BL_trans_by_id_store_detail_JL_20180415_20191012_JL_\"+str(datetime.datetime.now())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96816062, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_sales_by_id_store_18.shape)\n",
    "df_sales_by_id_store_18=df_sales_by_id_store_18.groupby(['customer_id_hashed','location_id'])['sales','trans'].sum().reset_index()\n",
    "df_sales_by_id_store_18=df_sales_by_id_store_18.sort_values([\"customer_id_hashed\",\"trans\",\"sales\"],ascending=[True,False,False])\n",
    "\n",
    "df_sales_by_id_store_18.to_csv(folder_write+\"/BL_trans_by_id_store_agg_JL_20180415_20191012_JL_\"+str(datetime.datetime.now())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21589087, 4)\n"
     ]
    }
   ],
   "source": [
    "df_sales_by_id_store_18=df_sales_by_id_store_18.drop_duplicates(\"customer_id_hashed\")\n",
    "print(df_sales_by_id_store_18.shape)\n",
    "df_sales_by_id_store_18.to_csv(folder_write+\"/BL_allocated_store_by_id_shoppers_18_months_JL_20180415_20191012_JL_\"+str(datetime.datetime.now())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
