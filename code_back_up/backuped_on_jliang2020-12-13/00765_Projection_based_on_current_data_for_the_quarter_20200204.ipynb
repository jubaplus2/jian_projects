{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-04 19:21:05.529127\n",
      "/home/jian/Projects/Big_Lots/Analysis/2019_Q4/Planner_Request/JT_non_rewards_sales_so_far_2019Q4_4_weeks\n",
      "ty_start_date: 2019-11-03\n",
      "ty_end_date: 2020-02-01\n",
      "ly_start_date: 2018-11-04\n",
      "ly_end_date: 2019-02-02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "print(datetime.datetime.now())\n",
    "print(os.getcwd())\n",
    "\n",
    "ty_start_date=datetime.date(2019,11,3)\n",
    "ty_end_date=datetime.date(2020,2,1)\n",
    "\n",
    "ly_start_date=ty_start_date-datetime.timedelta(days=52*7)\n",
    "ly_end_date=ty_end_date-datetime.timedelta(days=52*7)\n",
    "\n",
    "\n",
    "print(\"ty_start_date:\",ty_start_date)\n",
    "print(\"ty_end_date:\",ty_end_date)\n",
    "\n",
    "print(\"ly_start_date:\",ly_start_date)\n",
    "print(\"ly_end_date:\",ly_end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def recursive_file_gen(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root,file)\n",
    "list_files_ly=glob.glob(\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/*.txt\")\n",
    "list_files_ly=[x for x in list_files_ly if x.split(\"/MediaStormDailySalesHistory\")[1][:8]>=str(ly_start_date).replace(\"-\",\"\")]\n",
    "list_files_ly=[x for x in list_files_ly if x.split(\"/MediaStormDailySalesHistory\")[1][:8]<=str(ly_end_date).replace(\"-\",\"\")]\n",
    "list_files_ly.sort()\n",
    "print(len(list_files_ly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "list_files_ty=recursive_file_gen(\"/home/jian/BigLots/\")\n",
    "list_files_ty=[x for x in list_files_ty if \"dailysales\" in x.lower()]\n",
    "list_files_ty=[x for x in list_files_ty if \"/MediaStorm_\" in x]\n",
    "list_files_ty=[x for x in list_files_ty if x.split(\"/MediaStorm_\")[1][:10]>=str(ty_start_date)]\n",
    "list_files_ty=[x for x in list_files_ty if x.split(\"/MediaStorm_\")[1][:10]<=str(ty_end_date)]\n",
    "list_files_ty.sort()\n",
    "print(len(list_files_ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-09/MediaStormDailySales20191112-115210-002.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-16/MediaStormDailySales20191119-112232-478.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-23/MediaStormDailySales20191126-112901-552.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-11-30/MediaStormDailySales20191203.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-07/MediaStormDailySales.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-14/MediaStormDailySales20191217-195625-000.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-21/MediaStormDailySales20191226-122746-000.txt',\n",
       " '/home/jian/BigLots/2019_by_weeks/MediaStorm_2019-12-28/MediaStormDailySales20191231-112945-515.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-04/MediaStormDailySales20200107-112859-015.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-11/MediaStormDailySales20200114-115009-140.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-18/MediaStormDailySales20200121-111749-649.txt',\n",
       " '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-25/MediaStormDailySales20200128-111758-074.txt',\n",
       " '/home/jian/BigLots/MediaStorm_2020-02-01/MediaStormDailySales20200204-111741-091.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files_ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>division_id</th>\n",
       "      <th>department_id</th>\n",
       "      <th>class_code_id</th>\n",
       "      <th>subclass_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>11001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>11001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   division_id  department_id class_code_id subclass_id\n",
       "0            1            108         11001           2\n",
       "1            1            108         11001           4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_taxo=pd.read_table(\"/home/jian/BigLots/static_files/ProductTaxonomy/MediaStormProductTaxonomy20200101-135600-916.txt\",\n",
    "                              dtype=str,sep=\"|\")\n",
    "df_product_taxo=df_product_taxo[['division_id','department_id','class_code_id','subclass_id']].drop_duplicates()\n",
    "df_product_taxo['division_id']=df_product_taxo['division_id'].astype(int)\n",
    "df_product_taxo['department_id']=df_product_taxo['department_id'].astype(int)\n",
    "df_division_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Division Names.txt\",sep=\"|\")\n",
    "df_department_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",sep=\"|\")\n",
    "\n",
    "df_product_taxo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-48d9755ed049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mweek_num\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     df=pd.read_table(file,dtype=str,sep=\"|\",\n\u001b[0;32m---> 18\u001b[0;31m                     usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','class_code_id','subclass_id','item_transaction_amt'])\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_transaction_amt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_transaction_amt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_product_taxo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_code_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'subclass_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_summary_InWeek_ly=pd.DataFrame()\n",
    "df_summary_CumQ_ly=pd.DataFrame()\n",
    "\n",
    "df_rewards_ids_ly=pd.DataFrame()\n",
    "df_division_output_ly=pd.DataFrame()\n",
    "df_department_output_ly=pd.DataFrame()\n",
    "\n",
    "cum_R_sales=0\n",
    "cum_R_trans=0\n",
    "\n",
    "cum_N_sales=0\n",
    "cum_N_trans=0\n",
    "\n",
    "week_num=0\n",
    "for file in list_files_ly:\n",
    "    week_num+=1\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",\n",
    "                    usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','class_code_id','subclass_id','item_transaction_amt'])\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=pd.merge(df,df_product_taxo,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "    df['division_id']=df['division_id'].fillna(\"nan\")\n",
    "    df['department_id']=df['department_id'].fillna(\"nan\")\n",
    "    \n",
    "    date_min=df['transaction_dt'].min()\n",
    "    date_max=df['transaction_dt'].max()\n",
    "    \n",
    "    df['rewards_label']=np.where(pd.notnull(df['customer_id_hashed']),\"Rewards\",\"Non_Rewards\")\n",
    "    \n",
    "    \n",
    "    ######\n",
    "    # division view from department, no change of the order below\n",
    "    df_department_sales=df.groupby(['rewards_label','division_id','department_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_division_sales=df_department_sales.groupby(['rewards_label','division_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_division_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','rewards_label']].drop_duplicates()\n",
    "    df_division_trans['trans']=1\n",
    "    df_division_trans=df_division_trans.groupby(['rewards_label','division_id'])['trans'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_department_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','department_id','rewards_label']].drop_duplicates()\n",
    "    df_department_trans['trans']=1\n",
    "    df_department_trans=df_department_trans.groupby(['rewards_label','division_id','department_id'])['trans'].sum().to_frame().reset_index()\n",
    "\n",
    "    df_division=pd.merge(df_division_sales,df_division_trans,on=['rewards_label','division_id'],how=\"outer\")\n",
    "    df_department=pd.merge(df_department_sales,df_department_trans,on=['rewards_label','division_id','department_id'],how=\"outer\")\n",
    "    \n",
    "    df_division['week_num']=week_num\n",
    "    df_division['week_start']=date_min\n",
    "    df_division['week_end']=date_max\n",
    "\n",
    "    df_department['week_num']=week_num\n",
    "    df_department['week_start']=date_min\n",
    "    df_department['week_end']=date_max    \n",
    "    ######\n",
    "    df_division_output_ly=df_division_output_ly.append(df_division)\n",
    "    df_department_output_ly=df_department_output_ly.append(df_department)\n",
    "    \n",
    "    \n",
    "    df_rewards=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df_nonrewards=df[pd.isnull(df['customer_id_hashed'])]\n",
    "    \n",
    "    num_R_sales=df_rewards['item_transaction_amt'].sum()\n",
    "    num_R_trans=df_rewards[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates().shape[0]\n",
    "    num_R_shoppers=df_rewards['customer_id_hashed'].nunique()\n",
    "    \n",
    "    num_N_sales=df_nonrewards['item_transaction_amt'].sum()\n",
    "    num_N_trans=df_nonrewards[['location_id','transaction_dt','transaction_id']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_rewards_shoppers=df_rewards[['customer_id_hashed']].drop_duplicates()\n",
    "    df_rewards_shoppers['week_start']=date_min\n",
    "    df_rewards_shoppers['week_end']=date_max\n",
    "    df_rewards_ids_ly=df_rewards_ids_ly.append(df_rewards_shoppers)\n",
    "    \n",
    "    cum_R_sales+=num_R_sales\n",
    "    cum_R_trans+=num_R_trans\n",
    "    cum_N_sales+=num_N_sales\n",
    "    cum_N_trans+=num_N_trans\n",
    "    cum_R_shoppers=df_rewards_ids_ly['customer_id_hashed'].nunique()\n",
    "    \n",
    "    df_in_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":num_R_sales,\"Rewards_trans\":num_R_trans,\"Rewards_shoppers\":num_R_shoppers,\n",
    "                             'NonRewards_sales':num_N_sales,\"NonRewards_trans\":num_N_trans,\n",
    "                            },index=[0])\n",
    "    \n",
    "    df_cum_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":cum_R_sales,\"Rewards_trans\":cum_R_trans,\"Rewards_shoppers\":cum_R_shoppers,\n",
    "                             'NonRewards_sales':cum_N_sales,\"NonRewards_trans\":cum_N_trans,\n",
    "                            },index=[0])\n",
    "\n",
    "    df_summary_InWeek_ly=df_summary_InWeek_ly.append(df_in_week)\n",
    "    df_summary_CumQ_ly=df_summary_CumQ_ly.append(df_cum_week)\n",
    "    print(datetime.datetime.now(),week_num,\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_InWeek_ty=pd.DataFrame()\n",
    "df_summary_CumQ_ty=pd.DataFrame()\n",
    "\n",
    "df_rewards_ids_ty=pd.DataFrame()\n",
    "df_division_output_ty=pd.DataFrame()\n",
    "df_department_output_ty=pd.DataFrame()\n",
    "\n",
    "cum_R_sales=0\n",
    "cum_R_trans=0\n",
    "\n",
    "cum_N_sales=0\n",
    "cum_N_trans=0\n",
    "\n",
    "week_num=0\n",
    "for file in list_files_ty:\n",
    "    week_num+=1\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",\n",
    "                    usecols=['location_id','transaction_dt','transaction_id','customer_id_hashed','class_code_id','subclass_id','item_transaction_amt'])\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=pd.merge(df,df_product_taxo,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "    df['division_id']=df['division_id'].fillna(\"nan\")\n",
    "    df['department_id']=df['department_id'].fillna(\"nan\")\n",
    "    \n",
    "    date_min=df['transaction_dt'].min()\n",
    "    date_max=df['transaction_dt'].max()\n",
    "    \n",
    "    df['rewards_label']=np.where(pd.notnull(df['customer_id_hashed']),\"Rewards\",\"Non_Rewards\")\n",
    "    \n",
    "    \n",
    "    ######\n",
    "    # division view from department, no change of the order below\n",
    "    df_department_sales=df.groupby(['rewards_label','division_id','department_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_division_sales=df_department_sales.groupby(['rewards_label','division_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_division_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','rewards_label']].drop_duplicates()\n",
    "    df_division_trans['trans']=1\n",
    "    df_division_trans=df_division_trans.groupby(['rewards_label','division_id'])['trans'].sum().to_frame().reset_index()\n",
    "    \n",
    "    df_department_trans=df[['location_id','transaction_dt','transaction_id','customer_id_hashed','division_id','department_id','rewards_label']].drop_duplicates()\n",
    "    df_department_trans['trans']=1\n",
    "    df_department_trans=df_department_trans.groupby(['rewards_label','division_id','department_id'])['trans'].sum().to_frame().reset_index()\n",
    "\n",
    "    df_division=pd.merge(df_division_sales,df_division_trans,on=['rewards_label','division_id'],how=\"outer\")\n",
    "    df_department=pd.merge(df_department_sales,df_department_trans,on=['rewards_label','division_id','department_id'],how=\"outer\")\n",
    "    \n",
    "    df_division['week_num']=week_num\n",
    "    df_division['week_start']=date_min\n",
    "    df_division['week_end']=date_max\n",
    "\n",
    "    df_department['week_num']=week_num\n",
    "    df_department['week_start']=date_min\n",
    "    df_department['week_end']=date_max    \n",
    "    ######\n",
    "    df_division_output_ty=df_division_output_ty.append(df_division)\n",
    "    df_department_output_ty=df_department_output_ty.append(df_department)\n",
    "    \n",
    "    \n",
    "    df_rewards=df[pd.notnull(df['customer_id_hashed'])]\n",
    "    df_nonrewards=df[pd.isnull(df['customer_id_hashed'])]\n",
    "    \n",
    "    num_R_sales=df_rewards['item_transaction_amt'].sum()\n",
    "    num_R_trans=df_rewards[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates().shape[0]\n",
    "    num_R_shoppers=df_rewards['customer_id_hashed'].nunique()\n",
    "    \n",
    "    num_N_sales=df_nonrewards['item_transaction_amt'].sum()\n",
    "    num_N_trans=df_nonrewards[['location_id','transaction_dt','transaction_id']].drop_duplicates().shape[0]\n",
    "    \n",
    "    df_rewards_shoppers=df_rewards[['customer_id_hashed']].drop_duplicates()\n",
    "    df_rewards_shoppers['week_start']=date_min\n",
    "    df_rewards_shoppers['week_end']=date_max\n",
    "    df_rewards_ids_ty=df_rewards_ids_ty.append(df_rewards_shoppers)\n",
    "    \n",
    "    cum_R_sales+=num_R_sales\n",
    "    cum_R_trans+=num_R_trans\n",
    "    cum_N_sales+=num_N_sales\n",
    "    cum_N_trans+=num_N_trans\n",
    "    cum_R_shoppers=df_rewards_ids_ty['customer_id_hashed'].nunique()\n",
    "    \n",
    "    df_in_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":num_R_sales,\"Rewards_trans\":num_R_trans,\"Rewards_shoppers\":num_R_shoppers,\n",
    "                             'NonRewards_sales':num_N_sales,\"NonRewards_trans\":num_N_trans,\n",
    "                            },index=[0])\n",
    "    \n",
    "    df_cum_week=pd.DataFrame({\"week_num\":week_num,\"week_start\":date_min,\"week_end\":date_max,\n",
    "                             \"Rewards_sales\":cum_R_sales,\"Rewards_trans\":cum_R_trans,\"Rewards_shoppers\":cum_R_shoppers,\n",
    "                             'NonRewards_sales':cum_N_sales,\"NonRewards_trans\":cum_N_trans,\n",
    "                            },index=[0])\n",
    "\n",
    "    \n",
    "    df_summary_InWeek_ty=df_summary_InWeek_ty.append(df_in_week)\n",
    "    df_summary_CumQ_ty=df_summary_CumQ_ty.append(df_cum_week)\n",
    "    print(datetime.datetime.now(),week_num,\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_InWeek_ty=df_summary_InWeek_ty[df_summary_InWeek_ly.columns.tolist()]\n",
    "\n",
    "writer=pd.ExcelWriter(\"./BL_sales_summary_in_quarter_11weeks_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "df_summary_InWeek_ty.to_excel(writer,\"df_summary_InWeek_ty\",index=False)\n",
    "df_summary_CumQ_ty.to_excel(writer,\"df_summary_CumQ_ty\",index=False)\n",
    "df_division_output_ty.to_excel(writer,\"division_output_ty\",index=False)\n",
    "df_department_output_ty.to_excel(writer,\"department_output_ty\",index=False)\n",
    "\n",
    "df_summary_InWeek_ly.to_excel(writer,\"df_summary_InWeek_ly\",index=False)\n",
    "df_summary_CumQ_ly.to_excel(writer,\"df_summary_CumQ_ly\",index=False)\n",
    "df_division_output_ly.to_excel(writer,\"division_output_ly\",index=False)\n",
    "df_department_output_ly.to_excel(writer,\"department_output_ly\",index=False)\n",
    "\n",
    "df_division_name.to_excel(writer,\"df_division_name\",index=False)\n",
    "df_department_name.to_excel(writer,\"df_department_name\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards_ids_ty.to_csv(\"./df_rewards_ids_ty_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "df_rewards_ids_ly.to_csv(\"./df_rewards_ids_ly_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
