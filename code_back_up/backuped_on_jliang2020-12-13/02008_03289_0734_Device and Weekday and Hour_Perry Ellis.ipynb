{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_date = \"2017-09-04\"\n",
    "start_date = \"2017-07-18\"\n",
    "xls_path='/Users/JayLiang/Desktop/Media Storm/PEI/Perry Ellis/Auto Run Device Weekday and Hour/Device_Weekday_Hour_From_'+str(start_date)+'_To_'+str(end_date)+'.xlsx'\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 6\n",
    "import os\n",
    "import argparse\n",
    "import httplib2\n",
    "import pprint\n",
    "import time\n",
    "import datetime\n",
    "from io import StringIO\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from oauth2client import GOOGLE_TOKEN_URI\n",
    "from oauth2client.client import OAuth2Credentials\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "Group = pd.read_csv(\"/Users/JayLiang/Desktop/Media Storm/PEI/Perry Ellis/Auto Run Device Weekday and Hour/Group.csv\")\n",
    "Group['floodlightActivity'] = Group['Floodlight activity']\n",
    "del Group['Floodlight activity']\n",
    "\n",
    "def create_credentials():\n",
    "    \"\"\"Create Google OAuth2 credentials.\n",
    "\n",
    "    Args:\n",
    "        client_id: Client id of a Google Cloud console project.\n",
    "        client_secret: Client secret of a Google Cloud console project.\n",
    "        refresh_token: A refresh token authorizing the Google Cloud console project\n",
    "          to access the DS data of some Google user.\n",
    "    Returns:\n",
    "        OAuth2Credentials\n",
    "    \"\"\"\n",
    "    return OAuth2Credentials(access_token=None,\n",
    "                           client_id='549790627766-qnth4m8qvuimg87pnsp4b82lhte7dk5a.apps.googleusercontent.com',\n",
    "                           client_secret='Vta4lQLOL49vVYvktkcPGRNb',\n",
    "                           refresh_token='1/ab7pCGMu3K5AveG0UOUpQ0J08vCp6uM357O8qmoPDMs',\n",
    "                           token_expiry=None,\n",
    "                           token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "                           user_agent=None)\n",
    "\n",
    "def get_service(credentials):\n",
    "    \"\"\"Set up a new DoubleClick Search service.\n",
    "\n",
    "    Args:\n",
    "        credentials: An OAuth2Credentials generated with create_credentials, or\n",
    "        flows in the oatuh2client.client package.\n",
    "    Returns:\n",
    "        An authorized Doubleclicksearch serivce.\n",
    "    \"\"\"\n",
    "    # Use the authorize() function of OAuth2Credentials to apply necessary credential\n",
    "    # headers to all requests.\n",
    "    http = credentials.authorize(http = httplib2.Http())\n",
    "\n",
    "    # Construct the service object for the interacting with the DoubleClick Search API.\n",
    "    service = build('doubleclicksearch', 'v2', http=http)\n",
    "    return service\n",
    "\n",
    "def poll_report(service, report_id):\n",
    "    \"\"\"Poll the API with the reportId until the report is ready, up to ten times.\n",
    "\n",
    "    Args:\n",
    "        service: An authorized Doublelcicksearch service.\n",
    "        report_id: The ID DS has assigned to a report.\n",
    "    Returns:\n",
    "        pd.DataFrame, report file\n",
    "    \"\"\"\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            request = service.reports().get(reportId=report_id)\n",
    "            json_data = request.execute()\n",
    "            if json_data['isReportReady']:\n",
    "                pprint.pprint('The report is ready.')\n",
    "\n",
    "                # For large reports, DS automatically fragments the report into multiple\n",
    "                # files. The 'files' property in the JSON object that DS returns contains\n",
    "                # the list of URLs for file fragment. To download a report, DS needs to\n",
    "                # know the report ID and the index of a file fragment.\n",
    "                report = pd.DataFrame()\n",
    "                for i in range(len(json_data['files'])):\n",
    "                    pprint.pprint('Downloading fragment ' + str(i) + ' for report ' + report_id)\n",
    "                    report = report.append(download_files(service, report_id, str(i)), ignore_index = True) # See Download the report.\n",
    "                return report\n",
    "\n",
    "            else:\n",
    "                pprint.pprint('Report is not ready. I will try again.')\n",
    "                time.sleep(10)\n",
    "        except HttpError as e:\n",
    "            error = simplejson.loads(e.content)['error']['errors'][0]\n",
    "\n",
    "            # See Response Codes\n",
    "            pprint.pprint('HTTP code %d, reason %s' % (e.resp.status, error['reason']))\n",
    "            break\n",
    "        \n",
    "def download_files(service, report_id, report_fragment):\n",
    "    \"\"\"Generate and print sample report.\n",
    "\n",
    "    Args:\n",
    "        service: An authorized Doublelcicksearch service.\n",
    "        report_id: The ID DS has assigned to a report.\n",
    "        report_fragment: The 0-based index of the file fragment from the files array.\n",
    "    Returns:\n",
    "        pd.DataFrame report file\n",
    "    \"\"\"\n",
    "    request = service.reports().getFile(reportId=report_id, reportFragment=report_fragment)\n",
    "    return pd.read_csv(StringIO(request.execute().decode('utf-8')))\n",
    "\n",
    "def request_report(service, start_date, end_date, columns):\n",
    "    \"\"\"Request sample report and print the report ID that DS returns. See Set Up Your Application.\n",
    "\n",
    "    Args:\n",
    "        service: An authorized Doublelcicksearch service.\n",
    "        columns: list of columns will be in the report\n",
    "    Returns:\n",
    "        The report id.\n",
    "    \"\"\"\n",
    "    request = service.reports().request(\n",
    "        body={\n",
    "                \"reportScope\": {\n",
    "                    \"agencyId\": \"20100000000000932\",\n",
    "                    \"advertiserId\": \"21700000001423776\"\n",
    "                },\n",
    "                \"reportType\": \"account\",\n",
    "                \"columns\": [{'columnName': column} for column in columns],   \n",
    "                \"timeRange\" : {\n",
    "                    \"startDate\" : start_date,\n",
    "                    \"endDate\" : end_date\n",
    "                    },\n",
    "                \n",
    "                #\"filters\": [\n",
    "                #    {\n",
    "                #        \"column\" : { \"columnName\": \"keywordLabels\" },\n",
    "                #        \"operator\" : \"containsElement\",\n",
    "                #        \"values\" : [\"JubaNovTest\",]\n",
    "                #    }\n",
    "                #],\n",
    "                \n",
    "                \"downloadFormat\": \"csv\",\n",
    "                \"maxRowsPerFile\": 100000000,\n",
    "                \"statisticsCurrency\": \"agency\",\n",
    "                \"verifySingleTimeZone\": \"false\",\n",
    "                \"includeRemovedEntities\": \"false\"\n",
    "            }\n",
    "    )\n",
    "    json_data = request.execute()\n",
    "    return json_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-18\n",
      "2017-09-04\n",
      "'Report is not ready. I will try again.'\n",
      "'The report is ready.'\n",
      "'Downloading fragment 0 for report AAAnTEU9mrlRJVY3'\n",
      "'The report is ready.'\n",
      "'Downloading fragment 0 for report AAAnf4esM8nxi-JW'\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "creds = create_credentials()\n",
    "\n",
    "service = get_service(creds)\n",
    "\n",
    "\n",
    "print(start_date), print(end_date)\n",
    "REPORTID_nonHVA = request_report(service, start_date, end_date, \n",
    "                                 ['accountType', 'deviceSegment', 'clicks', 'cost', 'impr','dfaRevenue', 'dfaTransactions'])\n",
    "REPORTID_HVA = request_report(service, start_date, end_date, \n",
    "                              ['accountType', 'deviceSegment',\n",
    "                               'floodlightActivity', 'dfaActions'])\n",
    "non_hva_device= poll_report(service, REPORTID_nonHVA)\n",
    "hva_device = poll_report(service, REPORTID_HVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Device_1=non_hva_device\n",
    "Device_2=hva_device\n",
    "\n",
    "Device_2_Group_New=pd.merge(Device_2,Group,on=\"floodlightActivity\")\n",
    "len(pd.unique(Device_2_Group_New.floodlightActivity)) #53\n",
    "\n",
    "Device_2_Group_3HVA=Device_2_Group_New.groupby(['deviceSegment','accountType','Group Name'])[['dfaActions']].sum() \n",
    "type(Device_2_Group_3HVA)\n",
    "Device_2_Group_3HVA.reset_index(inplace = True)\n",
    "Device_2_Group_3HVA_Wide=Device_2_Group_3HVA.pivot_table(index=['deviceSegment','accountType'],columns=['Group Name'],values=['dfaActions'])\n",
    "\n",
    "Device_2_Group_floodlightActivity=Device_2_Group_New\n",
    "del Device_2_Group_floodlightActivity['Group Name']\n",
    "Device_2_Group_53Activities=Device_2_Group_floodlightActivity.pivot_table(index=['deviceSegment','accountType'],columns=['floodlightActivity'],values=['dfaActions'])\n",
    "Device_2_Group_3HVA_Wide.columns =Device_2_Group_3HVA_Wide.columns.get_level_values(1)\n",
    "Device_2_Group_3HVA_Wide.reset_index(inplace=True)\n",
    "Device_2_Group_53Activities.columns =Device_2_Group_53Activities.columns.get_level_values(1)\n",
    "Device_2_Group_53Activities.reset_index(inplace=True)\n",
    "\n",
    "Merge_1_Device=pd.merge(Device_1,Device_2_Group_3HVA_Wide,on=[\"deviceSegment\",\"accountType\"])\n",
    "Merge_2_Device=pd.merge(Merge_1_Device,Device_2_Group_53Activities,on=[\"deviceSegment\",\"accountType\"])\n",
    "Merge_2_Device['Conversions']=np.sum(Merge_2_Device.loc[:,['Perry Ellis - HVA 1','Perry Ellis - HVA 2','Perry Ellis - HVA 3']],axis=1)\n",
    "Merge_2_Device['ROI']=Merge_2_Device['dfaRevenue']/Merge_2_Device['cost']\n",
    "Merge_2_Device['Click Through Rate']=Merge_2_Device['clicks']/Merge_2_Device['impr']\n",
    "Merge_2_Device['Cost per Transaction']=Merge_2_Device['cost']/Merge_2_Device['dfaTransactions']  \n",
    "Merge_2_Device['Cost per Click']=Merge_2_Device['cost']/Merge_2_Device['clicks']\n",
    "Merge_2_Device['Cost per HVA (total)']=Merge_2_Device['cost']/Merge_2_Device['Conversions']\n",
    "Merge_2_Device['Cost per HVA 1']=Merge_2_Device['cost']/Merge_2_Device['Perry Ellis - HVA 1']\n",
    "Merge_2_Device['Cost per HVA 2']=Merge_2_Device['cost']/Merge_2_Device['Perry Ellis - HVA 2']\n",
    "Merge_2_Device['Cost per HVA 3']=Merge_2_Device['cost']/Merge_2_Device['Perry Ellis - HVA 3']\n",
    "\n",
    "cols = Merge_2_Device.columns.tolist()\n",
    "cols_new=cols[:2]+['ROI']+cols[2:7]+['Conversions']+cols[-7:]+cols[7:-9]\n",
    "Merge_2_Device_Final=Merge_2_Device[cols_new]\n",
    "Merge_2_Device_Final=Merge_2_Device_Final.sort_values(by=['accountType','ROI'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-18\n",
      "2017-09-04\n",
      "'Report is not ready. I will try again.'\n",
      "'The report is ready.'\n",
      "'Downloading fragment 0 for report AAAnr-tNbFrmxsfK'\n",
      "'The report is ready.'\n",
      "'Downloading fragment 0 for report AAAnr6TarOafa4Aw'\n"
     ]
    }
   ],
   "source": [
    "# Weekday\n",
    "creds = create_credentials()\n",
    "\n",
    "service = get_service(creds)\n",
    "\n",
    "\n",
    "print(start_date), print(end_date)\n",
    "REPORTID_nonHVA = request_report(service, start_date, end_date, \n",
    "                                 ['accountType', 'date', 'clicks', 'cost', 'impr','dfaRevenue', 'dfaTransactions'])\n",
    "REPORTID_HVA = request_report(service, start_date, end_date, \n",
    "                              ['accountType', 'date',\n",
    "                               'floodlightActivity', 'dfaActions'])\n",
    "non_hva_date= poll_report(service, REPORTID_nonHVA)\n",
    "hva_date= poll_report(service, REPORTID_HVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weekday_1=non_hva_date\n",
    "Weekday_2=hva_date\n",
    "import calendar\n",
    "Weekday_1['weekday']=Weekday_1['date'].apply(lambda x: calendar.day_name[pd.to_datetime(x).weekday()])\n",
    "Weekday_2['weekday']=Weekday_2['date'].apply(lambda x: calendar.day_name[pd.to_datetime(x).weekday()])\n",
    "\n",
    "Weekday_1_Agg=Weekday_1.groupby(['accountType','weekday'])[['clicks','cost','impr','dfaRevenue','dfaTransactions']].sum()\n",
    "Weekday_1_Agg.reset_index(inplace=True)\n",
    "\n",
    "Weekday_2_Agg=Weekday_2.groupby(['accountType','weekday','floodlightActivity'])[['dfaActions']].sum()\n",
    "Weekday_2_Agg.reset_index(inplace=True)\n",
    "\n",
    "Weekday_2_Group_New=pd.merge(Weekday_2_Agg,Group,on=\"floodlightActivity\")\n",
    "\n",
    "\n",
    "Weekday_2_Group_3HVA=Weekday_2_Group_New.groupby(['accountType','weekday','Group Name'])[['dfaActions']].sum() \n",
    "Weekday_2_Group_3HVA.reset_index(inplace = True)\n",
    "Weekday_2_Group_3HVA_Wide=Weekday_2_Group_3HVA.pivot_table(index=['weekday','accountType'],columns=['Group Name'],values=['dfaActions'])\n",
    "\n",
    "Weekday_2_Group_floodlightActivity=Weekday_2_Group_New\n",
    "del Weekday_2_Group_floodlightActivity['Group Name']\n",
    "Weekday_2_Group_53Activities=Weekday_2_Group_floodlightActivity.pivot_table(index=['weekday','accountType'],columns=['floodlightActivity'],values=['dfaActions'])\n",
    "Weekday_2_Group_3HVA_Wide.columns =Weekday_2_Group_3HVA_Wide.columns.get_level_values(1)\n",
    "Weekday_2_Group_3HVA_Wide.reset_index(inplace=True)\n",
    "Weekday_2_Group_53Activities.columns =Weekday_2_Group_53Activities.columns.get_level_values(1)\n",
    "Weekday_2_Group_53Activities.reset_index(inplace=True)\n",
    "\n",
    "Merge_1_Weekday=pd.merge(Weekday_1_Agg,Weekday_2_Group_3HVA_Wide,on=[\"weekday\",\"accountType\"])\n",
    "Merge_2_Weekday=pd.merge(Merge_1_Weekday,Weekday_2_Group_53Activities,on=[\"weekday\",\"accountType\"])\n",
    "Merge_2_Weekday['Conversions']=np.sum(Merge_2_Weekday.loc[:,['Perry Ellis - HVA 1','Perry Ellis - HVA 2','Perry Ellis - HVA 3']],axis=1)\n",
    "Merge_2_Weekday['ROI']=Merge_2_Weekday['dfaRevenue']/Merge_2_Weekday['cost']\n",
    "Merge_2_Weekday['Click Through Rate']=Merge_2_Weekday['clicks']/Merge_2_Weekday['impr']\n",
    "Merge_2_Weekday['Cost per Transaction']=Merge_2_Weekday['cost']/Merge_2_Weekday['dfaTransactions']  \n",
    "Merge_2_Weekday['Cost per Click']=Merge_2_Weekday['cost']/Merge_2_Weekday['clicks']\n",
    "Merge_2_Weekday['Cost per HVA (total)']=Merge_2_Weekday['cost']/Merge_2_Weekday['Conversions']\n",
    "Merge_2_Weekday['Cost per HVA 1']=Merge_2_Weekday['cost']/Merge_2_Weekday['Perry Ellis - HVA 1']\n",
    "Merge_2_Weekday['Cost per HVA 2']=Merge_2_Weekday['cost']/Merge_2_Weekday['Perry Ellis - HVA 2']\n",
    "Merge_2_Weekday['Cost per HVA 3']=Merge_2_Weekday['cost']/Merge_2_Weekday['Perry Ellis - HVA 3']\n",
    "\n",
    "cols = Merge_2_Weekday.columns.tolist()\n",
    "cols_new=cols[:2]+['ROI']+cols[2:7]+['Conversions']+cols[-7:]+cols[7:-9]\n",
    "Merge_2_Weekday_Final=Merge_2_Weekday[cols_new]\n",
    "\n",
    "Merge_2_Weekday_Final=Merge_2_Weekday_Final.sort_values(by=['accountType','ROI'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour1=pd.read_excel(\"/Users/JayLiang/Desktop/Media Storm/PEI/Perry Ellis/Auto Run Device Weekday and Hour/Hour Data/Hour 1.xlsx\")\n",
    "hour2=pd.read_excel(\"/Users/JayLiang/Desktop/Media Storm/PEI/Perry Ellis/Auto Run Device Weekday and Hour/Hour Data/Hour 2.xlsx\")\n",
    "Hour_of_day_1=hour1[[\"Engine\",\"Hour of day\",\"Clicks\",\"Cost\",\"Impr\",\"Revenue\",\"Trans\"]]\n",
    "Hour_of_day_2=hour2[[\"Engine\",\"Hour of day\",\"Floodlight activity\",\"Actions\"]]\n",
    "\n",
    "Group = pd.read_csv(\"/Users/JayLiang/Desktop/Media Storm/PEI/Perry Ellis/Auto Run Device Weekday and Hour/Group.csv\")\n",
    "\n",
    "Hour_of_day_2_Group_New=pd.merge(Hour_of_day_2,Group,  on=\"Floodlight activity\")\n",
    "\n",
    "Hour_of_day_2_Group_3HVA=Hour_of_day_2_Group_New.groupby(['Engine','Hour of day','Group Name'],as_index=False)['Actions'].sum()\n",
    "Hour_of_day_2_Group_3HVA_Wide=Hour_of_day_2_Group_3HVA.pivot_table(index=['Hour of day','Engine'],columns=['Group Name'],values=['Actions'])\n",
    "Hour_of_day_2_Group_3HVA_Wide.columns =Hour_of_day_2_Group_3HVA_Wide.columns.get_level_values(1)\n",
    "Hour_of_day_2_Group_3HVA_Wide.reset_index(inplace=True)\n",
    "\n",
    "Hour_of_day_2_Group_floodlightActivity=Hour_of_day_2_Group_New\n",
    "\n",
    "Hour_of_day_2_Group_53Activities_Wide=Hour_of_day_2_Group_floodlightActivity.pivot_table(index=['Hour of day','Engine'],columns=['Floodlight activity'],values=['Actions'],dropna=False)\n",
    "Hour_of_day_2_Group_53Activities_Wide.columns =Hour_of_day_2_Group_53Activities_Wide.columns.get_level_values(1)\n",
    "Hour_of_day_2_Group_53Activities_Wide.reset_index(inplace=True)\n",
    "##\n",
    "Merge_1_Hour=pd.merge(Hour_of_day_1,Hour_of_day_2_Group_3HVA_Wide,on=[\"Hour of day\",\"Engine\"])\n",
    "Merge_2_Hour_of_day=pd.merge(Merge_1_Hour,Hour_of_day_2_Group_53Activities_Wide,on=[\"Hour of day\",\"Engine\"])\n",
    "\n",
    "Merge_2_Hour_of_day['Conversions']=np.sum(Merge_2_Hour_of_day.loc[:,['Perry Ellis - HVA 1','Perry Ellis - HVA 2','Perry Ellis - HVA 3']],axis=1)\n",
    "Merge_2_Hour_of_day['ROI']=Merge_2_Hour_of_day['Revenue']/Merge_2_Hour_of_day['Cost']\n",
    "Merge_2_Hour_of_day['Click Through Rate']=Merge_2_Hour_of_day['Clicks']/Merge_2_Hour_of_day['Impr']\n",
    "Merge_2_Hour_of_day['Cost per Transaction']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Trans']  \n",
    "Merge_2_Hour_of_day['Cost per Click']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Clicks']\n",
    "Merge_2_Hour_of_day['Cost per HVA (total)']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Conversions']\n",
    "Merge_2_Hour_of_day['Cost per HVA 1']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Perry Ellis - HVA 1']\n",
    "Merge_2_Hour_of_day['Cost per HVA 2']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Perry Ellis - HVA 2']\n",
    "Merge_2_Hour_of_day['Cost per HVA 3']=Merge_2_Hour_of_day['Cost']/Merge_2_Hour_of_day['Perry Ellis - HVA 3']\n",
    "\n",
    "cols = Merge_2_Hour_of_day.columns.tolist()\n",
    "cols_new=cols[:2]+['ROI']+cols[2:7]+['Conversions']+cols[-7:]+cols[7:-9]\n",
    "Hour_of_day_Final=Merge_2_Hour_of_day[cols_new]\n",
    "\n",
    "Hour_of_day_Final=Hour_of_day_Final.sort_values(by=['Engine','ROI'],ascending=[True,False])\n",
    "# To change later #### Hour_of_day_Final.to_excel('Aug. 21/Weekday.xlsx', sheet_name='Hour_of_day_Final', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_dfs=[Merge_2_Device_Final,Merge_2_Weekday_Final,Hour_of_day_Final]\n",
    "def save_xls(list_dfs, xls_path):\n",
    "    writer = ExcelWriter(xls_path)\n",
    "    for n, df in enumerate(list_dfs):\n",
    "        df.to_excel(writer,'sheet%s' % n,index=False)\n",
    "    writer.save()\n",
    "save_xls(list_dfs,xls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
