{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_sturday: 2020-02-08\n",
      "Year 2020\n",
      "Quarter 1\n",
      "str_current_quarter 2020_Q1\n",
      "current_week 1\n",
      "current_quarter_beginning: 2020-02-02\n",
      "previous_quarter_beginning: 2019-11-03\n",
      "audience_Quarter_num: 1\n",
      "audience_year: 2020\n",
      "len(list_aud_files_uploaded_T): 272\n",
      "(17351682, 3) 17351682\n",
      "len(list_files_POS_in_6_weeks): 6\n",
      "len(list_files_POS_in_6_weeks): 6\n",
      "['/home/jian/BigLots/MediaStorm_2020-02-08/MediaStormDailySales20200211-120911-483.txt', '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-02-01/MediaStormDailySales20200204-111741-091.txt', '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-25/MediaStormDailySales20200128-111758-074.txt', '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-18/MediaStormDailySales20200121-111749-649.txt', '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-11/MediaStormDailySales20200114-115009-140.txt', '/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-01-04/MediaStormDailySales20200107-112859-015.txt']\n",
      "MediaStormDailySales20200211-120911-483.txt | 2020-02-11 23:48:54.530193\n",
      "MediaStormDailySales20200204-111741-091.txt | 2020-02-11 23:50:26.634767\n",
      "MediaStormDailySales20200128-111758-074.txt | 2020-02-11 23:51:33.984370\n",
      "MediaStormDailySales20200121-111749-649.txt | 2020-02-11 23:52:35.043554\n",
      "MediaStormDailySales20200114-115009-140.txt | 2020-02-11 23:53:40.340230\n",
      "MediaStormDailySales20200107-112859-015.txt | 2020-02-11 23:55:34.226705\n",
      "date_start_4_weeks_start_Sunday 2020-01-12\n",
      "date_start_6_weeks_start_Sunday 2019-12-29\n",
      "df_ids_purchased_positive_in_6_weeks['recency_group'].unique(): ['Shopped_in_4_weeks' 'NoShopped_4-6_weeks']\n",
      "df_current_test_aud['recency_group'].unique(): ['Shopped_in_4_weeks' 'NoShopped_7+_weeks' 'NoShopped_4-6_weeks']\n",
      "decile_details read done: 2020-02-12 00:02:02.608612\n",
      "decile_details merged into audience: 2020-02-12 00:03:12.264409\n",
      "(17351682, 6) 17351682\n",
      "(14734124, 6)\n",
      "['NoShopped_7+_weeks' 'NoShopped_4-6_weeks']\n",
      "['D01' 'D02' 'D03' 'D04' 'D05' 'D06' 'D07' 'D08' 'D09' 'D10']\n",
      "['P' 'S']\n",
      "local files saved: 2020-02-12 00:10:19.924261\n",
      "transfered to 64: 2020-02-12 00:15:15.603062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jian/celery/Audience_No_Shop/output_no_shopping/Q1_2020/output_2020-02-08/BL_summary_by_decile_zip_uploaded_audience_2020-02-08_JL_2020-02-12.csv',\n",
       " '/home/jian/celery/Audience_No_Shop/output_no_shopping/Q1_2020/output_2020-02-08/BL_df_no_shop_uploaded_audience_2020-02-08_JL_2020-02-12.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='/home/jian/BL_weekly_crontab/cron_7_no_shopper_in_audience/ID_list_no_shopping_6_weeks.log', level=logging.INFO)\n",
    "\n",
    "def recursive_file_gen(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "last_sturday = (datetime.datetime.now()-datetime.timedelta(days=(datetime.datetime.now().weekday()+2))).date()\n",
    "print(\"last_sturday: \"+str(last_sturday))\n",
    "logging.info(\"last_sturday: \"+str(last_sturday))\n",
    "last_day_of_2018Q4=datetime.date(2019,2,2)\n",
    "\n",
    "year_of_quarter=(last_sturday-last_day_of_2018Q4).days/(52*7)\n",
    "year_of_quarter=str(int(2019+np.floor(year_of_quarter)))\n",
    "print(\"Year\",year_of_quarter)\n",
    "logging.info(\"Year \"+str(year_of_quarter))\n",
    "\n",
    "\n",
    "quarter_of_quarter=(last_sturday-last_day_of_2018Q4).days/7\n",
    "quarter_of_quarter=np.floor(quarter_of_quarter/13)%4\n",
    "quarter_of_quarter=str(int(1+quarter_of_quarter))\n",
    "print(\"Quarter\",quarter_of_quarter)\n",
    "logging.info(\"Quarter \"+str(quarter_of_quarter))\n",
    "\n",
    "str_current_quarter=year_of_quarter+\"_Q\"+quarter_of_quarter\n",
    "\n",
    "print(\"str_current_quarter\", str_current_quarter)\n",
    "logging.info(\"str_current_quarter \"+str(str_current_quarter))\n",
    "\n",
    "current_week=int((last_sturday-last_day_of_2018Q4).days/7%13)\n",
    "print(\"current_week\",current_week)\n",
    "logging.info(\"current_week: \"+str(current_week))\n",
    "\n",
    "if current_week==0:\n",
    "    quarter_of_quarter=int(quarter_of_quarter)-1+4\n",
    "    year_of_quarter=str(int(year_of_quarter)-1)\n",
    "    str_current_quarter=year_of_quarter+\"_Q\"+str(quarter_of_quarter)\n",
    "    current_week=13\n",
    "    print(\"week0-Quarter\",quarter_of_quarter)\n",
    "    print(\"week0-current_week\",current_week)\n",
    "    \n",
    "    logging.info(\"week0-Quarter: \"+str(quarter_of_quarter))\n",
    "    logging.info(\"week0-current_week: \"+str(current_week))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "current_quarter_beginning=last_day_of_2018Q4+datetime.timedelta(days=(int(year_of_quarter)-2019)*52*7+                                                                (int(quarter_of_quarter)-1)*13*7+1)\n",
    "print(\"current_quarter_beginning: \"+str(current_quarter_beginning))\n",
    "logging.info(\"current_quarter_beginning: \"+str(current_quarter_beginning))\n",
    "\n",
    "previous_quarter_beginning=current_quarter_beginning-datetime.timedelta(days=7*13)\n",
    "print(\"previous_quarter_beginning: \"+str(previous_quarter_beginning))\n",
    "logging.info(\"current_quarter_beginning: \"+str(previous_quarter_beginning))\n",
    "\n",
    "Today_date=datetime.datetime.now().date()\n",
    "\n",
    "if Today_date>=current_quarter_beginning:\n",
    "    audience_Quarter_num=int(quarter_of_quarter)\n",
    "    audience_year=int(year_of_quarter)\n",
    "else:\n",
    "    audience_Quarter_num=int(quarter_of_quarter)-1\n",
    "    audience_year=int(year_of_quarter)\n",
    "    if audience_Quarter_num==0:\n",
    "        audience_Quarter_num=4\n",
    "        audience_year=int(year_of_quarter)-1\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"audience_Quarter_num: \"+str(audience_Quarter_num))\n",
    "logging.info(\"audience_Quarter_num: \"+str(audience_Quarter_num))\n",
    "\n",
    "print(\"audience_year: \"+str(audience_year))\n",
    "logging.info(\"audience_year: \"+str(audience_year))\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "folder_current_audience=\"/home/jian/Projects/Big_Lots/Live_Ramp/Quarterly_Update_\"+str(audience_year)+\"Q\"+str(audience_Quarter_num)+\"/final_segments_uploaded_LR_0_18/\"\n",
    "list_aud_files_uploaded=glob.glob(folder_current_audience+\"*.csv\")\n",
    "list_aud_files_uploaded_T=[x for x in list_aud_files_uploaded if os.path.basename(x)[0]==\"T\"]\n",
    "print(\"len(list_aud_files_uploaded_T): \"+str(len(list_aud_files_uploaded_T)))\n",
    "logging.info(\"len(list_aud_files_uploaded_T): \"+str(len(list_aud_files_uploaded_T)))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df_current_test_aud=pd.DataFrame()\n",
    "\n",
    "for file in list_aud_files_uploaded_T:\n",
    "    df=pd.read_csv(file,dtype=str)\n",
    "    df_current_test_aud=df_current_test_aud.append(df)\n",
    "print(df_current_test_aud.shape,df_current_test_aud['customer_id_hashed'].nunique()) \n",
    "logging.info(str(df_current_test_aud.shape)+\"|\"+str(df_current_test_aud['customer_id_hashed'].nunique()))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "date_start_4_weeks_start_Sunday=last_sturday-datetime.timedelta(days=27)\n",
    "date_start_6_weeks_start_Sunday=last_sturday-datetime.timedelta(days=27+14)\n",
    "\n",
    "\n",
    "list_files_POS_in_6_weeks=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "list_files_POS_in_6_weeks=[x for x in list_files_POS_in_6_weeks if \"daily\" in x.lower() and x[-4:]==\".txt\"]\n",
    "list_files_POS_in_6_weeks=[x for x in list_files_POS_in_6_weeks if \"/MediaStorm_\"in x]\n",
    "\n",
    "list_files_POS_in_6_weeks=[x for x in list_files_POS_in_6_weeks if x.split(\"/MediaStorm_\")[1][:10]>=str(date_start_6_weeks_start_Sunday)]\n",
    "print(\"len(list_files_POS_in_6_weeks): \"+str(len(list_files_POS_in_6_weeks)))\n",
    "print(\"len(list_files_POS_in_6_weeks): \"+str(len(list_files_POS_in_6_weeks)))\n",
    "\n",
    "logging.info(\"len(list_files_POS_in_6_weeks): \"+str(len(list_files_POS_in_6_weeks)))\n",
    "logging.info(\"len(list_files_POS_in_6_weeks): \"+str(len(list_files_POS_in_6_weeks)))\n",
    "\n",
    "list_files_POS_in_6_weeks=sorted(list_files_POS_in_6_weeks,key=lambda x: x.split(\"/MediaStorm_\")[1][:10],reverse=True)\n",
    "print(list_files_POS_in_6_weeks)\n",
    "logging.info(str(list_files_POS_in_6_weeks))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df_ids_purchased_positive_in_6_weeks=pd.DataFrame()\n",
    "\n",
    "for file in list_files_POS_in_6_weeks:\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",\n",
    "                     usecols=['customer_id_hashed','transaction_dt','item_transaction_amt'])\n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df.groupby(['customer_id_hashed','transaction_dt'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "    df=df[df['sales']>0]\n",
    "    df_ids_purchased_positive_in_6_weeks=df_ids_purchased_positive_in_6_weeks.append(df)\n",
    "    print(str(os.path.basename(file))+\" | \"+str(datetime.datetime.now()))\n",
    "    logging.info(str(os.path.basename(file))+\" | \"+str(datetime.datetime.now()))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "del df_ids_purchased_positive_in_6_weeks['sales']\n",
    "df_ids_purchased_positive_in_6_weeks=df_ids_purchased_positive_in_6_weeks.sort_values(\"transaction_dt\",ascending=False).drop_duplicates(\"customer_id_hashed\")\n",
    "df_ids_purchased_positive_in_6_weeks=df_ids_purchased_positive_in_6_weeks.rename(columns={\"transaction_dt\":\"last_transaction_dt\"})\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "df_ids_purchased_positive_in_6_weeks.head(2)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "print(\"date_start_4_weeks_start_Sunday\", date_start_4_weeks_start_Sunday)\n",
    "print(\"date_start_6_weeks_start_Sunday\", date_start_6_weeks_start_Sunday)\n",
    "\n",
    "logging.info(\"date_start_4_weeks_start_Sunday\"+str(date_start_4_weeks_start_Sunday))\n",
    "logging.info(\"date_start_6_weeks_start_Sunday\"+str(date_start_6_weeks_start_Sunday))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "df_ids_purchased_positive_in_6_weeks['recency_group']=np.where(df_ids_purchased_positive_in_6_weeks['last_transaction_dt']>=str(date_start_4_weeks_start_Sunday),\"Shopped_in_4_weeks\",\n",
    "                                                              np.where(df_ids_purchased_positive_in_6_weeks['last_transaction_dt']>=str(date_start_6_weeks_start_Sunday),\"NoShopped_4-6_weeks\",\"NoShopped_7+_weeks\")\n",
    "                                                              )\n",
    "print(\"df_ids_purchased_positive_in_6_weeks['recency_group'].unique(): \"+str(df_ids_purchased_positive_in_6_weeks['recency_group'].unique()))\n",
    "logging.info(\"df_ids_purchased_positive_in_6_weeks['recency_group'].unique(): \"+str(df_ids_purchased_positive_in_6_weeks['recency_group'].unique()))\n",
    "del df_ids_purchased_positive_in_6_weeks['last_transaction_dt']\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "df_current_test_aud=pd.merge(df_current_test_aud,df_ids_purchased_positive_in_6_weeks,on=\"customer_id_hashed\",how=\"left\")\n",
    "df_current_test_aud['recency_group']=df_current_test_aud['recency_group'].fillna(\"NoShopped_7+_weeks\")\n",
    "print(\"df_current_test_aud['recency_group'].unique(): \"+str(df_current_test_aud['recency_group'].unique()))\n",
    "logging.info(\"df_current_test_aud['recency_group'].unique(): \"+str(df_current_test_aud['recency_group'].unique()))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "df_current_test_aud.shape\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Add the decile\n",
    "\n",
    "df_defile_file=pd.read_csv(\"/home/jian/celery/Audience_No_Shop/Quarterly_Decile_Detail/df_detail_\"+str(audience_year)+\"Q\"+str(audience_Quarter_num)+\".csv\",\n",
    "                           dtype=str,usecols=['customer_id_hashed','frmindex','zip_type']).drop_duplicates()\n",
    "df_defile_file.shape\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "print(\"decile_details read done: \"+str(datetime.datetime.now()))\n",
    "logging.info(\"decile_details read done: \"+str(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "df_current_test_aud=pd.merge(df_current_test_aud,df_defile_file,on=\"customer_id_hashed\",how=\"left\")\n",
    "\n",
    "print(\"decile_details merged into audience: \"+str(datetime.datetime.now()))\n",
    "logging.info(\"decile_details merged into audience: \"+str(datetime.datetime.now()))\n",
    "\n",
    "df_current_test_aud.shape\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "writer_folder_quarter=\"/home/jian/celery/Audience_No_Shop/output_no_shopping/Q\"+str(audience_Quarter_num)+\"_\"+str(audience_year)+\"/\"\n",
    "\n",
    "try:\n",
    "    os.stat(writer_folder_quarter)\n",
    "except:\n",
    "    os.mkdir(writer_folder_quarter)\n",
    "    \n",
    "writer_folder_week=writer_folder_quarter+\"output_\"+str(last_sturday)+\"/\"\n",
    "\n",
    "try:\n",
    "    os.stat(writer_folder_week)\n",
    "except:\n",
    "    os.mkdir(writer_folder_week)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "print(df_current_test_aud.shape,df_current_test_aud['customer_id_hashed'].nunique())\n",
    "logging.info(str(df_current_test_aud.shape)+str(df_current_test_aud['customer_id_hashed'].nunique()))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "df_summary=df_current_test_aud.groupby(['zip_type',\"frmindex\",'recency_group'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "df_summary=df_summary.pivot_table(index=['zip_type',\"frmindex\"],columns=\"recency_group\",values='customer_id_hashed').reset_index()\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "df_output=df_current_test_aud[df_current_test_aud['recency_group']!='Shopped_in_4_weeks']\n",
    "\n",
    "print(df_output.shape)\n",
    "print(df_output['recency_group'].unique())\n",
    "print(df_output['frmindex'].unique())\n",
    "print(df_output['zip_type'].unique())\n",
    "\n",
    "logging.info(str(df_output.shape))\n",
    "logging.info(str(df_output['recency_group'].unique()))\n",
    "logging.info(str(df_output['frmindex'].unique()))\n",
    "logging.info(str(df_output['zip_type'].unique()))\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "df_output[pd.isnull(df_output['zip_type'])]['frmindex'].unique()\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "count_audience_total=df_defile_file.groupby([\"frmindex\",\"zip_type\"])['customer_id_hashed'].count().to_frame().reset_index()\n",
    "df_summary=pd.merge(count_audience_total,df_summary,on=[\"frmindex\",\"zip_type\"],how=\"outer\")\n",
    "\n",
    "df_summary=df_summary[df_summary['zip_type']!=\"T\"]\n",
    "df_summary=df_summary.fillna(0)\n",
    "df_summary=df_summary[['frmindex','zip_type','customer_id_hashed','Shopped_in_4_weeks','NoShopped_4-6_weeks','NoShopped_7+_weeks']]\n",
    "df_summary.to_csv(writer_folder_week+\"BL_summary_by_decile_zip_uploaded_audience_\"+str(last_sturday)+\"_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "df_current_test_aud.shape\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "df_output.shape\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "df_current_test_aud.to_csv(writer_folder_week+\"/BL_df_all_uploaded_audience_\"+str(last_sturday)+\"_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "df_output.to_csv(writer_folder_week+\"/BL_df_no_shop_uploaded_audience_\"+str(last_sturday)+\"_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "\n",
    "print(\"local files saved: \"+str(datetime.datetime.now()))\n",
    "logging.info(\"local files saved: \"+str(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "# # tranfer to 64 for SP\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "import paramiko\n",
    "\n",
    "host = \"64.237.51.251\" #hard-coded\n",
    "port = 22\n",
    "transport = paramiko.Transport((host, port))\n",
    "\n",
    "password = \"jian@juba2017\" #hard-coded\n",
    "username = \"jian\" #hard-coded\n",
    "transport.connect(username = username, password = password)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "remote_folder=\"/mnt/drv5/lr_biglots_data/Audience_NoShopping/output_\"+str(last_sturday)+\"/\"\n",
    "\n",
    "try:\n",
    "    sftp.listdir(remote_folder)\n",
    "except:\n",
    "    sftp.mkdir(remote_folder)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "local_file_list=glob.glob(writer_folder_week+\"*.csv\")\n",
    "local_file_list=[x for x in local_file_list if \"BL_df_all_uploaded_audience\" not in x]\n",
    "for local_file in local_file_list:\n",
    "    sftp.put(local_file,remote_folder+os.path.basename(local_file))\n",
    "sftp.close()\n",
    "transport.close()\n",
    "\n",
    "\n",
    "print(\"transfered to 64: \"+str(datetime.datetime.now()))\n",
    "logging.info(\"transfered to 64: \"+str(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "local_file_list\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
