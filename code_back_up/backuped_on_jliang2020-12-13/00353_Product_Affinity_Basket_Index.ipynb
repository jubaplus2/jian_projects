{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "price_threshold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_saturday=datetime.date(2019,1,12) # To be changed to the running Tuesday\n",
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "            \n",
    "most_recent_daily_data=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "most_recent_daily_data=[x for x in most_recent_daily_data if (\"MediaStormDailySales\" in x) and (str(last_saturday) in x)]\n",
    "\n",
    "if len(most_recent_daily_data)==1:\n",
    "    most_recent_daily_data=most_recent_daily_data[0]\n",
    "else:\n",
    "    most_recent_daily_data=np.nan\n",
    "    print(\"Last Weekly Daily Data Error\", str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/BigLots/MediaStorm_2019-01-12/MediaStormDailySales20190115-113411-782.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_sub_class_id: [1 2 3]\n",
      "len_class_code_id: [5]\n",
      "Row_RawData: (6159646, 8)\n",
      "Unique_id: 1091276\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_table(most_recent_daily_data,dtype=str,sep=\"|\")\n",
    "print(\"len_sub_class_id:\",data['subclass_id'].apply(lambda x: len(x)).unique())\n",
    "print(\"len_class_code_id:\",data['class_code_id'].apply(lambda x: len(x)).unique())\n",
    "data['subclass_id']=data['subclass_id'].apply(lambda x: x.zfill(3))\n",
    "data['product_comb']=data['class_code_id']+\"-\"+data['subclass_id']\n",
    "del data['class_code_id']\n",
    "del data['subclass_id']\n",
    "data=data[~pd.isnull(data['customer_id_hashed'])]\n",
    "data['subclass_transaction_amt']=data['subclass_transaction_amt'].astype(float)\n",
    "data['subclass_transaction_units']=data['subclass_transaction_units'].astype(int)\n",
    "data['price']=data['subclass_transaction_amt']/data['subclass_transaction_units']\n",
    "print(\"Row_RawData:\",data.shape)\n",
    "print(\"Unique_id:\", len(data['customer_id_hashed'].unique()))\n",
    "data=data[(data['subclass_transaction_amt']>0) & (data['subclass_transaction_units']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1526"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['product_comb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxonomy=pd.read_csv(\"/home/jian/BigLots/static_files/ProductTaxonomy/MediaStormProductTaxonomy20190101-135843-066.txt\",dtype=str,sep=\"|\")\n",
    "taxonomy['subclass_id']=taxonomy['subclass_id'].apply(lambda x: x.zfill(3))\n",
    "division_id_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Division Names.txt\",dtype=str,sep=\"|\")\n",
    "department_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",dtype=str,sep=\"|\")\n",
    "class_id_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Class Names.txt\",dtype=str,sep=\"|\",encoding ='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_item_avg_price=data[['product_comb','price']].groupby(['product_comb'])['price'].mean().to_frame().reset_index()\n",
    "data_item_avg_price=data_item_avg_price.rename(columns={\"price\":\"avg_price\"})\n",
    "\n",
    "data_item_avg_price['class_code_id']=data_item_avg_price['product_comb'].apply(lambda x: x.split(\"-\")[0])\n",
    "data_item_avg_price['subclass_id']=data_item_avg_price['product_comb'].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,taxonomy,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "\n",
    "\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,division_id_id_name,on=\"division_id\",how=\"left\")\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,department_id_name,on=\"department_id\",how=\"left\")\n",
    "data_item_avg_price=pd.merge(data_item_avg_price,class_id_name,on=\"class_code_id\",how=\"left\")\n",
    "data_item_avg_price=data_item_avg_price[['product_comb','avg_price','division_id','division_desc','department_id','department_desc',\n",
    "                                         'class_code_id','class_code_desc','subclass_id','subclass_desc']]\n",
    "\n",
    "data_item_avg_price.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/2018_Q4/Product_Basket/Price_\"+str(last_saturday)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6082846, 8)\n",
      "(480861, 8)\n"
     ]
    }
   ],
   "source": [
    "# $10 of all items as in the email on 2019-01-14\n",
    "\n",
    "product_comb_under_10_set=set(data_item_avg_price[data_item_avg_price['avg_price']<price_threshold]['product_comb'].unique().tolist())\n",
    "\n",
    "print(data.shape)\n",
    "data=data[~data['product_comb'].isin(product_comb_under_10_set)]\n",
    "data_under_10=data[data['product_comb'].isin(product_comb_under_10_set)]\n",
    "data=data.reset_index()\n",
    "del data['index']\n",
    "print(data.shape)\n",
    "dict_item_avg_price=data_item_avg_price.set_index(['product_comb'])['avg_price'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_comb_10_and_above_list=data_item_avg_price[data_item_avg_price['avg_price']>=10]['product_comb'].unique().tolist()\n",
    "\n",
    "data_transactions_list=data.groupby(['location_id','transaction_dt','transaction_id','customer_id_hashed'])['product_comb'].apply(list).to_frame().reset_index().rename(columns={\"product_comb\":\"basket_list\"})\n",
    "data_transactions_units_sales=data.groupby(['location_id','transaction_dt','transaction_id','customer_id_hashed'])['subclass_transaction_units','subclass_transaction_amt'].sum().reset_index().rename(columns={\"subclass_transaction_units\":\"total_item_units\",\"subclass_transaction_amt\":\"total_item_revenue\"})\n",
    "\n",
    "data_transactions=pd.merge(data_transactions_list,data_transactions_units_sales,on=['location_id','transaction_dt','transaction_id','customer_id_hashed'],how=\"left\")\n",
    "data_transactions['basket_str']=data_transactions['basket_list'].apply(lambda x: sorted(x)).astype(str)\n",
    "data_transactions['transactin_id_given']=[x for x in range(1,len(data_transactions)+1)]\n",
    "data_transactions['types']=data_transactions['basket_list'].apply(lambda x: len(x))\n",
    "\n",
    "# To save\n",
    "\n",
    "\n",
    "data=pd.merge(data,data_transactions,on=[\"location_id\",\"transaction_dt\",\"transaction_id\",\"customer_id_hashed\"],how=\"left\")\n",
    "apply_func={\"subclass_transaction_units\":\"sum\",\"transactin_id_given\":\"count\"}\n",
    "\n",
    "single_prod_df=data.groupby(['product_comb'])['subclass_transaction_units','transactin_id_given'].agg(apply_func).reset_index().rename(columns={\"subclass_transaction_units\":\"Total_Units\",\"transactin_id_given\":\"Total_Trans\"})\n",
    "total_unit=single_prod_df['Total_Units'].sum()\n",
    "total_trans=single_prod_df['Total_Trans'].sum()\n",
    "\n",
    "single_prod_df['prob_unit']=single_prod_df['Total_Units']/total_unit\n",
    "single_prod_df['prob_tran']=single_prod_df['Total_Trans']/total_trans\n",
    "\n",
    "dict_single_prod_prob_unit=single_prod_df.set_index(['product_comb'])['prob_unit'].to_dict()\n",
    "dict_single_prod_prob_tran=single_prod_df.set_index(['product_comb'])['prob_tran'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_basket=data_transactions.groupby(['basket_str'])['total_item_units','total_item_revenue','transactin_id_given'].agg(\n",
    "            {\"total_item_units\":\"sum\",\"total_item_revenue\":\"sum\",\"transactin_id_given\":\"count\"}).reset_index().rename(columns={\"transactin_id_given\":\"trans_count\"})\n",
    "data_basket['basket_list']=data_basket['basket_str'].apply(eval)\n",
    "data_basket['item_types']=data_basket['basket_list'].apply(len)\n",
    "data_basket=data_basket.sort_values(['item_types','basket_str'])\n",
    "\n",
    "data_basket=data_basket.reset_index()\n",
    "del data_basket['index']\n",
    "\n",
    "unique_id_by_basket=data_transactions.groupby(['basket_str'])['customer_id_hashed'].apply(lambda x: len(set(x))).to_frame().reset_index().rename(columns={'customer_id_hashed':\"unique_ids\"})\n",
    "data_basket=pd.merge(data_basket,unique_id_by_basket,on=\"basket_str\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfrom itertools import combinations\\ndef findsubsets(total_set,item_counts):\\n    return list(set(combinations(total_set, item_counts)))\\n\\nset_2_comb=sorted(set_2_comb)\\nset_3_comb=sorted(set_3_comb)\\nset_4_comb=sorted(set_4_comb)\\nset_5_comb=sorted(set_5_comb)\\n\\ni_counter=0\\nall_trans_list=data_transactions['basket_str'].tolist()\\ndict_basket_2={}\\nfor basket_2 in set_2_comb:\\n    basket_item_1=eval(basket)[0]\\n    basket_item_2=eval(basket)[1]\\n    \\n    trans=[x for x in all_trans_list if (basket_item_1 in x) & (basket_item_2 in x)]\\n    trans_basket_2=len(trans)\\n    dict_basket_2.update({basket_2:trans_basket_2})\\n    i_counter+=1\\n    if i_counter%1000==20:\\n        print(datetime.datetime.now(),i_counter)\\n    \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like not needed in this way\n",
    "'''\n",
    "\n",
    "from itertools import combinations\n",
    "def findsubsets(total_set,item_counts):\n",
    "    return list(set(combinations(total_set, item_counts)))\n",
    "\n",
    "set_2_comb=sorted(set_2_comb)\n",
    "set_3_comb=sorted(set_3_comb)\n",
    "set_4_comb=sorted(set_4_comb)\n",
    "set_5_comb=sorted(set_5_comb)\n",
    "\n",
    "i_counter=0\n",
    "all_trans_list=data_transactions['basket_str'].tolist()\n",
    "dict_basket_2={}\n",
    "for basket_2 in set_2_comb:\n",
    "    basket_item_1=eval(basket)[0]\n",
    "    basket_item_2=eval(basket)[1]\n",
    "    \n",
    "    trans=[x for x in all_trans_list if (basket_item_1 in x) & (basket_item_2 in x)]\n",
    "    trans_basket_2=len(trans)\n",
    "    dict_basket_2.update({basket_2:trans_basket_2})\n",
    "    i_counter+=1\n",
    "    if i_counter%1000==20:\n",
    "        print(datetime.datetime.now(),i_counter)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "for index, row in data_basket.iterrows():\n",
    "    basket=row['basket_list']\n",
    "    if len(basket)==1:\n",
    "        df=pd.DataFrame(row).T\n",
    "        df['BAI_units']=100\n",
    "        df['BAI_trans']=100\n",
    "        price_list=[{basket[0]:np.round(dict_item_avg_price[basket[0]],4)}]\n",
    "        df['price_list']=[price_list]\n",
    "        \n",
    "    elif len(basket)==2:\n",
    "        item_1=basket[0]\n",
    "        item_2=basket[1]\n",
    "        price_list=[{item_1:np.round(dict_item_avg_price[item_1],4)},{item_2:np.round(dict_item_avg_price[item_2],4)}]\n",
    "\n",
    "        BAI_units=(row['total_item_units']/total_unit) / (dict_single_prod_prob_unit[item_1]*dict_single_prod_prob_unit[item_2]) *100\n",
    "        BAI_trans=(row['trans_count']/total_trans) / (dict_single_prod_prob_tran[item_1]*dict_single_prod_prob_tran[item_2]) *100\n",
    "        \n",
    "        df=pd.DataFrame(row).T\n",
    "        df['BAI_units']=BAI_units\n",
    "        df['BAI_trans']=BAI_trans\n",
    "        df['price_list']=[price_list]\n",
    "        \n",
    "    elif len(basket)==3:\n",
    "        item_1=basket[0]\n",
    "        item_2=basket[1]\n",
    "        item_3=basket[2]\n",
    "        price_list=[{item_1:np.round(dict_item_avg_price[item_1],4)},\n",
    "                    {item_2:np.round(dict_item_avg_price[item_2],4)},\n",
    "                    {item_3:np.round(dict_item_avg_price[item_3],4)}]\n",
    "\n",
    "        BAI_units=(row['total_item_units']/total_unit) / (dict_single_prod_prob_unit[item_1]*dict_single_prod_prob_unit[item_2]*dict_single_prod_prob_tran[item_3]) *100\n",
    "        BAI_trans=(row['trans_count']/total_trans) / (dict_single_prod_prob_tran[item_1]*dict_single_prod_prob_tran[item_2]*dict_single_prod_prob_tran[item_3]) *100\n",
    "        \n",
    "        df=pd.DataFrame(row).T\n",
    "        df['BAI_units']=BAI_units\n",
    "        df['BAI_trans']=BAI_trans\n",
    "        df['price_list']=[price_list]\n",
    "        \n",
    "    elif len(basket)==4:\n",
    "        item_1=basket[0]\n",
    "        item_2=basket[1]\n",
    "        item_3=basket[2]\n",
    "        item_4=basket[3]\n",
    "        price_list=[{item_1:np.round(dict_item_avg_price[item_1],4)},\n",
    "                    {item_2:np.round(dict_item_avg_price[item_2],4)},\n",
    "                    {item_3:np.round(dict_item_avg_price[item_3],4)},\n",
    "                    {item_4:np.round(dict_item_avg_price[item_4],4)}]\n",
    "        BAI_units=(row['total_item_units']/total_unit) / (dict_single_prod_prob_unit[item_1]*dict_single_prod_prob_unit[item_2]*dict_single_prod_prob_tran[item_3]*dict_single_prod_prob_tran[item_4]) *100\n",
    "        BAI_trans=(row['trans_count']/total_trans) / (dict_single_prod_prob_tran[item_1]*dict_single_prod_prob_tran[item_2]*dict_single_prod_prob_tran[item_3]*dict_single_prod_prob_tran[item_4]) *100\n",
    "        \n",
    "        df=pd.DataFrame(row).T\n",
    "        df['BAI_units']=BAI_units\n",
    "        df['BAI_trans']=BAI_trans\n",
    "        df['price_list']=[price_list]\n",
    "    \n",
    "    elif len(basket)==5:\n",
    "        item_1=basket[0]\n",
    "        item_2=basket[1]\n",
    "        item_3=basket[2]\n",
    "        item_4=basket[3]\n",
    "        item_5=basket[4]\n",
    "        \n",
    "        BAI_units=(row['total_item_units']/total_unit) / (dict_single_prod_prob_unit[item_1]*dict_single_prod_prob_unit[item_2]*dict_single_prod_prob_tran[item_3]*dict_single_prod_prob_tran[item_4]*dict_single_prod_prob_tran[item_5]) *100\n",
    "        BAI_trans=(row['trans_count']/total_trans) / (dict_single_prod_prob_tran[item_1]*dict_single_prod_prob_tran[item_2]*dict_single_prod_prob_tran[item_3]*dict_single_prod_prob_tran[item_4]*dict_single_prod_prob_tran[item_5]) *100\n",
    "        price_list=[{item_1:np.round(dict_item_avg_price[item_1],4)},\n",
    "                    {item_2:np.round(dict_item_avg_price[item_2],4)},\n",
    "                    {item_3:np.round(dict_item_avg_price[item_3],4)},\n",
    "                    {item_4:np.round(dict_item_avg_price[item_4],4)},\n",
    "                    {item_5:np.round(dict_item_avg_price[item_5],4)}]\n",
    "        df=pd.DataFrame(row).T\n",
    "        df['BAI_units']=BAI_units\n",
    "        df['BAI_trans']=BAI_trans\n",
    "        df['price_list']=[price_list]\n",
    "        \n",
    "    else:\n",
    "        df=pd.DataFrame(row).T\n",
    "        \n",
    "\n",
    "    output=output.append(df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=output.sort_values(['item_types','BAI_trans','BAI_units'],ascending=[True,False,False]).reset_index()\n",
    "del output['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Analysis/2018_Q4/Product_Basket'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"by_transaction_10_plus_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter('/home/jian/Projects/Big_Lots/Analysis/2018_Q4/Product_Basket/BL_DBasket_Version1_JL_'+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "output=output[['basket_str','basket_list','BAI_trans','BAI_units','item_types','total_item_revenue','total_item_units','trans_count','unique_ids','price_list']]\n",
    "output.to_excel(writer,\"BAI_output_1_to_5\",index=False)\n",
    "data_transactions.to_excel(writer,\"by_transaction_10_plus_only\",index=False)\n",
    "single_prod_df=pd.merge(single_prod_df,data_item_avg_price,on=\"product_comb\",how=\"left\")\n",
    "single_prod_df.to_excel(writer,\"single_probilities_10_plus_only\",index=False)\n",
    "data_item_avg_price.to_excel(writer,\"item_sub_class_avg_price\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
