{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Analysis/2019_Q2/Planner_Requests/audience_Kaylar_SOTF_markets_20190708'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the SOTF\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "logging.basicConfig(filename='BL_audience_for_SOTF_0726_20190708.log', level=logging.INFO)\n",
    "logging.info('Started'+str(datetime.datetime.now()))\n",
    "\n",
    "def recurisve_file_gen(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root,file)\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 2)\n"
     ]
    }
   ],
   "source": [
    "samplerows = None\n",
    "\n",
    "lastdate = datetime.date(2019,6,29) # Recent Saturday\n",
    "start_Sunday = lastdate-datetime.timedelta(days=52*7-1)\n",
    "\n",
    "store_list=pd.read_excel(\"./7.26 Multiple Store Market Waves.xlsx\",dtype=str)\n",
    "store_list['Market']=store_list['Market'].replace(\"nan\",np.nan)\n",
    "store_list['Market']=store_list['Market'].fillna(method='ffill')\n",
    "print(store_list.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2)\n",
      "Min_Date: 2018-08-11\n",
      "Max_Date: 2019-02-09\n"
     ]
    }
   ],
   "source": [
    "# Up to 2019-03-30\n",
    "# All item level data, weekly and the 1-time transfered historical data\n",
    "historical_daily_data_folder=\"/home/jian/BigLots/hist_daily_data_itemlevel_decompressed/\"\n",
    "historical_daily_data_list=list(recurisve_file_gen(historical_daily_data_folder))\n",
    "historical_daily_data_list=[x for x in historical_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "historical_daily_df=pd.DataFrame({\"file_path\":historical_daily_data_list})\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"MediaStormDailySalesHistory\")[1])\n",
    "historical_daily_df['week_end_dt']=historical_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y%m%d\").date())\n",
    "historical_daily_df=historical_daily_df[historical_daily_df['week_end_dt']<=datetime.date(2019,2,9)]\n",
    "print(historical_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(historical_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(historical_daily_df['week_end_dt'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "Min_Date: 2019-02-16\n",
      "Max_Date: 2019-06-29\n"
     ]
    }
   ],
   "source": [
    "new_daily_data_folder=\"/home/jian/BigLots/2019_by_weeks/\"\n",
    "new_daily_data_list=list(recurisve_file_gen(new_daily_data_folder))\n",
    "new_daily_data_list=[x for x in new_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "new_daily_data_list=[x for x in new_daily_data_list if \"hist\" not in x]\n",
    "\n",
    "new_daily_df=pd.DataFrame({\"file_path\":new_daily_data_list})\n",
    "\n",
    "new_daily_df['week_end_dt']=new_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2019_by_weeks/MediaStorm_\")[1][:10])\n",
    "new_daily_df['week_end_dt']=new_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']>historical_daily_df['week_end_dt'].max()]\n",
    "new_daily_df=new_daily_df[new_daily_df['week_end_dt']<=datetime.date(2019,6,29)]\n",
    "print(new_daily_df.shape)\n",
    "print(\"Min_Date: \"+str(new_daily_df['week_end_dt'].min()))\n",
    "print(\"Max_Date: \"+str(new_daily_df['week_end_dt'].max()))\n",
    "\n",
    "daily_df_file_after_20180922=historical_daily_df.append(new_daily_df)\n",
    "\n",
    "new_dailysales_files=daily_df_file_after_20180922['file_path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files: 47\n",
      "1 done 2019-07-10 10:36:27.697562\n",
      "2 done 2019-07-10 10:38:58.471671\n",
      "3 done 2019-07-10 10:42:08.713859\n",
      "4 done 2019-07-10 10:45:33.338688\n",
      "5 done 2019-07-10 10:48:36.066649\n",
      "6 done 2019-07-10 10:50:38.196644\n",
      "7 done 2019-07-10 10:52:55.946503\n",
      "8 done 2019-07-10 10:56:29.069668\n",
      "9 done 2019-07-10 11:02:45.699516\n",
      "10 done 2019-07-10 11:06:37.681216\n",
      "11 done 2019-07-10 11:10:12.131824\n",
      "12 done 2019-07-10 11:13:55.536308\n",
      "13 done 2019-07-10 11:17:31.950436\n",
      "14 done 2019-07-10 11:22:23.110595\n",
      "15 done 2019-07-10 11:29:15.540918\n",
      "16 done 2019-07-10 11:37:55.461263\n",
      "17 done 2019-07-10 11:41:54.986219\n",
      "18 done 2019-07-10 11:45:15.560666\n",
      "19 done 2019-07-10 11:49:34.447416\n",
      "20 done 2019-07-10 11:53:23.253112\n",
      "21 done 2019-07-10 11:58:01.861451\n",
      "22 done 2019-07-10 11:59:23.359220\n",
      "23 done 2019-07-10 12:01:19.304394\n",
      "24 done 2019-07-10 12:09:52.341956\n",
      "25 done 2019-07-10 12:14:03.278500\n",
      "26 done 2019-07-10 12:15:05.389110\n",
      "27 done 2019-07-10 12:16:11.829143\n",
      "28 done 2019-07-10 12:17:35.298514\n",
      "29 done 2019-07-10 12:19:03.475920\n",
      "30 done 2019-07-10 12:20:50.749544\n",
      "31 done 2019-07-10 12:22:30.666761\n",
      "32 done 2019-07-10 12:23:47.070084\n",
      "33 done 2019-07-10 12:25:02.384868\n",
      "34 done 2019-07-10 12:26:23.031561\n",
      "35 done 2019-07-10 12:28:06.947688\n",
      "36 done 2019-07-10 12:29:46.599307\n",
      "37 done 2019-07-10 12:31:19.180608\n",
      "38 done 2019-07-10 12:32:53.535364\n",
      "39 done 2019-07-10 12:34:23.126989\n",
      "40 done 2019-07-10 12:35:45.995437\n",
      "41 done 2019-07-10 12:37:15.568012\n",
      "42 done 2019-07-10 12:38:56.611344\n",
      "43 done 2019-07-10 12:40:31.945703\n",
      "44 done 2019-07-10 12:41:59.532697\n",
      "45 done 2019-07-10 12:43:21.079746\n",
      "46 done 2019-07-10 12:44:38.968038\n",
      "47 done 2019-07-10 12:46:02.327318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >0 transactions only\n",
    "\n",
    "combined_rewards_transaction=pd.DataFrame() \n",
    "count_i=1\n",
    "print(\"Total Files: \"+str(len(new_dailysales_files)))\n",
    "\n",
    "for file_daily in new_dailysales_files:\n",
    "    df=pd.read_table(file_daily,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','item_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df=df[df['location_id']!=\"6990\"]\n",
    "    \n",
    "    df['item_transaction_amt']=df['item_transaction_amt'].astype(float)\n",
    "    df=df[df['item_transaction_amt']>0]\n",
    "    df_sales=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_trans=df[['customer_id_hashed','transaction_dt','transaction_id','location_id']].drop_duplicates().groupby(['customer_id_hashed','transaction_dt','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "    df=pd.merge(df_sales,df_trans,on=['customer_id_hashed','transaction_dt','location_id'],how=\"outer\")\n",
    "    df=df.rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "    combined_rewards_transaction=combined_rewards_transaction.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "\n",
    "# combined_rewards_transaction=combined_rewards_transaction.rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 8, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_df_file_after_20180922['week_end_dt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 7, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up to 2018-08-04\n",
    "# All subclass level data, weekly and the 1-time transfered historical data\n",
    "old_daily_data_folder=\"/home/jian/BigLots/2018_by_weeks/\"\n",
    "old_daily_data_list=list(recurisve_file_gen(old_daily_data_folder))\n",
    "old_daily_data_list=[x for x in old_daily_data_list if (\".txt\" in x) & (\"DailySales\" in x)]\n",
    "\n",
    "old_daily_df=pd.DataFrame({\"file_path\":old_daily_data_list})\n",
    "\n",
    "old_daily_df['week_end_dt']=old_daily_df['file_path'].apply(lambda x: x.split(\".\")[0].split(\"2018_by_weeks/MediaStorm_\")[1][:10])\n",
    "old_daily_df['week_end_dt']=old_daily_df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "old_daily_df=old_daily_df[old_daily_df['week_end_dt']>=start_Sunday]\n",
    "old_daily_df=old_daily_df[old_daily_df['week_end_dt']<daily_df_file_after_20180922['week_end_dt'].min()]\n",
    "old_daily_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 done 2019-07-10 12:47:33.903034\n",
      "49 done 2019-07-10 12:48:59.122271\n",
      "50 done 2019-07-10 12:50:13.630365\n",
      "51 done 2019-07-10 12:51:26.453740\n",
      "52 done 2019-07-10 12:52:49.418202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file_daily_subclass in old_daily_df['file_path'].tolist():\n",
    "    df=pd.read_table(file_daily_subclass,sep= '|',dtype =str,nrows=samplerows,\n",
    "                     usecols=['customer_id_hashed','transaction_dt','transaction_id','location_id','subclass_transaction_amt'])\n",
    "    df=df[~pd.isnull(df['customer_id_hashed'])]\n",
    "    df=df[df['location_id']!=\"6990\"]\n",
    "    \n",
    "    df['subclass_transaction_amt']=df['subclass_transaction_amt'].astype(float)\n",
    "    df=df[df['subclass_transaction_amt']>0]\n",
    "    df_sales=df.groupby(['customer_id_hashed','transaction_dt','transaction_id','location_id'])['subclass_transaction_amt'].sum().to_frame().reset_index()\n",
    "    df_trans=df[['customer_id_hashed','transaction_dt','transaction_id','location_id']].drop_duplicates().groupby(['customer_id_hashed','transaction_dt','location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans\"})\n",
    "    df=pd.merge(df_sales,df_trans,on=['customer_id_hashed','transaction_dt','location_id'],how=\"outer\")\n",
    "    df=df.rename(columns={'subclass_transaction_amt':\"sales\"})\n",
    "    combined_rewards_transaction=combined_rewards_transaction.append(df)\n",
    "    print(count_i,\"done\",datetime.datetime.now())\n",
    "    count_i+=1\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76486016, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rewards_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rewards_transaction['transaction_dt'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_func={\"trans\":\"sum\",\"sales\":\"sum\",\"transaction_dt\":[\"min\",\"max\",\"nunique\"]}\n",
    "combined_rewards_transaction=combined_rewards_transaction.groupby(['customer_id_hashed','location_id'])['trans','sales','transaction_dt'].agg(agg_func).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>location_id</th>\n",
       "      <th>trans</th>\n",
       "      <th>sales</th>\n",
       "      <th colspan=\"3\" halign=\"left\">transaction_dt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>sum</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "      <td>57.00</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>5275</td>\n",
       "      <td>1</td>\n",
       "      <td>70.78</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed location_id trans  sales  \\\n",
       "                                                                   sum    sum   \n",
       "0  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        1292     1  57.00   \n",
       "1  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        5275     1  70.78   \n",
       "\n",
       "  transaction_dt                      \n",
       "             min         max nunique  \n",
       "0     2018-10-30  2018-10-30       1  \n",
       "1     2018-10-13  2018-10-13       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rewards_transaction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Analysis/2019_Q2/Planner_Requests/audience_Kaylar_SOTF_markets_20190708'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23208038, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=combined_rewards_transaction.columns.tolist()\n",
    "new_cols=[]\n",
    "for col in cols:\n",
    "    if col[1]==\"\":\n",
    "        new_col=col[0]\n",
    "    else:\n",
    "        new_col=col[0]+\"_\"+col[1]\n",
    "    new_cols=new_cols+[new_col]\n",
    "combined_rewards_transaction.columns=new_cols\n",
    "combined_rewards_transaction.to_csv(\"./BL_recent_1_year_ids_by_store_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "combined_rewards_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17939232"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rewards_transaction['customer_id_hashed'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_rewards_transaction=pd.read_csv(\"./BL_recent_1_year_ids_by_store_JL_\"+str(datetime.datetime.now().date())+\".csv\",dtype=str)\n",
    "combined_rewards_transaction['trans_sum']=combined_rewards_transaction['trans_sum'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>location_id</th>\n",
       "      <th>trans_sum</th>\n",
       "      <th>sales_sum</th>\n",
       "      <th>transaction_dt_min</th>\n",
       "      <th>transaction_dt_max</th>\n",
       "      <th>transaction_dt_nunique</th>\n",
       "      <th>Market</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>5275</td>\n",
       "      <td>1</td>\n",
       "      <td>70.78</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...</td>\n",
       "      <td>5309</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001dadc0265bf9d250566d74e0006323f18b5826641...</td>\n",
       "      <td>4061</td>\n",
       "      <td>3</td>\n",
       "      <td>143.25</td>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000003066bafc6b19c55d3be5fc646a253c659e9e6baea...</td>\n",
       "      <td>5116</td>\n",
       "      <td>1</td>\n",
       "      <td>105.91000000000003</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed location_id  trans_sum  \\\n",
       "0  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        1292          1   \n",
       "1  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        5275          1   \n",
       "2  00000135f48c68690ad3d5fc9ada41bb5cd687452007e8...        5309          1   \n",
       "3  000001dadc0265bf9d250566d74e0006323f18b5826641...        4061          3   \n",
       "4  000003066bafc6b19c55d3be5fc646a253c659e9e6baea...        5116          1   \n",
       "\n",
       "            sales_sum transaction_dt_min transaction_dt_max  \\\n",
       "0                57.0         2018-10-30         2018-10-30   \n",
       "1               70.78         2018-10-13         2018-10-13   \n",
       "2                84.0         2018-10-09         2018-10-09   \n",
       "3              143.25         2018-09-22         2018-12-21   \n",
       "4  105.91000000000003         2018-11-12         2018-11-12   \n",
       "\n",
       "  transaction_dt_nunique Market  \n",
       "0                      1    NaN  \n",
       "1                      1    NaN  \n",
       "2                      1    NaN  \n",
       "3                      3    NaN  \n",
       "4                      1    NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_list=store_list.rename(columns={\"Store ID\":\"location_id\"})\n",
    "combined_rewards_transaction=pd.merge(combined_rewards_transaction,store_list,on=\"location_id\",how=\"left\")\n",
    "combined_rewards_transaction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_store_visites=combined_rewards_transaction.sort_values([\"customer_id_hashed\",'transaction_dt_max'], ascending=[False,False])\n",
    "most_recent_store_visites=most_recent_store_visites.drop_duplicates(\"customer_id_hashed\")\n",
    "\n",
    "id_total_trans=combined_rewards_transaction.groupby(\"customer_id_hashed\")['trans_sum'].sum().to_frame().reset_index().rename(columns={\"trans_sum\":\"total_transactions_for_id\"})\n",
    "combined_rewards_transaction=pd.merge(combined_rewards_transaction,id_total_trans,on=\"customer_id_hashed\",how=\"outer\")\n",
    "combined_rewards_transaction['trans_pctg']=combined_rewards_transaction['trans_sum']/combined_rewards_transaction['total_transactions_for_id']\n",
    "\n",
    "higher_50_df=combined_rewards_transaction[combined_rewards_transaction['trans_pctg']>=0.5]\n",
    "\n",
    "higher_50_df_target_SOTF=higher_50_df[pd.notnull(higher_50_df['Market'])]\n",
    "most_recent_store_visites_target_SOTF=most_recent_store_visites[pd.notnull(most_recent_store_visites['Market'])]\n",
    "all_inclued_output=most_recent_store_visites_target_SOTF.append(higher_50_df_target_SOTF)\n",
    "all_inclued_output=all_inclued_output[['customer_id_hashed','Market']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_inclued_output=all_inclued_output.rename(columns={\"Market\":\"segment\"})\n",
    "all_inclued_output['segment']=all_inclued_output['segment']+\"_0726SOTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_inclued_output.to_csv(\"./BL_all_ids_for_the_5_mkts_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_hashed</th>\n",
       "      <th>email_address_hash</th>\n",
       "      <th>customer_zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9b7f9a1ba51dbae2a393c273ce8c9aa9bf610dc03410fd...</td>\n",
       "      <td>089bc373eda16a771e023d87ee368fb961c4ff9b36c7d6...</td>\n",
       "      <td>44402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c2c5bd862113c4e77c918dd22a25744f2e1224ea63b166...</td>\n",
       "      <td>065edf59d0ed26bea352be5f44fd7a6f688a0fa28cddb3...</td>\n",
       "      <td>37066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  customer_id_hashed  \\\n",
       "0  9b7f9a1ba51dbae2a393c273ce8c9aa9bf610dc03410fd...   \n",
       "1  c2c5bd862113c4e77c918dd22a25744f2e1224ea63b166...   \n",
       "\n",
       "                                  email_address_hash customer_zip_code  \n",
       "0  089bc373eda16a771e023d87ee368fb961c4ff9b36c7d6...             44402  \n",
       "1  065edf59d0ed26bea352be5f44fd7a6f688a0fa28cddb3...             37066  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfiddetail = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/combined_masterids_up_to_20181229_JL.csv',nrows = samplerows)\n",
    "dfiddetail = dfiddetail.drop_duplicates('customer_id_hashed')\n",
    "\n",
    "dfiddetail=dfiddetail[['customer_id_hashed','email_address_hash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_2018Q1_2019Q1=pd.read_table(\"/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/All Rewards Members 2018-02-04 - 2019-05-04.zip\",\n",
    "                                             compression=\"zip\",dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_file_gap=pd.read_table(\"/home/jian/BigLots/New_Sing_Ups_2018_Fiscal_Year/MediaStorm Rewards Master P4 2019 - no transaction info.zip\",\n",
    "                                             compression=\"zip\",dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606131, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_master_files_after_201906=list(recurisve_file_gen(\"/home/jian/BigLots/2019_by_weeks/\"))\n",
    "all_master_files_after_201906=[x for x in all_master_files_after_201906 if (\"aster\" in x) & (\".txt\" in x)]\n",
    "all_master_files_after_201906=[x for x in all_master_files_after_201906 if x.split(\"ts/2019_by_weeks/MediaStorm_\")[1][:10]>\"2019-06-01\"]\n",
    "df_master_all_later_after_201906=pd.DataFrame()\n",
    "for file in all_master_files_after_201906:\n",
    "    df=pd.read_table(file,dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash'])\n",
    "    df_master_all_later_after_201906=df_master_all_later_after_201906.append(df)\n",
    "df_master_all_later_after_201906.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiddetail=df_master_all_later_after_201906.append(master_file_gap).append(master_file_2018Q1_2019Q1).append(dfiddetail)\n",
    "dfiddetail=dfiddetail.drop_duplicates(\"customer_id_hashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885985, 14)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_inclued_output=pd.merge(all_inclued_output,dfiddetail,on=\"customer_id_hashed\",how=\"left\")\n",
    "all_inclued_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inclued_output=all_inclued_output[['customer_id_hashed','email_address_hash','segment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_folder=\"/home/jian/Projects/Big_Lots/Analysis/2019_Q2/Planner_Requests/audience_Kaylar_SOTF_markets_20190708/\"+\"output_\"+str(datetime.datetime.now().date())+\"/\"\n",
    "try:\n",
    "    os.stat(writer_folder)\n",
    "except:\n",
    "    os.mkdir(writer_folder)\n",
    "all_inclued_output_non_naemail=all_inclued_output[pd.notnull(all_inclued_output['email_address_hash'])]\n",
    "\n",
    "for mkt,df_group in all_inclued_output_non_naemail.groupby(\"segment\"):\n",
    "    df_group.to_csv(writer_folder+\"BL_LR_segments_\"+mkt.replace(\" \",\"-\")+\"_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_count_by_mkt_email=all_inclued_output_non_naemail.groupby(\"segment\")['email_address_hash'].count().to_frame().reset_index()\n",
    "summary_count_by_mkt_id=all_inclued_output.groupby(\"segment\")['customer_id_hashed'].count().to_frame().reset_index()\n",
    "\n",
    "summary_df_output=pd.merge(summary_count_by_mkt_email,summary_count_by_mkt_id,on=\"segment\")\n",
    "summary_df_output.to_csv(\"/home/jian/Projects/Big_Lots/Analysis/2019_Q2/Planner_Requests/audience_Kaylar_SOTF_markets_20190708/summary_audience_df_0726_SOTF_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
