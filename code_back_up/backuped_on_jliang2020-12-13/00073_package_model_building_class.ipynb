{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SM_Logistic_Model:\n",
    "    \n",
    "    # 1\n",
    "    def __init__(self,\n",
    "                 config_path=\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/config.json\"):\n",
    "        self.output_folder=\"\"\n",
    "        \n",
    "        self.high_date=\"\"\n",
    "        self.username=\"\"\n",
    "        self.password=\"\"\n",
    "        self.database=\"\"\n",
    "        self.recent_n_month=\"\"\n",
    "\n",
    "        self.table_df_1=\"\"\n",
    "        self.table_2_1=\"\"\n",
    "        self.table_2_2=\"\"\n",
    "        self.table_0_train=\"\"\n",
    "        self.table_0_test=\"\"\n",
    "        \n",
    "        self.iv_end_date=\"\"\n",
    "        self.iv_start_date=\"\"\n",
    "        self.dv_end_date=\"\"\n",
    "        self.dv_start_date=\"\"        \n",
    "        \n",
    "        self.df_train_all=pd.DataFrame()\n",
    "        \n",
    "        self.df_date_range=pd.DataFrame()\n",
    "        self.output_folder=\"\"\n",
    "        self.output_path=\"\"\n",
    "        self.df_department_name=pd.read_table(\"/home/jian/BigLots/static_files/MediaStorm Data Extract - Department Names.txt\",\n",
    "                                              sep=\"|\").drop_duplicates() \n",
    "    # 2 config\n",
    "    def config(self):\n",
    "        with open('./config.json', 'rb') as f:\n",
    "            data = json.load(f)\n",
    "        self.high_date=data['pos_end_date']\n",
    "        self.username=data['username']\n",
    "        self.password=data['password']\n",
    "        self.database=data['database']\n",
    "        self.recent_n_month=data['recent_n_month']\n",
    "        \n",
    "        self.table_df_1=\"\"\n",
    "        self.table_2_1=\"\"\n",
    "        self.table_2_2=\"\"\n",
    "        self.table_0_train=\"\"\n",
    "        self.table_0_test=\"\"\n",
    "        \n",
    "        self.iv_end_date=\"\"\n",
    "        self.iv_start_date=\"\"\n",
    "        self.dv_end_date=\"\"\n",
    "        self.dv_start_date=\"\"  \n",
    "        \n",
    "        \n",
    "               \n",
    "    \n",
    "        \n",
    "    # 2     \n",
    "    def remove_low_variate_iv(self,r_variance=0.98):\n",
    "        threshold_variance_iv=r_variance*(1-r_variance)\n",
    "        selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "        df_redused_X=selector.fit_transform(self.df_X)\n",
    "        print(\"df_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "\n",
    "        indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "        self.X_features=self.df_X.iloc[:,indices].columns.tolist()\n",
    "\n",
    "        self.df_X=self.df_X.iloc[:,indices]\n",
    "    # 3\n",
    "    def remove_high_correlated_table(self,coorelation_threshold=0.8,pre_defined_kept_list=None,pre_defined_removed_list=None):\n",
    "\n",
    "        df_coor_X=self.df_X.corr().abs()\n",
    "        df_coor_X=df_coor_X.unstack().reset_index()\n",
    "        df_coor_X.columns=['iv_1','iv_2','coor']\n",
    "        df_coor_X_high=df_coor_X[df_coor_X['iv_1']!=df_coor_X['iv_2']]\n",
    "        df_coor_X_high=df_coor_X_high[df_coor_X_high['coor']>coorelation_threshold]\n",
    "        df_coor_X_high['high_coor_pairs']=df_coor_X_high[['iv_1','iv_2']].values.tolist()\n",
    "\n",
    "        list_highly_pairs=df_coor_X_high['high_coor_pairs'].tolist()\n",
    "        list_highly_pairs=[sorted(x) for x in list_highly_pairs]\n",
    "        print(\"len(list_highly_pairs)\",len(list_highly_pairs))\n",
    "\n",
    "        list_highly_pairs=[str(x) for x in list_highly_pairs]\n",
    "        list_highly_pairs=list(set(list_highly_pairs))\n",
    "        list_highly_pairs=[eval(x) for x in list_highly_pairs]\n",
    "\n",
    "        list_set_coor_unique=[]\n",
    "        for x in list_highly_pairs:\n",
    "            list_set_coor_unique=list_set_coor_unique+x\n",
    "        list_set_coor_unique=list(set(list_set_coor_unique))\n",
    "        if pre_defined_removed_list:\n",
    "            set_kept=set(pre_defined_kept_list)\n",
    "            set_removed=set(pre_defined_removed_list)\n",
    "        else:\n",
    "            print(\"we have %i pairs of highly correlated ivs, with %i unique values\"%(len(list_highly_pairs),len(list_set_coor_unique)))\n",
    "            print(\"pairs as below: \\n%s\"%str(list_highly_pairs))\n",
    "            list_kept=[]\n",
    "            list_removed=[]\n",
    "\n",
    "            input_str=np.nan\n",
    "            print(\"type in the col to put into kept(type 1 to move forward): \")\n",
    "            while input_str!=str(1):\n",
    "                input_str=input()\n",
    "                try:                \n",
    "                    list_set_coor_unique.remove(input_str)\n",
    "                    list_kept.append(input_str)\n",
    "                    print(\"list_kept for now: \\n%s\"%list_kept)\n",
    "                    print(\"remaining unique elements: \\n%s\"%list_set_coor_unique)\n",
    "                except:\n",
    "                    print(\"moving next\")\n",
    "\n",
    "\n",
    "            print(\"type in the col to put into remove(type 2 to move forward): \")    \n",
    "            while input_str!=str(2):\n",
    "                input_str=input()\n",
    "                try:                \n",
    "                    list_set_coor_unique.remove(input_str)\n",
    "                    list_removed.append(input_str)\n",
    "                    print(\"removed for now: \\n%s\"%list_removed)\n",
    "                    print(\"remaining unique elements: \\n%s\"%list_set_coor_unique)\n",
    "                except:\n",
    "                    print(\"moving next\")\n",
    "\n",
    "            set_kept=set(list_kept)\n",
    "            set_removed=set(list_removed)\n",
    "        set_common_cols=set_kept.intersection(set_removed)\n",
    "        if set_common_cols:\n",
    "            print(\"common col appared in both kept and remove: %s\"%str(set_common_cols))\n",
    "        else:\n",
    "            self.X_features=[x for x in self.X_features if x not in set_removed]\n",
    "            self.df_X=self.df_X[self.X_features]\n",
    "        print(\"done of removing highly correlcated cols\")\n",
    "        print(\"cols removed: %s\"%set_removed)\n",
    "        \n",
    "        # qc\n",
    "        df_coor_X=self.df_X.corr().abs()\n",
    "        df_coor_X=df_coor_X.unstack().reset_index()\n",
    "        df_coor_X.columns=['iv_1','iv_2','coor']\n",
    "        df_coor_X_high=df_coor_X[df_coor_X['iv_1']!=df_coor_X['iv_2']]\n",
    "        df_coor_X_high=df_coor_X_high[df_coor_X_high['coor']>coorelation_threshold]\n",
    "        if df_coor_X_high.shape[0]>0:\n",
    "            print(\"still have high correlation above %s\"%coorelation_threshold)\n",
    "            \n",
    "            \n",
    "        self.removed_list_because_coor=list(set_removed)\n",
    "        self.kept_list_because_coor=list(set_kept)\n",
    "        \n",
    "    # 4      \n",
    "    def split_train_test(self,test_pctg=0.25):\n",
    "        self.X_train, self.X_test, self.df_y_train, self.df_y_test = train_test_split(self.df_X, self.df_Y, test_size=test_pctg, random_state=42)\n",
    "\n",
    "        self.X_train_scaled=scale(self.X_train)\n",
    "        self.X_test_scaled=scale(self.X_test)\n",
    "        # note: scale the X just to make the rfe faster\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        print(\"X_test_scaled.shape\",self.X_test_scaled.shape)\n",
    "\n",
    "        print(\"X_train.shape\",self.X_train.shape)\n",
    "        print(\"X_test.shape\",self.X_test.shape)\n",
    "        \n",
    "        \n",
    "        self.input_y_train_list=self.df_y_train.iloc[:,self.n_week_DV-1].values.tolist()\n",
    "        self.input_y_test_list=self.df_y_test.iloc[:,self.n_week_DV-1].values.tolist()\n",
    "        \n",
    "        \n",
    "        list_index_y_train=list(self.df_y_train.index)\n",
    "        self.list_ids_y_train=[self.list_all_ids[i] for i in list_index_y_train]\n",
    "        list_index_y_test=list(self.df_y_test.index)\n",
    "        self.list_ids_y_test=[self.list_all_ids[i] for i in list_index_y_test]\n",
    "    # 5\n",
    "    def run_updating_df_count(self):\n",
    "        df_trian_X_count=pd.DataFrame({\"row\":self.X_train.shape[0],\"col\":self.X_train.shape[1]},index=[\"X_train\"])\n",
    "        df_test_X_count=pd.DataFrame({\"row\":self.X_test.shape[0],\"col\":self.X_test.shape[1]},index=[\"X_test\"])\n",
    "        self.db_row_counts=self.db_row_counts.append(df_trian_X_count)\n",
    "        self.db_row_counts=self.db_row_counts.append(df_test_X_count)\n",
    "    # 6    \n",
    "    def generate_DV_distribution(self):\n",
    "        df_y_train_count=pd.DataFrame()\n",
    "        for col in self.df_y_train.columns.tolist():\n",
    "            count_1=self.df_y_train[self.df_y_train[col]==1].shape[0]\n",
    "            count_0=self.df_y_train[self.df_y_train[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_train_count=df_y_train_count.append(df)\n",
    "        df_y_train_count.insert(0,\"set\",\"y_train\")\n",
    "\n",
    "\n",
    "        df_y_test_count=pd.DataFrame()\n",
    "        for col in self.df_y_test.columns.tolist():\n",
    "            count_1=self.df_y_test[self.df_y_test[col]==1].shape[0]\n",
    "            count_0=self.df_y_test[self.df_y_test[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_test_count=df_y_test_count.append(df)\n",
    "        df_y_test_count.insert(0,\"set\",\"y_test\")\n",
    "\n",
    "        self.df_y_train_count=df_y_train_count\n",
    "        self.df_y_test_count=df_y_test_count\n",
    "    # 7\n",
    "    def select_from_model_n_features(self, N_feature_select_from_models=60):\n",
    "        print(\"Starting select_from_model_n_features: \",datetime.datetime.now())\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(random_state=0,\n",
    "                                                                solver=\"saga\",\n",
    "                                                                max_iter=2000,\n",
    "                                                                n_jobs=24,\n",
    "                                                                tol=0.0001),\n",
    "                                   max_features=N_feature_select_from_models,\n",
    "                                   threshold=-np.inf).fit(self.X_train_scaled, self.input_y_train_list)\n",
    "\n",
    "        print(\"selector.threshold_\",selector.threshold_)\n",
    "        selector_support_FROMMODEL=selector.get_support()\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_FROMMODEL) if v==True]\n",
    "\n",
    "        self.X_train=self.X_train.loc[:,self.X_features]\n",
    "        self.X_test=self.X_test.loc[:,self.X_features]\n",
    "\n",
    "        print(\"X_train.shape\",self.X_train.shape)\n",
    "        print(\"X_test.shape\",self.X_test.shape)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        self.X_train_scaled=scale(self.X_train)\n",
    "        self.X_test_scaled=scale(self.X_test)\n",
    "        # note: scale the X just to make the rfe faster\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        print(\"X_test_scaled.shape\",self.X_test_scaled.shape)\n",
    "        print(\"Done select_from_model_n_features: \",datetime.datetime.now())\n",
    "    # 8    \n",
    "    def select_REF(self):\n",
    "        print(\"Starting select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        estimator = LogisticRegression(fit_intercept=True,solver='saga',max_iter=2000,n_jobs=24,tol=0.001)\n",
    "        selector = RFE(estimator,step=1,n_features_to_select=40)\n",
    "        selector = selector.fit(self.X_train_scaled, self.input_y_train_list)\n",
    "        selector_support_REF=selector.support_\n",
    "        print(\"Done select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_REF) if v==True]\n",
    "\n",
    "        self.X_train=self.X_train.loc[:,self.X_features]\n",
    "        self.X_test=self.X_test.loc[:,self.X_features]\n",
    "\n",
    "        print(\"X_train.shape\",self.X_train.shape)\n",
    "        print(\"X_test.shape\",self.X_test.shape)\n",
    "        self.X_train_scaled=scale(self.X_train)\n",
    "        self.X_test_scaled=scale(self.X_test)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        print(\"X_test_scaled.shape\",self.X_test_scaled.shape)\n",
    "    # 9\n",
    "    def forwards_feature_elimination_based_on_p_and_vif(self,niter=50,method=\"lbfgs\",p_tol=0.1,vif_tol=5):\n",
    "        len_x_features=self.X_train.shape[1]\n",
    "        len_x_features_new=0\n",
    "        df_x_dropped=self.X_train.copy()\n",
    "        i_iter=0\n",
    "        while len_x_features_new<len_x_features and i_iter<=100:\n",
    "            i_iter+=1\n",
    "            len_x_features=df_x_dropped.shape[1]\n",
    "            mod=sm.Logit(self.input_y_train_list,sm.add_constant(df_x_dropped),niter=niter,method=method)\n",
    "            res=mod.fit()\n",
    "            table=res.summary2().tables[1]   \n",
    "            X=add_constant(scale(df_x_dropped))\n",
    "            list_cols=table.index.tolist()\n",
    "            table[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "\n",
    "            max_vif=table['VIF Factor'].max()\n",
    "            max_p=table['P>|z|'].max()\n",
    "            \n",
    "            if max_vif>vif_tol:\n",
    "                col_name_to_drop=table.index[table['VIF Factor']==max_vif][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to high vif\"%col_name_to_drop)\n",
    "                \n",
    "            elif max_p>p_tol:\n",
    "                col_name_to_drop=table.index[table['P>|z|']==max_p][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to p value\"%col_name_to_drop)\n",
    "            else:\n",
    "                i_iter+=100\n",
    "                \n",
    "\n",
    "        self.X_train=df_x_dropped\n",
    "        self.X_features=df_x_dropped.columns.tolist()\n",
    "        self.X_test=self.X_test[self.X_features]\n",
    "        self.X_train_scaled=scale(self.X_train)\n",
    "        self.X_test_scaled=scale(self.X_test)\n",
    "    # 10\n",
    "    def run_sm_logR_model(self):\n",
    "        self.sm_model=sm.Logit(self.input_y_train_list,sm.add_constant(self.X_train),niter=50,method=\"lbfgs\")\n",
    "        self.res_of_model=self.sm_model.fit()\n",
    "        self.summary_table_over=self.res_of_model.summary2().tables[0].reset_index()\n",
    "        self.summary_table_output=self.res_of_model.summary2().tables[1].reset_index()\n",
    "        \n",
    "        \n",
    "        std=self.sm_model.exog.std(axis=0)\n",
    "        std[0] = 1\n",
    "        tt = self.res_of_model.t_test(np.diag(std))\n",
    "        df_std_coef=tt.summary_frame()\n",
    "        list_std_coefficients=df_std_coef['coef'].tolist()\n",
    "        self.summary_table_output['std_coef']=list_std_coefficients\n",
    "        \n",
    "        self.list_train_pred=self.res_of_model.predict()\n",
    "        self.list_test_pred=self.res_of_model.predict(sm.add_constant(self.X_test))\n",
    "        \n",
    "        coefficient_of_dermination = r2_score(self.input_y_train_list, self.list_train_pred)\n",
    "        self.summary_table_over=self.summary_table_over.append(pd.DataFrame({\"index\":[8],0:\"calculated_r_squared\",1:coefficient_of_dermination},index=[8]))\n",
    "    \n",
    "        #VIF\n",
    "        X=add_constant(self.X_train_scaled)\n",
    "        list_cols=self.summary_table_output['index'].tolist()\n",
    "        self.summary_table_output[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        self.summary_table_output=self.summary_table_output.sort_values(\"std_coef\")\n",
    "    # 11\n",
    "    def generate_step_table_of_test_SM(self,threshold_list = [(x+1)/100 for x in range(0,100)]):\n",
    "        list_prob_test=self.res_of_model.predict(sm.add_constant(self.X_test))\n",
    "        df_output=pd.DataFrame()\n",
    "        for i in threshold_list:\n",
    "            y_test_pred=[1 if x>i else 0 for x in list_prob_test]\n",
    "\n",
    "            accuracy_score = metrics.accuracy_score(self.input_y_test_list,y_test_pred)    \n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(self.input_y_test_list, y_test_pred).ravel()\n",
    "            # \n",
    "            TPR=tp/(tp+fn) #recall\n",
    "            FNR=fn/(tp+fn)\n",
    "            FPR=fp/(fp+tn)\n",
    "            TNR=tn/(fp+tn)\n",
    "\n",
    "            PPV=tp/(tp+fp) #precission\n",
    "            f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "            df=pd.DataFrame({\"predicted_positive\":len([x for x in y_test_pred if x==1]),\n",
    "                             \"predicted_negative\":len([x for x in y_test_pred if x==0]),\n",
    "                             \"accuracy_score\":accuracy_score,\n",
    "                             'true_negative':tn,\n",
    "                             'false_positive':fp,\n",
    "                             'false_negative':fn,\n",
    "                             'true_positive':tp,\n",
    "                             'true_positive_rate':TPR,\n",
    "                             'false_negative_rate':FNR,\n",
    "                             'false_positive_rate':FPR,\n",
    "                             'true_negative_rate':TNR,\n",
    "                             'precission_(Positive predictive value)':PPV,\n",
    "                             'f1_score':f1_score\n",
    "                            },index=[i])\n",
    "            df_output=df_output.append(df)\n",
    "        \n",
    "        self.df_step_table=df_output\n",
    "    # 11-2\n",
    "    def generate_step_table_of_test_SM_001_range(self,start_prob_pctg,end_prob_pctg,step=0.001):\n",
    "        threshold_list = [(x+1)/1000 for x in range(int(start_prob_pctg*1000),int(end_prob_pctg*1000))]\n",
    "        \n",
    "        list_prob_test=self.res_of_model.predict(sm.add_constant(self.X_test))\n",
    "        df_output=pd.DataFrame()\n",
    "        for i in threshold_list:\n",
    "            y_test_pred=[1 if x>i else 0 for x in list_prob_test]\n",
    "\n",
    "            accuracy_score = metrics.accuracy_score(self.input_y_test_list,y_test_pred)    \n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(self.input_y_test_list, y_test_pred).ravel()\n",
    "            # \n",
    "            TPR=tp/(tp+fn) #recall\n",
    "            FNR=fn/(tp+fn)\n",
    "            FPR=fp/(fp+tn)\n",
    "            TNR=tn/(fp+tn)\n",
    "\n",
    "            PPV=tp/(tp+fp) #precission\n",
    "            f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "            df=pd.DataFrame({\"predicted_positive\":len([x for x in y_test_pred if x==1]),\n",
    "                             \"predicted_negative\":len([x for x in y_test_pred if x==0]),\n",
    "                             \"accuracy_score\":accuracy_score,\n",
    "                             'true_negative':tn,\n",
    "                             'false_positive':fp,\n",
    "                             'false_negative':fn,\n",
    "                             'true_positive':tp,\n",
    "                             'true_positive_rate':TPR,\n",
    "                             'false_negative_rate':FNR,\n",
    "                             'false_positive_rate':FPR,\n",
    "                             'true_negative_rate':TNR,\n",
    "                             'precission_(Positive predictive value)':PPV,\n",
    "                             'f1_score':f1_score\n",
    "                            },index=[i])\n",
    "            df_output=df_output.append(df)\n",
    "        \n",
    "        self.df_step_table_2=df_output\n",
    "        \n",
    "    # 12\n",
    "    def select_best_scored_pred_prob(self,defined_best_score=None):\n",
    "        if defined_best_score is None:\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_confusion_matrix(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative']),axis=1)\n",
    "            threshold_max_selfdefinedscore=self.df_step_table[self.df_step_table['self_defined_score']==self.df_step_table['self_defined_score'].max()].index[0]\n",
    "            self.threshold_max_selfdefinedscore=threshold_max_selfdefinedscore\n",
    "            self.df_step_table=self.df_step_table.reset_index()\n",
    "            self.df_confusion_table=self.df_step_table.loc[self.df_step_table['index']==threshold_max_selfdefinedscore,:]\n",
    "        else:\n",
    "            threshold_max_selfdefinedscore=defined_best_score\n",
    "            self.threshold_max_selfdefinedscore=threshold_max_selfdefinedscore\n",
    "            self.df_step_table_2=self.df_step_table_2.reset_index()\n",
    "            self.df_confusion_table=self.df_step_table_2.loc[self.df_step_table_2['index']==threshold_max_selfdefinedscore,:]\n",
    "            \n",
    "    # 13\n",
    "    def generate_gain_chart(self):\n",
    "        self.df_gainchart_train=generate_gain_chart_function(df_X=self.X_train,\n",
    "                                                             list_y=self.input_y_train_list,\n",
    "                                                             list_ids=self.list_ids_y_train,\n",
    "                                                             result_sm_model=self.res_of_model,\n",
    "                                                             threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                             list_selected_features=self.X_features)\n",
    "        \n",
    "        self.df_gainchart_test=generate_gain_chart_function(df_X=self.X_test,\n",
    "                                                            list_y=self.input_y_test_list,\n",
    "                                                            list_ids=self.list_ids_y_test,\n",
    "                                                            result_sm_model=self.res_of_model,\n",
    "                                                            threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                            list_selected_features=self.X_features)\n",
    "    def save_outputs(self):\n",
    "        \n",
    "        writer=pd.ExcelWriter(self.output_path,engine=\"xlsxwriter\")\n",
    "        \n",
    "        self.db_row_counts.to_excel(writer,\"df_dataset_shape\")\n",
    "        self.df_date_range.to_excel(writer,\"df_date_range\")\n",
    "        self.df_y_train_count.to_excel(writer,\"df_y_train_count\")\n",
    "        self.df_y_test_count.to_excel(writer,\"df_y_test_count\")\n",
    "        self.summary_table_over.to_excel(writer,\"summary_table_over\")\n",
    "        self.summary_table_output.to_excel(writer,\"summary_table_output\")\n",
    "        self.df_step_table.to_excel(writer,\"step_table\",index=True)\n",
    "        if self.df_step_table_2 is not None:\n",
    "            self.df_step_table_2.to_excel(writer,\"step_table_2\",index=True)\n",
    "        self.df_confusion_table.to_excel(writer,\"select_score_matrix\",index=False)\n",
    "        \n",
    "        self.df_gainchart_train.to_excel(writer,\"gainchart_train\",index=False)\n",
    "        self.df_gainchart_test.to_excel(writer,\"gainchart_test\",index=False)\n",
    "        self.df_department_name.to_excel(writer,\"department_name\",index=False)\n",
    "        writer.save()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_config=json.load(open(\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/config.json\"))\n",
    "high_date=dict_config['pos_end_date']\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "recent_n_month=dict_config['recent_n_month']\n",
    "\n",
    "dict_tables=json.load(open(\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/table_names_%s.json\"%str(high_date).replace(\"-\",\"\")))\n",
    "table_df_1=dict_tables['table_df_1']\n",
    "table_2_1=dict_tables['table_2_1']\n",
    "table_2_2=dict_tables['table_2_2']\n",
    "table_0_train=dict_tables['table_crm_id_list_train']\n",
    "table_0_test=dict_tables['table_crm_id_list_test']\n",
    "\n",
    "BL_engine=sqlalchemy.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count(*)    1975547\n",
      "Name: 0, dtype: int64 table_pred_1_crm_up_to_20200704\n",
      "count(*)    1975547\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_1_pos_until_20200704\n",
      "count(*)    1975547\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_2_pos_until_20200704\n",
      "count(*)    1000000\n",
      "Name: 0, dtype: int64 crm_table_id_list_train_20200704\n"
     ]
    }
   ],
   "source": [
    "for t in [table_df_1,table_2_1,table_2_2,table_0_train]:\n",
    "    print(pd.read_sql(\"select count(*) from %s\"%t,con=BL_engine).iloc[0],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_df0_train=pd.read_sql(\"desc %s\"%table_0_train,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_df_1=pd.read_sql(\"desc %s\"%table_df_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_1=pd.read_sql(\"desc %s\"%table_2_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_2=pd.read_sql(\"desc %s\"%table_2_2,con=BL_engine)['Field'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_no_need=['sign_up_location','customer_zip_code','nearest_BL_store',\n",
    "              'week_1st_trans','week_recent_one_trans','week_recent_two_trans']\n",
    "for col_remove in cols_no_need:\n",
    "    col_list_df_1=[x for x in col_list_df_1 if x != col_remove and x!=\"customer_id_hashed\"]\n",
    "    col_list_2_1=[x for x in col_list_2_1 if x != col_remove and x!=\"id\"]\n",
    "    col_list_2_2=[x for x in col_list_2_2 if x != col_remove and x!=\"id\"]\n",
    "    \n",
    "sql_str_cols_df0_train=str([\"t0.\"+x for x in col_list_df0_train]).replace(\"'\",\"\")[1:-1]  \n",
    "sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_df_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_2_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_2_2]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_all=\", \".join([sql_str_cols_df0_train,sql_str_cols_df_1,sql_str_cols_2_1,sql_str_cols_2_2])\n",
    "# sql_str_cols_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queary=\"SELECT %s from %s as t0 \\\n",
    "left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed \\\n",
    "left join %s as t2_1 on t0.customer_id_hashed=t2_1.id \\\n",
    "left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%(sql_str_cols_all,table_0_train,table_df_1,table_2_1,table_2_2)\n",
    "# queary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 15:57:24.063688\n",
      "(1000000, 249)\n",
      "2020-08-13 16:10:34.706826\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "df_train=pd.read_sql(queary,con=BL_engine)\n",
    "print(df_train.shape)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_nunique==1 dropped: department_115_trans\n",
      "col_nunique==1 dropped: department_115_1st_trans\n",
      "col_nunique==1 dropped: department_540_1st_trans\n",
      "col_nunique==1 dropped: department_115_recent_one\n",
      "col_nunique==1 dropped: department_540_recent_one\n",
      "col_nunique==1 dropped: department_115_recent_two\n"
     ]
    }
   ],
   "source": [
    "list_cols=df_train.columns.tolist()\n",
    "\n",
    "for col in list_cols:\n",
    "    if df_train[col].nunique()==1:\n",
    "        del df_train[col]\n",
    "        print(\"col_nunique==1 dropped: %s\"%col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 16:52:22.396399\n",
      "(300000, 243)\n",
      "2020-08-13 16:59:15.046820\n",
      "(300000, 240)\n",
      "(1000000, 240)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "df_train_copy=df_train.head(3*10**5)\n",
    "print(df_train_copy.shape)\n",
    "df_train_copy=df_train_copy.T.drop_duplicates().T\n",
    "print(datetime.datetime.now())\n",
    "print(df_train_copy.shape)\n",
    "\n",
    "list_cols_keep=df_train_copy.columns.tolist()\n",
    "df_train=df_train[list_cols_keep]\n",
    "print(df_train.shape)\n",
    "\n",
    "del df_train_copy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv_start_date\n",
    "iv_end_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()\n",
    "iv_start_date=iv_end_date-datetime.timedelta(days=int(np.ceil(365*recent_n_month/12)))\n",
    "\n",
    "dv_end_date=iv_end_date+datetime.timedelta(days=28)\n",
    "dv_start_date=iv_end_date+datetime.timedelta(days=1)  \n",
    "df_date_range=pd.DataFrame({\"start\":[iv_start_date,dv_start_date],\"end\":[iv_end_date,dv_end_date]},index=['IVs',\"DVs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_confusion_matrix(tp,tn,fp,fn):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "    \n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "    f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "    \n",
    "    score=9*tp-8*fn*0.5-fp\n",
    "    # the score ignored the f1 and overall accuracy due to low pctg\n",
    "    \n",
    "    \n",
    "    \n",
    "    # consider the profit vs lost 10:1 (30%*$30) vs (cpc*frequecy or click)\n",
    "    # which means 1 missed (fn) is 10 times of 1 wrong targeted (fp)\n",
    "    # very aribitury\n",
    "    return score\n",
    "\n",
    "def write_aggregate_func_gain_chart(list_selected_features,df_pred_table_detail):\n",
    "    func_dict={\"customer_id_hashed\":\"count\"}\n",
    "    list_cols_for_ratios=['y_true','y_hat']\n",
    "    for col in list_selected_features:\n",
    "        if len(df_pred_table_detail[col].unique())==2:\n",
    "            func_dict.update({col:'sum'})\n",
    "            list_cols_for_ratios.append(col)\n",
    "        else:\n",
    "            func_dict.update({col:\"mean\"})\n",
    "    func_dict.update({\"y_true\":\"sum\"})\n",
    "    func_dict.update({\"y_hat\":\"sum\"})\n",
    "    # func_dict.update({\"pred_prob\":['max','min']})\n",
    "    return func_dict,list_cols_for_ratios\n",
    "\n",
    "\n",
    "def generate_gain_chart_function(df_X,list_y,list_ids,result_sm_model,threshold,list_selected_features):\n",
    "    list_pred_prob=result_sm_model.predict(sm.add_constant(df_X)).values\n",
    "    df_pred_by_id=pd.DataFrame({\"customer_id_hashed\":list_ids,\"pred_prob\":list_pred_prob},index=range(len(list_pred_prob)))\n",
    "    copy_xtrain=df_X.copy().reset_index()\n",
    "    del copy_xtrain['index']\n",
    "    df_pred_by_id=pd.concat([df_pred_by_id,copy_xtrain],axis=1,ignore_index=False)\n",
    "\n",
    "    df_pred_by_id['decile']=pd.qcut(df_pred_by_id['pred_prob'], 10, labels=False)\n",
    "    df_pred_by_id['decile']=df_pred_by_id['decile'].apply(lambda x: \"D\"+str(x+1).zfill(2))\n",
    "    df_pred_by_id['y_true']=list_y\n",
    "    df_pred_by_id['y_hat']=np.where(df_pred_by_id['pred_prob']>threshold,1,0)\n",
    "\n",
    "    agg_func,list_cols_to_get_ratio=write_aggregate_func_gain_chart(list_selected_features,df_pred_by_id)\n",
    "    df_gainchart=df_pred_by_id.groupby(\"decile\")[['customer_id_hashed']+list_selected_features].agg(agg_func).reset_index()\n",
    "\n",
    "    df_prob_max=df_pred_by_id.groupby(\"decile\")['pred_prob'].max().to_frame().reset_index().rename(columns={\"pred_prob\":\"max_prob\"})\n",
    "    df_prob_min=df_pred_by_id.groupby(\"decile\")['pred_prob'].min().to_frame().reset_index().rename(columns={\"pred_prob\":\"min_prob\"})\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_max,on=\"decile\")\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_min,on=\"decile\")\n",
    "    df_gainchart.insert(2,\"actual_ratio\",df_gainchart['y_true']/df_gainchart['customer_id_hashed'])\n",
    "    df_gainchart.insert(3,\"pred_ratio\",df_gainchart['y_hat']/df_gainchart['customer_id_hashed'])\n",
    "\n",
    "\n",
    "    df_gainchart.insert(4,\"max_pred_prob\",df_gainchart['max_prob'])\n",
    "    df_gainchart.insert(5,\"min_pred_prob\",df_gainchart['min_prob'])\n",
    "    del df_gainchart['max_prob']\n",
    "    del df_gainchart['min_prob']\n",
    "\n",
    "    return df_gainchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
