{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n",
      "Job start: model building 2020-11-24 23:25:15.213052\n",
      "count(*)    21255179\n",
      "Name: 0, dtype: int64 table_pred_1_crm_up_to_20200822\n",
      "count(*)    21255179\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_1_pos_until_20200822\n",
      "count(*)    21255179\n",
      "Name: 0, dtype: int64 all_NEall_id_pred_pos_2_2_pos_until_20200822\n",
      "count(*)    1000000\n",
      "Name: 0, dtype: int64 crm_table_id_list_train_20200822\n",
      "2020-11-24 23:25:42.676966\n",
      "(1000000, 246)\n",
      "2020-11-24 23:38:45.622481\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: total_trans_since_registration\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: trans_in_store\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: unique_stores\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: trans_online\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_250_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_526_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_250_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_526_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_1st_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_250_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_526_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_recent_one\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: week_counts_to_now_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: recent_two_trans_also_1st\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: purchase_channel_1st_trans_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: total_sales_recent_two_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: total_units_recent_two_trans\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_minus_one_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_108_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_109_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_110_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_111_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_114_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_115_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_120_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_130_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_140_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_150_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_160_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_170_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_210_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_230_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_250_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_270_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_310_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_320_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_330_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_340_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_350_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_351_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_352_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_353_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_354_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_355_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_360_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_361_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_362_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_363_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_364_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_365_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_366_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_367_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_370_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_410_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_425_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_470_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_480_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_510_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_520_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_521_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_526_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_530_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_540_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_550_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_560_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_608_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_610_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_612_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_615_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_710_recent_two\n",
      "df_train_trans_1_only, col_nunique<=1 dropped: department_800_recent_two\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_1st_trans\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: recent_one_trans_also_1st\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_365_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_540_recent_one\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_115_recent_two\n",
      "df_train_trans_2_plus, col_nunique<=1 dropped: department_540_recent_two\n",
      "df_train_trans_1_only 2020-11-24 23:39:05.952069\n",
      "df_train_trans_1_only (199530, 164)\n",
      "df_train_trans_1_only 2020-11-24 23:41:33.667723\n",
      "(199530, 111)\n",
      "df_train_trans_2_plus 2020-11-24 23:41:33.668060\n",
      "df_train_trans_2_plus_copy (300000, 238)\n",
      "df_train_trans_2_plus_copy 2020-11-24 23:47:43.590829\n",
      "(300000, 235)\n",
      "df_train_trans_2_plus (800470, 235)\n",
      "nearest_BL_dist with na to delete 43438\n",
      "nearest_BL_dist with na to delete 28001\n",
      "df_train_trans_2_plus_X reduced to the shape due to 0.98 variante (757032, 187)\n",
      "df_train_trans_1_only_X reduced to the shape due to 0.98 variante (171529, 82)\n",
      "2020-11-24 23:47:54.960510 Start remove correlated cols: df_train_trans_2_plus_X (757032, 187)\n",
      "total_trans_since_registration removed due to high coor with others\n",
      "total_items removed due to high coor with others\n",
      "department_110_trans removed due to high coor with others\n",
      "2020-11-24 23:48:52.690277 Done remove correlated cols: df_train_trans_2_plus_X (757032, 184)\n",
      "2020-11-24 23:48:52.690529 Start remove correlated cols: df_train_trans_1_only_X (171529, 82)\n",
      "department_350_1st_trans removed due to high coor with others\n",
      "department_310_1st_trans removed due to high coor with others\n",
      "department_710_1st_trans removed due to high coor with others\n",
      "department_355_1st_trans removed due to high coor with others\n",
      "2020-11-24 23:48:55.211941 Done remove correlated cols: df_train_trans_1_only_X (171529, 78)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "print(sklearn.__version__)\n",
    "print(\"Job start: model building\", datetime.datetime.now())\n",
    "\n",
    "\n",
    "\n",
    "dict_config=json.load(open(\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/config.json\"))\n",
    "high_date=dict_config['pos_end_date']\n",
    "username=dict_config['username']\n",
    "password=dict_config['password']\n",
    "database=dict_config['database']\n",
    "recent_n_month=dict_config['recent_n_month']\n",
    "path_dept_name=dict_config[\"path_dept_name\"]\n",
    "model_output_folder=dict_config['model_output_folder']\n",
    "\n",
    "dict_tables=json.load(open(\"/mnt/clients/juba/hqjubaapp02/jliang/Projects/Big_Lots/Predictive_Model/Model_Scripts/table_names_%s.json\"%str(high_date).replace(\"-\",\"\")))\n",
    "table_df_1=dict_tables['table_df_1']\n",
    "table_2_1=dict_tables['table_2_1']\n",
    "table_2_2=dict_tables['table_2_2']\n",
    "table_0_train=dict_tables['table_crm_id_list_train']\n",
    "table_0_test=dict_tables['table_crm_id_list_test']\n",
    "\n",
    "BL_engine=sqlalchemy.create_engine(\"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))\n",
    "nth_cutoff_X_Y=4\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "for t in [table_df_1,table_2_1,table_2_2,table_0_train]:\n",
    "    print(pd.read_sql(\"select count(*) from %s\"%t,con=BL_engine).iloc[0],t)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "col_list_df0_train=pd.read_sql(\"desc %s\"%table_0_train,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_df_1=pd.read_sql(\"desc %s\"%table_df_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_1=pd.read_sql(\"desc %s\"%table_2_1,con=BL_engine)['Field'].values.tolist()\n",
    "col_list_2_2=pd.read_sql(\"desc %s\"%table_2_2,con=BL_engine)['Field'].values.tolist()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "cols_no_need=['sign_up_location','customer_zip_code','nearest_BL_store','distc_to_sign_up',\n",
    "              'week_1st_trans','week_recent_one_trans','week_recent_two_trans']\n",
    "for col_remove in cols_no_need:\n",
    "    col_list_df_1=[x for x in col_list_df_1 if x != col_remove and (x not in [\"customer_id_hashed\", \"sign_up_date\"])]\n",
    "    col_list_2_1=[x for x in col_list_2_1 if x != col_remove and x!=\"id\"]\n",
    "    col_list_2_2=[x for x in col_list_2_2 if x != col_remove and x!=\"id\"]\n",
    "\n",
    "sql_str_cols_df0_train=str([\"t0.\"+x for x in col_list_df0_train]).replace(\"'\",\"\")[1:-1]  \n",
    "sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_df_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_2_1]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_2_2]).replace(\"'\",\"\")[1:-1]\n",
    "sql_str_cols_all=\", \".join([sql_str_cols_df0_train,sql_str_cols_df_1,sql_str_cols_2_1,sql_str_cols_2_2])\n",
    "# sql_str_cols_all\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "dict_cols_in_table={\n",
    "    \"t0_train\":{\"table_name\":table_0_train,\"cols\":['customer_id_hashed']},\n",
    "    \"t0_test\":{\"table_name\":table_0_test,\"cols\":['customer_id_hashed']},\n",
    "    \"t1\":{\"table_name\":table_df_1,\"cols\":col_list_df_1},\n",
    "    \"t2_1\":{\"table_name\":table_2_1,\"cols\":col_list_2_1},\n",
    "    \"t2_2\":{\"table_name\":table_2_2,\"cols\":col_list_2_2}\n",
    "}\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "queary=\"SELECT %s from %s as t0 left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join %s as t2_1 on t0.customer_id_hashed=t2_1.id left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%(sql_str_cols_all,table_0_train,table_df_1,table_2_1,table_2_2)\n",
    "# queary\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "df_train=pd.read_sql(queary,con=BL_engine)\n",
    "print(df_train.shape)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "df_train_trans_1_only=df_train[pd.isnull(df_train['total_sales_recent_two_trans'])]\n",
    "df_train_trans_2_plus=df_train[pd.notnull(df_train['total_sales_recent_two_trans'])]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "list_cols=df_train.columns.tolist()\n",
    "del df_train\n",
    "\n",
    "for col in list_cols:\n",
    "    if df_train_trans_1_only[col].nunique()<=1:\n",
    "        del df_train_trans_1_only[col]\n",
    "        print(\"df_train_trans_1_only, col_nunique<=1 dropped: %s\"%col)\n",
    "\n",
    "for col in list_cols:\n",
    "    if df_train_trans_2_plus[col].nunique()<=1:\n",
    "        del df_train_trans_2_plus[col]\n",
    "        print(\"df_train_trans_2_plus, col_nunique<=1 dropped: %s\"%col)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "print(\"df_train_trans_1_only\",datetime.datetime.now())\n",
    "print(\"df_train_trans_1_only\",df_train_trans_1_only.shape)\n",
    "df_train_trans_1_only=df_train_trans_1_only.T.drop_duplicates().T\n",
    "print(\"df_train_trans_1_only\",datetime.datetime.now())\n",
    "print(df_train_trans_1_only.shape)\n",
    "\n",
    "## \n",
    "print(\"df_train_trans_2_plus\",datetime.datetime.now())\n",
    "df_train_trans_2_plus_copy=df_train_trans_2_plus.head(3*10**5)\n",
    "print(\"df_train_trans_2_plus_copy\",df_train_trans_2_plus_copy.shape)\n",
    "df_train_trans_2_plus_copy=df_train_trans_2_plus_copy.T.drop_duplicates().T\n",
    "print(\"df_train_trans_2_plus_copy\",datetime.datetime.now())\n",
    "print(df_train_trans_2_plus_copy.shape)\n",
    "\n",
    "list_cols_keep=df_train_trans_2_plus_copy.columns.tolist()\n",
    "df_train_trans_2_plus=df_train_trans_2_plus[list_cols_keep]\n",
    "print(\"df_train_trans_2_plus\",df_train_trans_2_plus.shape)\n",
    "\n",
    "del df_train_trans_2_plus_copy\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "list_cols_remove_na_rows=['nearest_BL_dist']\n",
    "\n",
    "for col in df_train_trans_2_plus.columns.tolist():\n",
    "    df_na=df_train_trans_2_plus[pd.isnull(df_train_trans_2_plus[col])]\n",
    "    if df_na.shape[0]>0:\n",
    "        if col in list_cols_remove_na_rows:\n",
    "            df_train_trans_2_plus=df_train_trans_2_plus[pd.notnull(df_train_trans_2_plus[col])]\n",
    "            print(col,\"with na to delete\", df_na.shape[0])\n",
    "        else:\n",
    "            print(\"Warning: nan detected in the df_train_trans_2_plus col -- %s\"%col)\n",
    "\n",
    "\n",
    "for col in df_train_trans_1_only.columns.tolist():\n",
    "    df_na=df_train_trans_1_only[pd.isnull(df_train_trans_1_only[col])]\n",
    "    if df_na.shape[0]>0:\n",
    "        if col in list_cols_remove_na_rows:\n",
    "            df_train_trans_1_only=df_train_trans_1_only[pd.notnull(df_train_trans_1_only[col])]\n",
    "            print(col,\"with na to delete\", df_na.shape[0])\n",
    "        else:\n",
    "            print(\"Warning: nan detected in the df_train_trans_1_only col -- %s\"%col)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "list_train_trans_2_plus_id=df_train_trans_2_plus.iloc[:,0].values.tolist()\n",
    "df_train_trans_2_plus_Y=df_train_trans_2_plus.iloc[:,1:nth_cutoff_X_Y]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus.iloc[:,nth_cutoff_X_Y:]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus_X.astype(float)\n",
    "del df_train_trans_2_plus\n",
    "\n",
    "list_train_trans_1_only_id=df_train_trans_1_only.iloc[:,0].values.tolist()\n",
    "df_train_trans_1_only_Y=df_train_trans_1_only.iloc[:,1:nth_cutoff_X_Y]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only.iloc[:,nth_cutoff_X_Y:]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only_X.astype(float)\n",
    "\n",
    "del df_train_trans_1_only\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# remove_low_variance columns\n",
    "\n",
    "r_variance=0.98\n",
    "threshold_variance_iv=r_variance*(1-r_variance)\n",
    "# df_train_trans_2_plus_X\n",
    "selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "df_redused_X=selector.fit_transform(df_train_trans_2_plus_X)\n",
    "print(\"df_train_trans_2_plus_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "df_train_trans_2_plus_X=df_train_trans_2_plus_X.iloc[:,indices]\n",
    "\n",
    "# df_train_trans_1_only_X\n",
    "selector = VarianceThreshold(threshold=threshold_variance_iv)\n",
    "df_redused_X=selector.fit_transform(df_train_trans_1_only_X)\n",
    "print(\"df_train_trans_1_only_X reduced to the shape due to %s variante\"%(str(r_variance)),df_redused_X.shape)\n",
    "indices = [i for i, x in enumerate(list(selector.get_support())) if x == True]\n",
    "df_train_trans_1_only_X=df_train_trans_1_only_X.iloc[:,indices]\n",
    "\n",
    "del df_redused_X\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# remove high correlated cols\n",
    "\n",
    "def remove_cols_with_high_coor(df_X,coorelation_threshold):\n",
    "    df_coor_X=df_X.corr().abs()\n",
    "    df_coor_X=df_coor_X.unstack().reset_index()\n",
    "    df_coor_X.columns=['iv_1','iv_2','coor']\n",
    "\n",
    "    df_coor_X_high=df_coor_X[df_coor_X['iv_1']!=df_coor_X['iv_2']]\n",
    "    df_coor_X_high=df_coor_X_high[df_coor_X_high['coor']>coorelation_threshold]\n",
    "    df_coor_X_high['high_coor_pairs']=df_coor_X_high[['iv_1','iv_2']].values.tolist()\n",
    "\n",
    "    list_highly_pairs=df_coor_X_high['high_coor_pairs'].tolist()\n",
    "    list_highly_pairs=[sorted(x) for x in list_highly_pairs]\n",
    "\n",
    "    list_highly_pairs=[str(x) for x in list_highly_pairs]\n",
    "    list_highly_pairs=list(set(list_highly_pairs))\n",
    "    list_highly_pairs=[eval(x) for x in list_highly_pairs]\n",
    "\n",
    "    list_col_keep_in_priority=['trans_in_store','total_items','total_trans_since_registration']\n",
    "    list_cols_to_remove=[]\n",
    "    list_cols_to_keep=[]\n",
    "\n",
    "    def remove_p_with_v(list_all_pairs,v_remove):\n",
    "        for p in list_all_pairs:\n",
    "            if v_remove in p:\n",
    "                list_all_pairs.remove(p)\n",
    "        return list_all_pairs\n",
    "    def remaining_unique_list(list_all_pairs):\n",
    "        res=[]\n",
    "        for x in list_all_pairs:\n",
    "            res.extend(x)\n",
    "        res=list(set(res))\n",
    "        return res\n",
    "    def update_paired_list_in_priority(l1_to_keep_priority,l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_unique=remaining_unique_list(l2_all_for_now)\n",
    "        for v_keep in l1_to_keep_priority:\n",
    "            if v_keep in list_unique:\n",
    "                l4_keep_for_now.append(v_keep)\n",
    "                list_removed_due_to_vkeep=[]\n",
    "                for p in l2_all_for_now:\n",
    "                    if v_keep in p:\n",
    "                        v_remove=[x for x in p if x!=v_keep][0]\n",
    "                        list_removed_due_to_vkeep.append(v_remove)\n",
    "                list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "                if len(list_removed_due_to_vkeep)>0:\n",
    "                    for v_remove in list_removed_due_to_vkeep:\n",
    "                        l3_remove_for_now.append(v_remove)\n",
    "                        l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                        if v_remove in l1_to_keep_priority:\n",
    "                            l1_to_keep_priority.remove(v_remove)\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now=list(set(l4_keep_for_now))\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "\n",
    "    def update_paired_list_v_total(l2_all_for_now,l3_remove_for_now,l4_keep_for_now):\n",
    "        list_keep_unique_total=[]\n",
    "        for p in l2_all_for_now:\n",
    "            for v in p:\n",
    "                if \"total\" in v:\n",
    "                    list_keep_unique_total.append(v)\n",
    "        list_keep_unique_total=list(set(list_keep_unique_total))\n",
    "        for v_keep in list_keep_unique_total:\n",
    "            list_removed_due_to_vkeep=[]\n",
    "            for p in l2_all_for_now:\n",
    "                if v_keep in p:\n",
    "                    v_remove=[x for x in p if x!=v_keep][0]\n",
    "                    list_removed_due_to_vkeep.append(v_remove)\n",
    "            list_removed_due_to_vkeep=list(set(list_removed_due_to_vkeep))\n",
    "            if len(list_removed_due_to_vkeep)>0:\n",
    "                for v_remove in list_removed_due_to_vkeep:\n",
    "                    l3_remove_for_now.append(v_remove)\n",
    "                    l2_all_for_now=remove_p_with_v(list_all_pairs=l2_all_for_now,v_remove=v_remove)\n",
    "\n",
    "                    if v_remove in list_keep_unique_total:\n",
    "                        list_keep_unique_total.remove(v_remove)\n",
    "\n",
    "        for p in l2_all_for_now:\n",
    "            v1=p[0]\n",
    "            v2=p[1]\n",
    "            if v1 in (l3_remove_for_now) and (v2 in l3_remove_for_now):\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v1 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "            elif v2 in l3_remove_for_now:\n",
    "                l2_all_for_now.remove(p)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        l4_keep_for_now.extend(list_keep_unique_total)\n",
    "        return l2_all_for_now, l3_remove_for_now, l4_keep_for_now\n",
    "\n",
    "    def remove_remaining_arbitrary(l2_all_for_now,l3_remove_for_now):\n",
    "        list_remove_arbitrary=[]\n",
    "        if len(l2_all_for_now)==0:\n",
    "            return l2_all_for_now,l3_remove_for_now\n",
    "        while len(l2_all_for_now)>0:\n",
    "            for p in l2_all_for_now:\n",
    "                v_remove=p[0]\n",
    "                list_remove_arbitrary.append(v_remove)\n",
    "                l2_all_for_now.remove(p)\n",
    "                for p2 in l2_all_for_now:\n",
    "                    if v_remove in p2:\n",
    "                        l2_all_for_now.remove(p2)\n",
    "        l3_remove_for_now.extend(list_remove_arbitrary)\n",
    "        l3_remove_for_now=list(set(l3_remove_for_now))\n",
    "        return l2_all_for_now,l3_remove_for_now \n",
    "\n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_in_priority(l1_to_keep_priority=list_col_keep_in_priority,\n",
    "                                                 l2_all_for_now=list_highly_pairs,\n",
    "                                                 l3_remove_for_now=list_cols_to_remove,\n",
    "                                                 l4_keep_for_now=list_cols_to_keep\n",
    "                                                )\n",
    "\n",
    "    list_highly_pairs, list_cols_to_remove, list_cols_to_keep=update_paired_list_v_total(\n",
    "                                                     l2_all_for_now=list_highly_pairs,\n",
    "                                                     l3_remove_for_now=list_cols_to_remove,\n",
    "                                                     l4_keep_for_now=list_cols_to_keep\n",
    "                                                    )\n",
    "\n",
    "    list_highly_pairs,list_cols_to_remove=remove_remaining_arbitrary(l2_all_for_now=list_highly_pairs,\n",
    "                                                                     l3_remove_for_now=list_cols_to_remove)\n",
    "\n",
    "    for col in list_cols_to_remove:\n",
    "        del df_X[col]\n",
    "        print(col, \"removed due to high coor with others\")\n",
    "    return df_X\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "coorelation_threshold=0.8\n",
    "\n",
    "print(datetime.datetime.now(),\"Start remove correlated cols: df_train_trans_2_plus_X\",df_train_trans_2_plus_X.shape)\n",
    "df_train_trans_2_plus_X=remove_cols_with_high_coor(df_X=df_train_trans_2_plus_X,coorelation_threshold=coorelation_threshold)\n",
    "print(datetime.datetime.now(),\"Done remove correlated cols: df_train_trans_2_plus_X\",df_train_trans_2_plus_X.shape)\n",
    "###### \n",
    "print(datetime.datetime.now(),\"Start remove correlated cols: df_train_trans_1_only_X\",df_train_trans_1_only_X.shape)\n",
    "df_train_trans_1_only_X=remove_cols_with_high_coor(df_X=df_train_trans_1_only_X,coorelation_threshold=coorelation_threshold)\n",
    "print(datetime.datetime.now(),\"Done remove correlated cols: df_train_trans_1_only_X\",df_train_trans_1_only_X.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "dict_df_type={\n",
    "    \"trans_1_only\":{\n",
    "        \"df_X\":df_train_trans_1_only_X,\n",
    "        \"df_Y\":df_train_trans_1_only_Y,\n",
    "        \"list_id\":list_train_trans_1_only_id\n",
    "    },\n",
    "    \"trans_2_plus\":{\n",
    "        \"df_X\":df_train_trans_2_plus_X,\n",
    "        \"df_Y\":df_train_trans_2_plus_Y,\n",
    "        \"list_id\":list_train_trans_2_plus_id\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# iv_start_date\n",
    "iv_end_date=datetime.datetime.strptime(high_date,\"%Y-%m-%d\").date()\n",
    "iv_start_date=iv_end_date-datetime.timedelta(days=int(np.ceil(365*recent_n_month/12)))\n",
    "\n",
    "dv_end_date=iv_end_date+datetime.timedelta(days=28)\n",
    "dv_start_date=iv_end_date+datetime.timedelta(days=1)  \n",
    "df_date_range=pd.DataFrame({\"start\":[iv_start_date,dv_start_date],\"end\":[iv_end_date,dv_end_date]},index=['IVs',\"DVs\"])\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "def scoring_above_5pctg_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "\n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "    f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "    score=(9*tp-8*fn*(1-f1_score)-fp)*accuracy\n",
    "    # the score ignored the f1 and overall accuracy due to low pctg\n",
    "\n",
    "\n",
    "\n",
    "    # consider the profit vs lost 10:1 (30%*$30) vs (cpc*frequecy or click)\n",
    "    # which means 1 missed (fn) is 10 times of 1 wrong targeted (fp)\n",
    "    # very aribitury\n",
    "    return score\n",
    "\n",
    "def scoring_below_5pctg_thresh(tp,tn,fp,fn,pctg):\n",
    "    total=sum([tp,tn,fp,fn])\n",
    "    accuracy=(tp+tn)/total\n",
    "    ppv=tp/(tp+fp) # positive predict value\n",
    "    fdr=fp/(tp+fp) # false discover rate\n",
    "    fpr=fp/(tn+fp) # false positive rate\n",
    "\n",
    "    TPR=tp/(tp+fn) #recall\n",
    "    PPV=tp/(tp+fp) #precission\n",
    "\n",
    "    r_1=tn/(tn+fp)\n",
    "    r_2=tp/(tp+fn)\n",
    "\n",
    "    # consider the 2 pctgs that matter\n",
    "    score=(1+r_1)*(1+r_2*(1+pctg))\n",
    "    '''\n",
    "    score=tp*4*(1-pctg*3)-fn*pctg*4-fp*0.1*(1-pctg)+tn*pctg*0.025 # just work for this\n",
    "    # 4=8*0.5 as the benefit * the posible inherit purchase rate\n",
    "    # the false negative is the missed should be getting benefit, but the pctg is the one that most will be 0s\n",
    "    # the false positive is only the one that spend money wrong, ~0.25 cost per reach on FB - 3 weeks, 4 times adjust\n",
    "    # true negative ignored\n",
    "    '''\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def write_aggregate_func_gain_chart(list_selected_features,df_pred_table_detail):\n",
    "    func_dict={\"customer_id_hashed\":\"count\"}\n",
    "    list_cols_for_ratios=['y_true','y_hat']\n",
    "    for col in list_selected_features:\n",
    "        if len(df_pred_table_detail[col].unique())==2:\n",
    "            func_dict.update({col:'sum'})\n",
    "            list_cols_for_ratios.append(col)\n",
    "        else:\n",
    "            func_dict.update({col:\"mean\"})\n",
    "    func_dict.update({\"y_true\":\"sum\"})\n",
    "    func_dict.update({\"y_hat\":\"sum\"})\n",
    "    # func_dict.update({\"pred_prob\":['max','min']})\n",
    "    return func_dict,list_cols_for_ratios\n",
    "\n",
    "\n",
    "def generate_gain_chart_function(df_X,list_y,list_ids,result_sm_model,threshold,list_selected_features):\n",
    "    list_pred_prob=result_sm_model.predict(sm.add_constant(df_X)).values\n",
    "    df_pred_by_id=pd.DataFrame({\"customer_id_hashed\":list_ids,\"pred_prob\":list_pred_prob},index=range(len(list_pred_prob)))\n",
    "    copy_xtrain=df_X.copy().reset_index()\n",
    "    del copy_xtrain['index']\n",
    "    df_pred_by_id=pd.concat([df_pred_by_id,copy_xtrain],axis=1,ignore_index=False)\n",
    "\n",
    "    df_pred_by_id['decile']=pd.qcut(df_pred_by_id['pred_prob'], 10, labels=False)\n",
    "    df_pred_by_id['decile']=df_pred_by_id['decile'].apply(lambda x: \"D\"+str(x+1).zfill(2))\n",
    "    df_pred_by_id['y_true']=list_y\n",
    "    df_pred_by_id['y_hat']=np.where(df_pred_by_id['pred_prob']>threshold,1,0)\n",
    "\n",
    "    agg_func,list_cols_to_get_ratio=write_aggregate_func_gain_chart(list_selected_features,df_pred_by_id)\n",
    "    df_gainchart=df_pred_by_id.groupby(\"decile\")[['customer_id_hashed']+list_selected_features+['y_true', 'y_hat']].agg(agg_func).reset_index()\n",
    "\n",
    "\n",
    "    df_prob_max=df_pred_by_id.groupby(\"decile\")['pred_prob'].max().to_frame().reset_index().rename(columns={\"pred_prob\":\"max_prob\"})\n",
    "    df_prob_min=df_pred_by_id.groupby(\"decile\")['pred_prob'].min().to_frame().reset_index().rename(columns={\"pred_prob\":\"min_prob\"})\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_max,on=\"decile\")\n",
    "    df_gainchart=pd.merge(df_gainchart,df_prob_min,on=\"decile\")\n",
    "    df_gainchart.insert(2,\"actual_ratio\",df_gainchart['y_true']/df_gainchart['customer_id_hashed'])\n",
    "    df_gainchart.insert(3,\"pred_ratio\",df_gainchart['y_hat']/df_gainchart['customer_id_hashed'])\n",
    "\n",
    "\n",
    "    df_gainchart.insert(4,\"max_pred_prob\",df_gainchart['max_prob'])\n",
    "    df_gainchart.insert(5,\"min_pred_prob\",df_gainchart['min_prob'])\n",
    "    del df_gainchart['max_prob']\n",
    "    del df_gainchart['min_prob']\n",
    "\n",
    "    return df_gainchart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_train_trans_2_plus_X</td>\n",
       "      <td>1120407392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_train_trans_1_only_X</td>\n",
       "      <td>108406360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_train_trans_2_plus_Y</td>\n",
       "      <td>24225056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_train_trans_1_only_Y</td>\n",
       "      <td>17888232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>list_train_trans_2_plus_id</td>\n",
       "      <td>6056328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>list_train_trans_1_only_id</td>\n",
       "      <td>1372304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_i1</td>\n",
       "      <td>19785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_i</td>\n",
       "      <td>19785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queary</td>\n",
       "      <td>7827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql_str_cols_all</td>\n",
       "      <td>7489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          obj        size\n",
       "0     df_train_trans_2_plus_X  1120407392\n",
       "0     df_train_trans_1_only_X   108406360\n",
       "0     df_train_trans_2_plus_Y    24225056\n",
       "0     df_train_trans_1_only_Y    17888232\n",
       "0  list_train_trans_2_plus_id     6056328\n",
       "0  list_train_trans_1_only_id     1372304\n",
       "0                         _i1       19785\n",
       "0                          _i       19785\n",
       "0                      queary        7827\n",
       "0            sql_str_cols_all        7489"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "df_size=pd.DataFrame()\n",
    "for i in dir():\n",
    "    df_size=df_size.append(pd.DataFrame({\"obj\":i,\"size\":sys.getsizeof(locals()[i])},index=[0]))\n",
    "df_size=df_size.sort_values(\"size\",ascending=False) \n",
    "\n",
    "df_size.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-91b8a54d8fcc>, line 169)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-91b8a54d8fcc>\"\u001b[0;36m, line \u001b[0;32m169\u001b[0m\n\u001b[0;31m    total_test_count=\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class SM_Logistic_Model_dvN:\n",
    "    # 1\n",
    "    def __init__(self,n_week_DV,key_df_type,df_date_range,sql_engine=BL_engine,dict_cols_in_table=dict_cols_in_table,max_test_ids_df_limit=10**7):\n",
    "\n",
    "# n_week_DV: 1-4\n",
    "# key_df_type: \"trans_1_only\" or \"trans_2_plus\" in the keys of dict_df_type\n",
    "# df_date_range: defined global df -- df_date_range\n",
    "# sql_engine: mysql engine to BigLots database\n",
    "        self.max_test_ids_df_limit=max_test_ids_df_limit\n",
    "        self.n_week_DV=n_week_DV\n",
    "        self.dict_cols_in_table=dict_cols_in_table\n",
    "        self.key_df_type=key_df_type\n",
    "        self.sql_engine=BL_engine\n",
    "        self.df_train_X=dict_df_type[key_df_type]['df_X']\n",
    "        self.X_train_scaled=scale(self.df_train_X) # will be wroten later in #2, 3 & 4\n",
    "\n",
    "        self.df_train_Y=dict_df_type[key_df_type]['df_Y']\n",
    "        self.input_y_train_list=self.df_train_Y[\"DV_cumulative_week_updated_%i\"%n_week_DV].values.tolist()\n",
    "\n",
    "        self.list_ids_y_train=dict_df_type[key_df_type]['list_id']\n",
    "\n",
    "        self.X_features=self.df_train_X.columns.tolist()\n",
    "        \n",
    "        self.query_full_part_1=None\n",
    "        self.query_full_part_2=None\n",
    "        \n",
    "        self.df_test_X_part_1=pd.DataFrame()\n",
    "        self.X_test_scaled_part_1=pd.DataFrame()\n",
    "        self.df_test_Y_part_1=pd.DataFrame()\n",
    "        self.df_test_id_part_1=pd.DataFrame()\n",
    "        self.input_y_test_list_part_1=[]\n",
    "\n",
    "        self.df_test_X_part_2=pd.DataFrame()\n",
    "        self.X_test_scaled_part_2=pd.DataFrame()\n",
    "        self.df_test_Y_part_2=pd.DataFrame()\n",
    "        self.df_test_id_part_2=pd.DataFrame()\n",
    "        self.input_y_test_list_part_2=[]\n",
    "        \n",
    "        \n",
    "        self.df_date_range=df_date_range\n",
    "\n",
    "        self.db_row_counts=pd.DataFrame({\"records\":self.df_train_X.shape[0],\"IVs\":self.df_train_X.shape[1]},index=[\"X_train\"])        \n",
    "        self.df_y_train_count=pd.DataFrame()\n",
    "        self.df_y_test_count=pd.DataFrame()\n",
    "        self.pctg=None\n",
    "        self.threshold_max_selfdefinedscore=None\n",
    "        self.df_step_table=pd.DataFrame()\n",
    "        self.df_confusion_table=pd.DataFrame()\n",
    "        self.df_gainchart_train=pd.DataFrame()\n",
    "        self.df_gainchart_test=pd.DataFrame()\n",
    "\n",
    "        self.df_train_ids_labeled_summary=pd.DataFrame()\n",
    "        self.df_test_id_part_1s_labeled_summary=pd.DataFrame()\n",
    "\n",
    "        self.df_train_ids_labeled=pd.DataFrame()\n",
    "        self.df_test_id_part_1s_labeled=pd.DataFrame()\n",
    "\n",
    "\n",
    "        self.output_folder=model_output_folder+\"output_No_DCM_%s_%s/\"%(str(self.df_date_range.iloc[0,1]),str(datetime.datetime.now().date()))\n",
    "\n",
    "        try:\n",
    "            os.stat(self.output_folder)\n",
    "        except:\n",
    "            os.mkdir(self.output_folder)\n",
    "\n",
    "        self.output_path=self.output_folder+\"BL_LRModeling_NoDCM_%s_DV%s_%s_JL_%s.xlsx\"%(key_df_type,str(n_week_DV),str(self.df_date_range.iloc[0,1]),str(datetime.datetime.now()))\n",
    "        self.df_department_name=pd.read_table(path_dept_name,sep=\"|\").drop_duplicates()\n",
    "    # 2\n",
    "    def select_from_model_n_features(self, N_feature_select_from_models):\n",
    "        print('start',datetime.datetime.now(),\"select_from_model_n_features\")\n",
    "        print(\"Starting select_from_model_n_features: \",datetime.datetime.now())\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(random_state=0,\n",
    "                                                                solver=\"saga\",\n",
    "                                                                max_iter=2000,\n",
    "                                                                n_jobs=24,\n",
    "                                                                tol=0.0001),\n",
    "                                   max_features=N_feature_select_from_models,\n",
    "                                   threshold=-np.inf).fit(self.X_train_scaled, self.input_y_train_list)\n",
    "\n",
    "        print(\"selector.threshold_\",selector.threshold_)\n",
    "        selector_support_FROMMODEL=selector.get_support()\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_FROMMODEL) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "        print(\"Done select_from_model_n_features %d: \"%N_feature_select_from_models,datetime.datetime.now())\n",
    "\n",
    "    #3\n",
    "    def select_REF(self,n_features_to_select):\n",
    "        print('start',datetime.datetime.now(),\"select_REF\")\n",
    "        print(\"Starting select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        estimator = LogisticRegression(fit_intercept=True,solver='saga',max_iter=2000,n_jobs=24,tol=0.001)\n",
    "        selector = RFE(estimator,step=1,n_features_to_select=n_features_to_select)\n",
    "        selector = selector.fit(self.X_train_scaled, self.input_y_train_list)\n",
    "        selector_support_REF=selector.support_\n",
    "        print(\"Done select_REF: \",datetime.datetime.now())\n",
    "\n",
    "        self.X_features=[self.X_features[i] for i,v in enumerate(selector_support_REF) if v==True]\n",
    "\n",
    "        self.df_train_X=self.df_train_X.loc[:,self.X_features]\n",
    "\n",
    "        print(\"df_train_X.shape\",self.df_train_X.shape)\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "        print(\"X_train_scaled.shape\",self.X_train_scaled.shape)\n",
    "\n",
    "    #4\n",
    "    def forwards_feature_elimination_based_on_p_and_vif(self,niter=50,method=\"lbfgs\",p_tol=0.1,vif_tol=5):\n",
    "        print('start',datetime.datetime.now(),\"forwards_feature_elimination_based_on_p_and_vif\")\n",
    "        len_x_features=self.df_train_X.shape[1]\n",
    "        len_x_features_new=0\n",
    "        df_x_dropped=self.df_train_X.copy()\n",
    "        i_iter=0\n",
    "        while len_x_features_new<len_x_features and i_iter<=100:\n",
    "            i_iter+=1\n",
    "            len_x_features=df_x_dropped.shape[1]\n",
    "            mod=sm.Logit(self.input_y_train_list,sm.add_constant(df_x_dropped),niter=niter,method=method)\n",
    "            res=mod.fit()\n",
    "            table=res.summary2().tables[1]   \n",
    "            X=add_constant(scale(df_x_dropped))\n",
    "            list_cols=table.index.tolist()\n",
    "            table[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "\n",
    "            max_vif=table['VIF Factor'].max()\n",
    "            max_p=table['P>|z|'].max()\n",
    "\n",
    "            if max_vif>vif_tol:\n",
    "                col_name_to_drop=table.index[table['VIF Factor']==max_vif][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to high vif\"%col_name_to_drop)\n",
    "\n",
    "            elif max_p>p_tol:\n",
    "                col_name_to_drop=table.index[table['P>|z|']==max_p][0]\n",
    "                del df_x_dropped[col_name_to_drop]\n",
    "                len_x_features_new=df_x_dropped.shape[1]\n",
    "                print(df_x_dropped.shape,\"column %s dropped due to p value\"%col_name_to_drop)\n",
    "            else:\n",
    "                i_iter+=100\n",
    "\n",
    "        self.df_train_X=df_x_dropped\n",
    "        self.X_features=df_x_dropped.columns.tolist()\n",
    "        self.X_train_scaled=scale(self.df_train_X)\n",
    "\n",
    "\n",
    "    # 5\n",
    "    def run_sm_logR_model(self):\n",
    "        print('start',datetime.datetime.now(),\"run_sm_logR_model\")\n",
    "        self.sm_model=sm.Logit(self.input_y_train_list,sm.add_constant(self.df_train_X),niter=50,method=\"lbfgs\")\n",
    "        self.res_of_model=self.sm_model.fit()\n",
    "        self.summary_table_over=self.res_of_model.summary2().tables[0].reset_index()\n",
    "        self.summary_table_output=self.res_of_model.summary2().tables[1].reset_index()\n",
    "\n",
    "        std=self.sm_model.exog.std(axis=0)\n",
    "        std[0] = 1\n",
    "        tt = self.res_of_model.t_test(np.diag(std))\n",
    "        df_std_coef=tt.summary_frame()\n",
    "        list_std_coefficients=df_std_coef['coef'].tolist()\n",
    "        self.summary_table_output['std_coef']=list_std_coefficients\n",
    "\n",
    "        self.list_train_pred=self.res_of_model.predict()\n",
    "        # \n",
    "\n",
    "        coefficient_of_dermination = r2_score(self.input_y_train_list, self.list_train_pred)\n",
    "        self.summary_table_over=self.summary_table_over.append(pd.DataFrame({\"index\":[8],0:\"calculated_r_squared\",1:coefficient_of_dermination},index=[8]))\n",
    "\n",
    "        #VIF\n",
    "        X=add_constant(self.X_train_scaled)\n",
    "        list_cols=self.summary_table_output['index'].tolist()\n",
    "        self.summary_table_output[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        self.summary_table_output=self.summary_table_output.sort_values(\"std_coef\")\n",
    "\n",
    "    # 6\n",
    "    \n",
    "# !! Due to the limit memory, have to split the test dataframe into 2 pieces\n",
    "# The fisrt part with 10 mm (10**7)ids used to determine the threshold and run through all the functions in the class\n",
    "# The seconde part (rest, off set 10 mm)ids used to apply the parameters abovea and just do prediction only, then combin the summary and save the result\n",
    "\n",
    "    def select_test_df_from_mysql(self):\n",
    "        total_test_count=pd.read_sql(\"select count(*) from %s\"%table_0_test,con=BL_engine).iloc[0]\n",
    "        \n",
    "        if total_test_count>=self.max_test_ids_df_limit:\n",
    "            limit_clause=\"limit %i\"%self.max_test_ids_df_limit\n",
    "        else:\n",
    "            limit_clause=None\n",
    "            \n",
    "        print('start',datetime.datetime.now(),\"select_test_df_from_mysql\")\n",
    "        cols_in_X=self.summary_table_output.iloc[:,0].values.tolist()\n",
    "        cols_in_X.remove(\"const\")\n",
    "        self.X_features=cols_in_X\n",
    "\n",
    "        table_name_t0=self.dict_cols_in_table['t0_test']['table_name']\n",
    "        col_list_t0=self.dict_cols_in_table['t0_test']['cols']\n",
    "\n",
    "        table_name_t1=self.dict_cols_in_table['t1']['table_name']\n",
    "        col_list_t1=self.dict_cols_in_table['t1']['cols']\n",
    "\n",
    "        table_name_t2_1=self.dict_cols_in_table['t2_1']['table_name']\n",
    "        col_list_t2_1=self.dict_cols_in_table['t2_1']['cols']\n",
    "\n",
    "        table_name_t2_2=self.dict_cols_in_table['t2_2']['table_name']\n",
    "        col_list_t2_2=self.dict_cols_in_table['t2_2']['cols']\n",
    "\n",
    "        col_list_t1=[x for x in col_list_t1 if x in cols_in_X]\n",
    "        col_list_t2_1=[x for x in col_list_t2_1 if x in cols_in_X]\n",
    "        col_list_t2_2=[x for x in col_list_t2_2 if x in cols_in_X]\n",
    "\n",
    "\n",
    "        sql_str_cols_df0_test=str([\"t0.\"+x for x in col_list_t0]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list=[sql_str_cols_df0_test]\n",
    "        list_tables=[\"t0\"]\n",
    "        col_list_dv=[x for x in self.df_train_Y.columns.tolist()]\n",
    "        sql_str_cols_dv=str([\"t1.\"+x for x in col_list_dv]).replace(\"'\",\"\")[1:-1]\n",
    "        list_query_col_list.append(sql_str_cols_dv)\n",
    "        if len(col_list_t1)>0:\n",
    "            sql_str_cols_df_1=str([\"t1.\"+x for x in col_list_t1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_df_1)\n",
    "            list_tables.append(\"t1\")\n",
    "        if len(col_list_t2_1)>0:\n",
    "            sql_str_cols_2_1=str([\"t2_1.\"+x for x in col_list_t2_1]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_1)\n",
    "            list_tables.append(\"t2_1\")\n",
    "        if len(col_list_t2_2)>0:\n",
    "            sql_str_cols_2_2=str([\"t2_2.\"+x for x in col_list_t2_2]).replace(\"'\",\"\")[1:-1]\n",
    "            list_query_col_list.append(sql_str_cols_2_2)\n",
    "            list_tables.append(\"t2_2\")\n",
    "        str_cols_test=\", \".join(list_query_col_list)\n",
    "        select_clause=\"SELECT %s from %s as t0\"%(str_cols_test,table_name_t0)\n",
    "\n",
    "        list_of_join_clause=[]\n",
    "        if \"t1\" in list_tables:\n",
    "            str_join_clause_t1=\"left join %s as t1 on t0.customer_id_hashed=t1.customer_id_hashed\"%table_name_t1\n",
    "            list_of_join_clause.append(str_join_clause_t1)\n",
    "        if \"t2_1\" in list_tables:\n",
    "            str_join_clause_t2_1=\"left join %s as t2_1 on t0.customer_id_hashed=t2_1.id\"%table_name_t2_1\n",
    "            list_of_join_clause.append(str_join_clause_t2_1)\n",
    "        if \"t2_2\" in list_tables:\n",
    "            str_join_clause_t2_2=\"left join %s as t2_2 on t0.customer_id_hashed=t2_2.id\"%table_name_t2_2\n",
    "            list_of_join_clause.append(str_join_clause_t2_2)\n",
    "\n",
    "        if self.key_df_type==\"trans_1_only\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is null\"\n",
    "        elif self.key_df_type==\"trans_2_plus\":\n",
    "            where_clause=\"where t2_2.total_sales_recent_two_trans is not null\"\n",
    "        else:\n",
    "            where_clause=\"\"\n",
    "            print(\"key_df_type not specified, please choose either trans_1_only or trans_2_plus\")\n",
    "        query_full=\" \".join([select_clause]+list_of_join_clause+[where_clause]).strip()\n",
    "        print(query_full)\n",
    "        if limit_clause:\n",
    "            self.query_full_part_1=query_full+\" \"+limit_clause\n",
    "            self.query_full_part_2=query_full+\" off set %i limit %i\"%(self.max_test_ids_df_limit,total_test_count)\n",
    "        else:\n",
    "            self.query_full_part_1=query_full\n",
    "            self.query_full_part_2=None\n",
    "\n",
    "        self.df_test_X_part_1=pd.read_sql(query_full_part_1,con=self.sql_engine)\n",
    "        if \"nearest_BL_dist\" in self.df_test_X_part_1.columns.tolist():\n",
    "            self.df_test_X_part_1=self.df_test_X_part_1[pd.notnull(self.df_test_X_part_1['nearest_BL_dist'])]\n",
    "        for col in self.df_test_X_part_1.columns.tolist():\n",
    "            # df_nan=df[pd.isnull(df[col])]\n",
    "            if self.df_test_X_part_1[pd.isnull(self.df_test_X_part_1[col])].shape[0]>0:\n",
    "                raise ValueError(\"%s in the selected test df is null\"%col)\n",
    "\n",
    "        cols_Y=[x for x in self.df_test_X_part_1.columns.tolist() if \"cumulative\" in x]\n",
    "        self.df_test_Y_part_1=self.df_test_X_part_1[cols_Y]\n",
    "        for col in cols_Y:\n",
    "            del self.df_test_X_part_1[col]\n",
    "        self.list_ids_y_test=self.df_test_X_part_1['customer_id_hashed'].values.tolist()\n",
    "        del self.df_test_X_part_1['customer_id_hashed']\n",
    "        self.input_y_test_list_part_1=self.df_test_Y_part_1[\"DV_cumulative_week_updated_%i\"%self.n_week_DV].values.tolist()     \n",
    "        # self.df_test_X_part_1=df\n",
    "        self.X_test_scaled_part_1=scale(self.df_test_X_part_1)\n",
    "    # 7\n",
    "    def run_updating_df_count(self):\n",
    "        print('start',datetime.datetime.now(),\"run_updating_df_count\")\n",
    "        df_test_X_count=pd.DataFrame({\"records\":self.df_test_X_part_1.shape[0],\"IVs\":self.df_test_X_part_1.shape[1]},index=[\"X_test\"])\n",
    "        self.db_row_counts=self.db_row_counts.append(df_test_X_count)\n",
    "\n",
    "    # 8\n",
    "    def generate_DV_distribution(self):\n",
    "        print('start',datetime.datetime.now(),\"generate_DV_distribution\")\n",
    "        df_y_train_count=pd.DataFrame()\n",
    "        for col in self.df_train_Y.columns.tolist():\n",
    "            count_1=self.df_train_Y[self.df_train_Y[col]==1].shape[0]\n",
    "            count_0=self.df_train_Y[self.df_train_Y[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_train_count=df_y_train_count.append(df)\n",
    "        df_y_train_count.insert(0,\"set\",\"y_train\")\n",
    "\n",
    "\n",
    "        df_y_test_count=pd.DataFrame()\n",
    "        for col in self.df_test_Y_part_1.columns.tolist():\n",
    "            count_1=self.df_test_Y_part_1[self.df_test_Y_part_1[col]==1].shape[0]\n",
    "            count_0=self.df_test_Y_part_1[self.df_test_Y_part_1[col]==0].shape[0]\n",
    "\n",
    "            df=pd.DataFrame({\"0\":count_0,\"1\":count_1},index=[col])\n",
    "            df_y_test_count=df_y_test_count.append(df)\n",
    "        df_y_test_count.insert(0,\"set\",\"y_test\")\n",
    "\n",
    "        self.df_y_train_count=df_y_train_count\n",
    "        self.df_y_test_count=df_y_test_count\n",
    "        self.pctg=(sum(self.input_y_train_list)+sum(self.input_y_test_list_part_1))/(len(self.input_y_train_list)+len(self.input_y_test_list_part_1))\n",
    "\n",
    "    # 9\n",
    "    def pred_test_Y(self):\n",
    "        print('start',datetime.datetime.now(),\"pred_test_Y\")\n",
    "        self.list_test_pred=self.res_of_model.predict(sm.add_constant(self.df_test_X_part_1)).tolist()\n",
    "\n",
    "    # 10\n",
    "    def generate_step_table_of_test_SM(self,):\n",
    "        print('start',datetime.datetime.now(),\"generate_step_table_of_test_SM\")\n",
    "        if self.pctg>=0.05:\n",
    "            threshold_list = [(x+1)/100 for x in range(0,100)] \n",
    "        else:\n",
    "            start_prob_pctg=max(0.001,int(np.floor((self.pctg-0.02)*100))/100)\n",
    "            end_prob_pctg=int(np.floor((self.pctg+0.02)*100))/100\n",
    "            threshold_list = [(x+1)/1000 for x in range(int(start_prob_pctg*1000),int(end_prob_pctg*1000))]\n",
    "\n",
    "        list_prob_test=self.list_test_pred\n",
    "        df_output=pd.DataFrame()\n",
    "        for i in threshold_list:\n",
    "            y_test_pred=[1 if x>i else 0 for x in list_prob_test]\n",
    "\n",
    "            accuracy_score = metrics.accuracy_score(self.input_y_test_list_part_1,y_test_pred)    \n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(self.input_y_test_list_part_1, y_test_pred).ravel()\n",
    "            # \n",
    "            TPR=tp/(tp+fn) #recall\n",
    "            FNR=fn/(tp+fn)\n",
    "            FPR=fp/(fp+tn)\n",
    "            TNR=tn/(fp+tn)\n",
    "\n",
    "            PPV=tp/(tp+fp) #precission\n",
    "            f1_score = 2*(TPR*PPV)/(TPR+PPV)\n",
    "\n",
    "            df=pd.DataFrame({\"predicted_positive\":len([x for x in y_test_pred if x==1]),\n",
    "                             \"predicted_negative\":len([x for x in y_test_pred if x==0]),\n",
    "                             \"accuracy_score\":accuracy_score,\n",
    "                             'true_negative':tn,\n",
    "                             'false_positive':fp,\n",
    "                             'false_negative':fn,\n",
    "                             'true_positive':tp,\n",
    "                             'true_positive_rate':TPR,\n",
    "                             'false_negative_rate':FNR,\n",
    "                             'false_positive_rate':FPR,\n",
    "                             'true_negative_rate':TNR,\n",
    "                             'precission_(Positive predictive value)':PPV,\n",
    "                             'f1_score':f1_score\n",
    "                            },index=[i])\n",
    "            df_output=df_output.append(df)\n",
    "\n",
    "        self.df_step_table=df_output\n",
    "\n",
    "    # 11\n",
    "    def select_best_scored_pred_prob(self):\n",
    "        print('start',datetime.datetime.now(),\"select_best_scored_pred_prob\")\n",
    "        if self.pctg>=0.05:\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_above_5pctg_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)\n",
    "        else:\n",
    "            self.df_step_table['self_defined_score']=self.df_step_table.apply(lambda df: scoring_below_5pctg_thresh(df['true_positive'],df['true_negative'],df['false_positive'],df['false_negative'],self.pctg),axis=1)\n",
    "\n",
    "        threshold_max_selfdefinedscore=self.df_step_table[self.df_step_table['self_defined_score']==self.df_step_table['self_defined_score'].max()].index[0]\n",
    "        self.threshold_max_selfdefinedscore=threshold_max_selfdefinedscore\n",
    "        print(\"threshold_max_selfdefinedscore\",threshold_max_selfdefinedscore)\n",
    "        self.df_step_table=self.df_step_table.reset_index()\n",
    "        self.df_confusion_table=self.df_step_table.loc[self.df_step_table['index']==threshold_max_selfdefinedscore,:]\n",
    "\n",
    "    # 12\n",
    "    def generate_gain_chart(self):\n",
    "        print('start',datetime.datetime.now(),\"generate_gain_chart\")\n",
    "        self.df_gainchart_train=generate_gain_chart_function(df_X=self.df_train_X,\n",
    "                                                             list_y=self.input_y_train_list,\n",
    "                                                             list_ids=self.list_ids_y_train,\n",
    "                                                             result_sm_model=self.res_of_model,\n",
    "                                                             threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                             list_selected_features=self.X_features)\n",
    "\n",
    "        self.df_gainchart_test=generate_gain_chart_function(df_X=self.df_test_X_part_1,\n",
    "                                                            list_y=self.input_y_test_list_part_1,\n",
    "                                                            list_ids=self.list_ids_y_test,\n",
    "                                                            result_sm_model=self.res_of_model,\n",
    "                                                            threshold=self.threshold_max_selfdefinedscore,\n",
    "                                                            list_selected_features=self.X_features)\n",
    "\n",
    "    # 13\n",
    "    def check_shopper_type(self):\n",
    "        print('start',datetime.datetime.now(),\"check_shopper_type\")\n",
    "        recent_4_week_sign_up_end_dt=self.df_date_range.iloc[0,1]\n",
    "        recent_4_week_sign_up_start_dt=recent_4_week_sign_up_end_dt-datetime.timedelta(days=27)\n",
    "        str_start_sign_up=\"'\"+str(recent_4_week_sign_up_start_dt)+\"'\"\n",
    "        str_end_sign_up=\"'\"+str(recent_4_week_sign_up_end_dt)+\"'\"\n",
    "        print(\"new sign up date range below: \\n\",recent_4_week_sign_up_start_dt,recent_4_week_sign_up_end_dt)\n",
    "\n",
    "        df_recent_4_week_new_sings=pd.read_sql(\"select customer_id_hashed from BL_Rewards_Master where sign_up_date between %s and %s\"%(str_start_sign_up,str_end_sign_up),con=BL_engine)\n",
    "        df_recent_4_week_new_sings=df_recent_4_week_new_sings.drop_duplicates()\n",
    "        df_recent_4_week_new_sings['sign_up_label']=\"new_signs\"\n",
    "        # \n",
    "        df_train_ids_labeled=pd.DataFrame({\"y_hat\":self.list_train_pred},index=self.list_ids_y_train).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_train_ids_labeled['selection_label']=np.where(df_train_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_train_ids_labeled=pd.merge(df_train_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_train_ids_labeled['sign_up_label']=df_train_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_train_ids_labeled['actual_shopping_label']=self.input_y_train_list\n",
    "        df_train_ids_labeled['actual_shopping_label']=df_train_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "\n",
    "        self.df_train_ids_labeled_summary=df_train_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_train_ids_labeled=df_train_ids_labeled\n",
    "\n",
    "        # \n",
    "        df_test_ids_labeled=pd.DataFrame({\"y_hat\":self.list_test_pred},index=self.list_ids_y_test).reset_index().rename(columns={\"index\":\"customer_id_hashed\"})\n",
    "        df_test_ids_labeled['selection_label']=np.where(df_test_ids_labeled['y_hat']>=self.threshold_max_selfdefinedscore,\"target\",\"nonselect\")\n",
    "        df_test_ids_labeled=pd.merge(df_test_ids_labeled,df_recent_4_week_new_sings,on=\"customer_id_hashed\",how=\"left\")\n",
    "        df_test_ids_labeled['sign_up_label']=df_test_ids_labeled['sign_up_label'].fillna(\"existing\")\n",
    "        df_test_ids_labeled['actual_shopping_label']=self.input_y_test_list_part_1\n",
    "        df_test_ids_labeled['actual_shopping_label']=df_test_ids_labeled['actual_shopping_label'].replace(0,\"no\").replace(1,\"shopper\")\n",
    "\n",
    "        self.df_test_id_part_1s_labeled_summary=df_test_ids_labeled.groupby(['selection_label','sign_up_label','actual_shopping_label'])['customer_id_hashed'].nunique().to_frame().reset_index()\n",
    "        self.df_test_id_part_1s_labeled=df_test_ids_labeled        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 14\n",
    "    def save_outputs(self):\n",
    "        print('start',datetime.datetime.now(),\"save_outputs\")\n",
    "        writer=pd.ExcelWriter(self.output_path,engine=\"xlsxwriter\")\n",
    "\n",
    "        self.db_row_counts.to_excel(writer,\"df_dataset_shape\")\n",
    "        self.df_date_range.to_excel(writer,\"df_date_range\")\n",
    "        self.df_y_train_count.to_excel(writer,\"df_y_train_count\")\n",
    "        self.df_y_test_count.to_excel(writer,\"df_y_test_count\")\n",
    "        self.summary_table_over.to_excel(writer,\"summary_table_over\")\n",
    "        self.summary_table_output.to_excel(writer,\"summary_table_output\")\n",
    "        self.df_step_table.to_excel(writer,\"step_table\",index=True)\n",
    "        self.df_confusion_table.to_excel(writer,\"select_score_matrix\",index=False)\n",
    "\n",
    "        self.df_gainchart_train.to_excel(writer,\"gainchart_train\",index=False)\n",
    "        self.df_gainchart_test.to_excel(writer,\"gainchart_test\",index=False)\n",
    "        self.df_department_name.to_excel(writer,\"department_name\",index=False)\n",
    "\n",
    "        self.df_train_ids_labeled_summary.to_excel(writer,\"train_id_summary\",index=False)\n",
    "        self.df_test_id_part_1s_labeled_summary.to_excel(writer,\"test_id_summary\",index=False)\n",
    "\n",
    "        writer.save()\n",
    "        str_high_date=str(self.df_date_range.iloc[0,1])\n",
    "        str_dv_type=\"DV%i_%s\"%(self.n_week_DV,self.key_df_type)\n",
    "        self.df_train_ids_labeled.to_csv(self.output_folder+\"df_train_ids_labeled_%s_%s.csv\"%(str_high_date,str_dv_type),index=False)\n",
    "        self.df_test_id_part_1s_labeled.to_csv(self.output_folder+\"df_test_ids_labeled_%s_%s.csv\"%(str_high_date,str_dv_type),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_2_plus DV 2 start:  2020-10-30 12:48:13.178352\n",
      "start 2020-10-30 12:48:20.378326 select_from_model_n_features\n",
      "Starting select_from_model_n_features:  2020-10-30 12:48:20.378947\n",
      "selector.threshold_ -inf\n",
      "df_train_X.shape (757032, 60)\n",
      "X_train_scaled.shape (757032, 60)\n",
      "Done select_from_model_n_features 60:  2020-10-30 12:56:53.189909\n",
      "start 2020-10-30 12:56:53.190488 select_REF\n",
      "Starting select_REF:  2020-10-30 12:56:53.190623\n",
      "Done select_REF:  2020-10-30 13:04:01.581925\n",
      "df_train_X.shape (757032, 40)\n",
      "X_train_scaled.shape (757032, 40)\n",
      "start 2020-10-30 13:04:03.104341 forwards_feature_elimination_based_on_p_and_vif\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.300042\n",
      "         Iterations 8\n",
      "(757032, 39) column trans_in_store dropped due to high vif\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301273\n",
      "         Iterations 8\n",
      "(757032, 38) column department_114_recent_one dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301273\n",
      "         Iterations 8\n",
      "(757032, 37) column department_111_recent_two dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301273\n",
      "         Iterations 8\n",
      "(757032, 36) column department_354_recent_one dropped due to p value\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301274\n",
      "         Iterations 8\n",
      "start 2020-10-30 13:08:24.999650 run_sm_logR_model\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301274\n",
      "         Iterations 8\n",
      "start 2020-10-30 13:09:12.729971 select_test_df_from_mysql\n",
      "SELECT t0.customer_id_hashed, t1.DV_cumulative_week_updated_1, t1.DV_cumulative_week_updated_2, t1.DV_cumulative_week_updated_3, t1.P_zip, t2_1.department_minus_one_trans, t2_1.department_108_trans, t2_1.department_111_trans, t2_1.department_114_trans, t2_1.department_120_trans, t2_1.department_130_trans, t2_1.department_140_trans, t2_1.department_340_trans, t2_1.department_352_trans, t2_1.department_354_trans, t2_1.department_367_trans, t2_1.department_530_trans, t2_2.total_sales_1st_trans, t2_2.department_170_1st_trans, t2_2.department_210_1st_trans, t2_2.department_360_1st_trans, t2_2.week_counts_to_now_recent_one, t2_2.department_110_recent_one, t2_2.department_160_recent_one, t2_2.department_210_recent_one, t2_2.department_353_recent_one, t2_2.department_363_recent_one, t2_2.department_608_recent_one, t2_2.department_615_recent_one, t2_2.week_counts_to_now_recent_two, t2_2.recent_two_trans_also_1st, t2_2.total_sales_recent_two_trans, t2_2.department_minus_one_recent_two, t2_2.department_110_recent_two, t2_2.department_160_recent_two, t2_2.department_210_recent_two, t2_2.department_352_recent_two, t2_2.department_360_recent_two, t2_2.department_610_recent_two, t2_2.department_615_recent_two from crm_table_id_list_test_20200822 as t0 left join table_pred_1_crm_up_to_20200822 as t1 on t0.customer_id_hashed=t1.customer_id_hashed left join all_NEall_id_pred_pos_2_1_pos_until_20200822 as t2_1 on t0.customer_id_hashed=t2_1.id left join all_NEall_id_pred_pos_2_2_pos_until_20200822 as t2_2 on t0.customer_id_hashed=t2_2.id where t2_2.total_sales_recent_two_trans is not null\n"
     ]
    }
   ],
   "source": [
    "# In[22]:\n",
    "\n",
    "'''\n",
    "n_week_DV=3\n",
    "key_df_type=\"trans_1_only\"\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type,\n",
    "                               df_date_range=df_date_range,\n",
    "                               sql_engine=BL_engine,\n",
    "                               dict_cols_in_table=dict_cols_in_table\n",
    "                              )\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN,N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.pred_test_Y(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_gain_chart(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.check_shopper_type(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.save_outputs(SM_Logistic_Model_dvN)\n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "'''\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "n_week_DV=2\n",
    "key_df_type=\"trans_2_plus\"\n",
    "print(\"%s DV %i start: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "SM_Logistic_Model_dvN.__init__(self=SM_Logistic_Model_dvN,\n",
    "                               n_week_DV=n_week_DV,\n",
    "                               key_df_type=key_df_type,\n",
    "                               df_date_range=df_date_range,\n",
    "                               sql_engine=BL_engine,\n",
    "                               dict_cols_in_table=dict_cols_in_table\n",
    "                              )\n",
    "SM_Logistic_Model_dvN.select_from_model_n_features(SM_Logistic_Model_dvN,N_feature_select_from_models=min(60,int(SM_Logistic_Model_dvN.df_train_X.shape[1]*0.7)))\n",
    "SM_Logistic_Model_dvN.select_REF(SM_Logistic_Model_dvN,n_features_to_select=40)\n",
    "SM_Logistic_Model_dvN.forwards_feature_elimination_based_on_p_and_vif(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_sm_logR_model(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.select_test_df_from_mysql(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.run_updating_df_count(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_DV_distribution(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.pred_test_Y(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.generate_step_table_of_test_SM(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.select_best_scored_pred_prob(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.generate_gain_chart(SM_Logistic_Model_dvN)\n",
    "SM_Logistic_Model_dvN.check_shopper_type(SM_Logistic_Model_dvN)\n",
    "\n",
    "SM_Logistic_Model_dvN.save_outputs(SM_Logistic_Model_dvN)\n",
    "print(\"%s DV %i done: \"%(key_df_type,n_week_DV),datetime.datetime.now())\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "print(\"Job done: model building\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
