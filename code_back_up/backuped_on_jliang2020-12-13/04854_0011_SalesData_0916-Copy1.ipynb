{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "folderpath = '/home/jubauser1/BiglotsCode/outputs/'\n",
    "lastweeksdate = '2017-09-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsalespath = '/home/jubauser1/BigLots/MediaStorm Data Extract week ending 2017-09-16/MediaStormSalesWeekly.txt'\n",
    "newtrafficpath = '/home/jubauser1/BigLots/MediaStorm Data Extract week ending 2017-09-16/MediaStormTrafficWeekly.txt'\n",
    "newinventorypath = '/home/jubauser1/BigLots/MediaStorm Data Extract week ending 2017-09-16/MediaStormInventoryWeekly.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "closed_onlinestorelist = ['6990','280', '298', '388', '507', '824', '1098', \n",
    "                          '1120', '1148', '1182', '1374', '1773', '1822', '1913',\n",
    "                          '1967', '4085', '4099', '4113', '145',\n",
    "                          '4165', '4280', '4326', '4362', '4382', '4428', '5133']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sales data column header matches:\n",
      "[ True  True  True  True  True  True  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsales = pd.read_csv(folderpath + 'combinedsales'+ lastweeksdate + '.csv',sep = '|',dtype = 'str')\n",
    "df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new sales data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "       'gross_transaction_cnt', 'class_code_id', 'class_gross_sales_amt'])\n",
    "dfsales = dfsales.append(df,ignore_index = True)\n",
    "a = (len(dfsales.index))\n",
    "dfsales = dfsales.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "       'class_code_id'])\n",
    "b = (len(dfsales.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"last week traffic data duplication deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek = (max(dfsales['week_end_dt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales.to_csv(folderpath + 'combinedsales'+ recentweek + '.csv',index = False,sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales = dfsales[~dfsales['location_id'].isin(closed_onlinestorelist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputpath = folderpath +'Output_' + recentweek +'/'\n",
    "try:\n",
    "    os.stat(outputpath)\n",
    "except:\n",
    "    os.mkdir(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with ? sales/transaction: 0\n"
     ]
    }
   ],
   "source": [
    "dfnodata = dfsales[(dfsales['class_gross_sales_amt'] == '?')&\\\n",
    "                   (dfsales['week_end_dt'] == recentweek)]\n",
    "dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "print(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales['week_end_dt'] = pd.to_datetime(dfsales['week_end_dt'])\n",
    "dfsales = dfsales[dfsales['class_gross_sales_amt']!='?']\n",
    "dfsales = dfsales.reset_index(drop = True)\n",
    "\n",
    "dfsales['gross_sales_amt'] = dfsales['gross_sales_amt'].astype('float')\n",
    "dfsales['gross_transaction_cnt'] = dfsales['gross_transaction_cnt'].astype('float')\n",
    "dfsales['class_gross_sales_amt'] = dfsales['class_gross_sales_amt'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweeklist = dfsales[['week_end_dt','fiscal_week_nbr']].drop_duplicates()\n",
    "dfweeklist = dfweeklist.sort_values('week_end_dt',ascending = False)\n",
    "dfweeklist.reset_index(drop = True,inplace = True)\n",
    "dfweeklist.reset_index(inplace = True)\n",
    "\n",
    "dfweeklist_wow = dfweeklist.copy()\n",
    "dfweeklist_wow['index'] = dfweeklist_wow['index'] - 1\n",
    "dfweeklist_wow = dfweeklist_wow[['index','week_end_dt']]\n",
    "dfweeklist_wow.columns = ['index','weeklastweek']\n",
    "\n",
    "dfweeklist = dfweeklist[dfweeklist['index']<104]\n",
    "dfweeklist.reset_index(drop = True,inplace = True)\n",
    "dfweeklist['year'] = np.ceil((dfweeklist['index'] + 1)/52)\n",
    "\n",
    "dfweeklist1 = dfweeklist[dfweeklist['year'] == 1]\n",
    "dfweeklist1 = dfweeklist1[['index', 'week_end_dt', 'fiscal_week_nbr']]\n",
    "dfweeklist2 = dfweeklist[dfweeklist['year'] == 2]\n",
    "dfweeklist2 = dfweeklist2[['week_end_dt', 'fiscal_week_nbr']]\n",
    "dfweeklist2.columns = ['weeklastyear', 'fiscal_week_nbr']\n",
    "\n",
    "dfweeklist = pd.merge(dfweeklist1,dfweeklist2,on ='fiscal_week_nbr' )\n",
    "dfweeklist = pd.merge(dfweeklist,dfweeklist_wow,on ='index')\n",
    "del dfweeklist1,dfweeklist2,dfweeklist_wow\n",
    "\n",
    "dfweeklist.to_csv(outputpath + 'weeklist.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek_date = (max(dfsales['week_end_dt']))\n",
    "dfcheck = dfsales[dfsales['week_end_dt'] == recentweek_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stores with zero sales/transaction: 0\n",
      "stores gross sales can not match sum of class sales: 1\n",
      "stores with zero class sales: 0\n"
     ]
    }
   ],
   "source": [
    "dfcheck_total1 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "a = (len(dfcheck_total1.index))\n",
    "dfcheck_total1 = dfcheck_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "b = (len(dfcheck_total1.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"last week sales multiple gross sales/trasaction in the same store\")\n",
    "\n",
    "dfcheck_total2 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfcheck_total2.reset_index(inplace = True)\n",
    "\n",
    "dfcheck_total = pd.merge(dfcheck_total1,dfcheck_total2,\n",
    "                        on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                        how = 'outer')\n",
    "del dfcheck_total1,dfcheck_total2\n",
    "\n",
    "dfcheck_zero = dfcheck_total[(dfcheck_total['class_gross_sales_amt']<=0)|\\\n",
    "                            (dfcheck_total['gross_transaction_cnt']<=0) ]\n",
    "\n",
    "dfcheck_zero.to_csv(outputpath + 'zerosales.csv',index = False)\n",
    "print(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "del dfcheck_zero\n",
    "\n",
    "dfcheck_total['TotalDiff'] = dfcheck_total['gross_sales_amt']-dfcheck_total['class_gross_sales_amt']\n",
    "dfcheck_total['TotalDiff'] = dfcheck_total['TotalDiff'].round()\n",
    "dfcheck_totalnonmatch = dfcheck_total[dfcheck_total['TotalDiff']!=0]\n",
    "print(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "\n",
    "dfcheck_totalnonmatch.to_csv(outputpath + 'totalnonmatch.csv',index = False)\n",
    "del dfcheck_totalnonmatch\n",
    "\n",
    "dfcheck_zeroclass = dfcheck[(dfcheck['class_gross_sales_amt']==0)]\n",
    "dfcheck_zeroclass.to_csv(outputpath + 'zeroclasssales.csv',index = False)\n",
    "print(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "del dfcheck_zeroclass\n",
    "del dfcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total1 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "dfsales_total1 = dfsales_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "dfsales_total2 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfsales_total2.reset_index(inplace = True)\n",
    "\n",
    "dfsales_total = pd.merge(dfsales_total1,dfsales_total2,\n",
    "                        on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                        how = 'outer')\n",
    "del dfsales_total1,dfsales_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfstore = pd.read_table('/home/jubauser1/BigLots/static_files/MediaStormStoresList_0913.txt',\n",
    "                        sep = '|',dtype = 'str')\n",
    "dfstore['open_dt'] = pd.to_datetime(dfstore['open_dt'])\n",
    "dfstore['open_dtwd'] = dfstore['open_dt'].dt.dayofweek\n",
    "dfstore['open_wk'] = np.where(dfstore['open_dtwd']<=5,\n",
    "                       dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(5-x.weekday()))),\n",
    "                       dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(12-x.weekday()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores w/o detailed info: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "dfstorematch = dfsales_total[['location_id']].drop_duplicates()\n",
    "dfstorematch = pd.merge(dfstorematch,dfstore[['location_id','address_line_1']],\n",
    "                        on = 'location_id',how = 'left')\n",
    "dfstorematch['address_line_1'].fillna('empty',inplace = True)\n",
    "dfstorematch = dfstorematch[dfstorematch['address_line_1']=='empty']\n",
    "print(\"stores w/o detailed info: \")\n",
    "print(dfstorematch['location_id'].unique())\n",
    "del dfstorematch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total = pd.merge(dfsales_total,dfstore[['location_id','open_wk']],\n",
    "                        on = 'location_id',how = 'left')\n",
    "dfsales_total['open_wk'].fillna(datetime.datetime.strptime(str(20200101), '%Y%m%d').date(),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new traffic data column header matches:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftraffic = pd.read_csv(folderpath + 'combinedtraffic'+ lastweeksdate + '.csv',\n",
    "               sep = '|',dtype = 'str')\n",
    "\n",
    "df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new traffic data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "       'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "       'traffic_day_6', 'traffic_day_7'])\n",
    "dftraffic = dftraffic.append(df,ignore_index = True)\n",
    "a = (len(dftraffic.index))\n",
    "dftraffic = dftraffic.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "b = (len(dftraffic.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"last week traffic data duplication deduped\")\n",
    "dftraffic.to_csv(folderpath + 'combinedtraffic'+ recentweek + '.csv',index = False,sep = '|')\n",
    "\n",
    "dftraffic['traffic_week'] = 0 \n",
    "for i in ['traffic_day_1','traffic_day_2', 'traffic_day_3', 'traffic_day_4',\n",
    "          'traffic_day_5', 'traffic_day_6', 'traffic_day_7']:\n",
    "    dftraffic[i] = dftraffic[i].astype('float')\n",
    "    dftraffic['traffic_week'] = dftraffic['traffic_week'] +dftraffic[i]\n",
    "dftraffic['week_end_dt'] = pd.to_datetime(dftraffic['week_end_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new inventory data column header matches:\n",
      "[ True  True  True  True False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfinventory = pd.read_csv(folderpath + 'combinedinventory'+ lastweeksdate + '.csv',\n",
    "               sep = '|',dtype = 'str')\n",
    "\n",
    "df = pd.read_csv(newinventorypath,sep = '|',dtype = 'str')\n",
    "a = df.columns\n",
    "print(\"new inventory data column header matches:\")\n",
    "print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "df.columns = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand']\n",
    "dfinventory = dfinventory.append(df,ignore_index = True)\n",
    "a = (len(dfinventory.index))\n",
    "dfinventory = dfinventory.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id'])\n",
    "b = (len(dfinventory.index))\n",
    "if a==b:\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"last week inventory data duplication deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory.to_csv(folderpath + 'combinedinventory'+ recentweek + '.csv',index = False,sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory['week_end_dt'] = pd.to_datetime(dfinventory['week_end_dt'])\n",
    "dfinventory['on_hand'] = dfinventory['on_hand'].astype('float')\n",
    "dfinventory_total = dfinventory.groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "dfinventory_total.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total,\n",
    "                                dfweeklist[['week_end_dt', 'fiscal_week_nbr','weeklastyear','weeklastweek']],\n",
    "                                on= ['week_end_dt', 'fiscal_week_nbr'])\n",
    "\n",
    "dfsales_total_lastyear = pd.merge(dfsales_total,\n",
    "                                 dfweeklist[['weeklastyear']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "dfsales_total_lastyear = dfsales_total_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "dfsales_total_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastyear,\n",
    "                                on = ['location_id','weeklastyear'],how = 'outer')\n",
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "\n",
    "dfsales_total_recent['Store_Category'] = np.where(dfsales_total_recent['open_wk']>=dfsales_total_recent['weeklastyear'],'New',\n",
    "                                         np.where((dfsales_total_recent['gross_transaction_cnt_ly']==0)&(dfsales_total_recent['class_gross_sales_amt_ly']==0),\n",
    "                                         'Converted',\n",
    "                                         np.where((dfsales_total_recent['gross_transaction_cnt']==0)&(dfsales_total_recent['class_gross_sales_amt']==0),\n",
    "                                        'Converted','Complete')))\n",
    "\n",
    "dfsales_total_lastweek = pd.merge(dfsales_total,\n",
    "                                 dfweeklist[['weeklastweek']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastweek')\n",
    "\n",
    "dfsales_total_lastweek = dfsales_total_lastweek[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastweek']]\n",
    "dfsales_total_lastweek.columns = ['location_id','gross_transaction_cnt_lw', 'class_gross_sales_amt_lw','weeklastweek']\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastweek,\n",
    "                                on = ['location_id','weeklastweek'],how = 'left')\n",
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "\n",
    "dfsales_total_recent['week_end_dt'] = np.where(dfsales_total_recent['week_end_dt']=='1970-01-01',\n",
    "                                       dfsales_total_recent['weeklastyear'] + pd.DateOffset(364),\n",
    "                                       dfsales_total_recent['week_end_dt'])\n",
    "dfsales_total_recent['weeklastyear'] = np.where(dfsales_total_recent['weeklastyear']=='1970-01-01',\n",
    "                                       dfsales_total_recent['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                       dfsales_total_recent['weeklastyear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with no 2017&2016 sales and transaction data: 466\n",
      "Last Week: stores with no 2017&2016 sales and transaction data: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "dfallstorelist = dfstore[~dfstore['location_id'].isin(closed_onlinestorelist)]\n",
    "dfallstorelist.reset_index(drop = True, inplace = True)\n",
    "dfweeklist2 = dfweeklist.copy()\n",
    "dfallstorelist['concat'] = 1\n",
    "dfweeklist2['concat'] = 1\n",
    "dfallstorelist = pd.merge(dfallstorelist,dfweeklist2,on='concat')\n",
    "dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "dfallstorelist = pd.merge(dfallstorelist,dfsales_total_recent,on=['location_id','week_end_dt'],\n",
    "                         how = 'left')\n",
    "\n",
    "dfallstorelist.fillna(0,inplace = True)\n",
    "dfallstorelist = dfallstorelist[(dfallstorelist['gross_sales_amt']==0)&\\\n",
    "                               (dfallstorelist['gross_transaction_cnt']==0)&\\\n",
    "                               (dfallstorelist['class_gross_sales_amt']==0)&\\\n",
    "                               (dfallstorelist['gross_transaction_cnt_ly']==0)&\\\n",
    "                               (dfallstorelist['class_gross_sales_amt_ly']==0)]\n",
    "dfallstorelist = dfallstorelist.sort_values(['week_end_dt','location_id'],ascending = [0,1])\n",
    "dfallstorelist = dfallstorelist[['week_end_dt','location_id']]\n",
    "dfallstorelist.to_csv(outputpath + 'nobothyeardatastores.csv',index = False)\n",
    "print(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "test = dfallstorelist[dfallstorelist['week_end_dt']==recentweek_date]\n",
    "print(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "del test,dfweeklist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_end_dt</th>\n",
       "      <th>location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65262</th>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65263</th>\n",
       "      <td>2017-08-26</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74207</th>\n",
       "      <td>2017-08-26</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65264</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74208</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74157</th>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65265</th>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74209</th>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74158</th>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65266</th>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74210</th>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74159</th>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65267</th>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74211</th>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74160</th>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65268</th>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74212</th>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65165</th>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65217</th>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74161</th>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65269</th>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74213</th>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65166</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65218</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74162</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65270</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74214</th>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65167</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65219</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74163</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67650</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>4671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58758</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>4672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67702</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>4673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73994</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65102</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74046</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65154</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74098</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65206</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74150</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65258</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74202</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65310</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67599</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>4666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58707</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67651</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>4671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58759</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>4672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67703</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>4673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73995</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65103</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74047</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65155</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74099</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65207</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74151</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65259</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74203</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65311</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      week_end_dt location_id\n",
       "65262  2017-09-02        5350\n",
       "65263  2017-08-26        5350\n",
       "74207  2017-08-26        5351\n",
       "65264  2017-08-19        5350\n",
       "74208  2017-08-19        5351\n",
       "74157  2017-08-12        5348\n",
       "65265  2017-08-12        5350\n",
       "74209  2017-08-12        5351\n",
       "74158  2017-08-05        5348\n",
       "65266  2017-08-05        5350\n",
       "74210  2017-08-05        5351\n",
       "74159  2017-07-29        5348\n",
       "65267  2017-07-29        5350\n",
       "74211  2017-07-29        5351\n",
       "74160  2017-07-22        5348\n",
       "65268  2017-07-22        5350\n",
       "74212  2017-07-22        5351\n",
       "65165  2017-07-15        5344\n",
       "65217  2017-07-15        5346\n",
       "74161  2017-07-15        5348\n",
       "65269  2017-07-15        5350\n",
       "74213  2017-07-15        5351\n",
       "65166  2017-07-08        5344\n",
       "65218  2017-07-08        5346\n",
       "74162  2017-07-08        5348\n",
       "65270  2017-07-08        5350\n",
       "74214  2017-07-08        5351\n",
       "65167  2017-07-01        5344\n",
       "65219  2017-07-01        5346\n",
       "74163  2017-07-01        5348\n",
       "...           ...         ...\n",
       "67650  2016-10-01        4671\n",
       "58758  2016-10-01        4672\n",
       "67702  2016-10-01        4673\n",
       "73994  2016-10-01        5335\n",
       "65102  2016-10-01        5336\n",
       "74046  2016-10-01        5338\n",
       "65154  2016-10-01        5339\n",
       "74098  2016-10-01        5343\n",
       "65206  2016-10-01        5344\n",
       "74150  2016-10-01        5345\n",
       "65258  2016-10-01        5346\n",
       "74202  2016-10-01        5348\n",
       "65310  2016-10-01        5350\n",
       "74254  2016-10-01        5351\n",
       "67599  2016-09-24        4666\n",
       "58707  2016-09-24        4669\n",
       "67651  2016-09-24        4671\n",
       "58759  2016-09-24        4672\n",
       "67703  2016-09-24        4673\n",
       "73995  2016-09-24        5335\n",
       "65103  2016-09-24        5336\n",
       "74047  2016-09-24        5338\n",
       "65155  2016-09-24        5339\n",
       "74099  2016-09-24        5343\n",
       "65207  2016-09-24        5344\n",
       "74151  2016-09-24        5345\n",
       "65259  2016-09-24        5346\n",
       "74203  2016-09-24        5348\n",
       "65311  2016-09-24        5350\n",
       "74255  2016-09-24        5351\n",
       "\n",
       "[466 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfallstorelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfallstorelist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfallstorelist['location_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic,\n",
    "                                on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "dftraffic2 = dftraffic[['location_id', 'week_end_dt','traffic_day_1',\n",
    "       'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "       'traffic_day_6', 'traffic_day_7','traffic_week']]\n",
    "dftraffic2.columns = ['location_id', 'weeklastyear','traffic_day_1_ly',\n",
    "       'traffic_day_2_ly', 'traffic_day_3_ly', 'traffic_day_4_ly', 'traffic_day_5_ly',\n",
    "       'traffic_day_6_ly', 'traffic_day_7_ly','traffic_week_ly']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic2,\n",
    "                                on=['location_id', 'weeklastyear'],how = 'left')\n",
    "del dftraffic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total,\n",
    "                                on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "\n",
    "dfinventory_total2 = dfinventory_total[['location_id', 'week_end_dt','on_hand']]\n",
    "dfinventory_total2.columns = ['location_id', 'weeklastyear','on_hand_ly']\n",
    "\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total2,\n",
    "                                on=['location_id', 'weeklastyear'],how = 'left')\n",
    "del dfinventory_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recentweek_last=datetime.datetime.strptime(recentweek, '%Y-%m-%d').date()\n",
    "recentweek_last=recentweek_last+datetime.timedelta(days=(-84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent['yoysales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_ly'] - 1\n",
    "dfsales_total_recent['yoytrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_ly'] - 1\n",
    "dfsales_total_recent['wowsales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_lw'] - 1\n",
    "dfsales_total_recent['wowtrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_lw'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores with high yoy change: 0\n",
      "Last Week: stores with high yoy change: 0\n"
     ]
    }
   ],
   "source": [
    "dfsales_total_recent_delete = dfsales_total_recent[(dfsales_total_recent['Store_Category']=='Complete')&\\\n",
    "                                                   ((abs(dfsales_total_recent['yoysales'])>0.2)&\\\n",
    "                                                   (abs(dfsales_total_recent['yoytrans'])>0.2))]#|\\\n",
    "                                                   #(abs(dfsales_total_recent['wowsales'])>0.2)|\\\n",
    "                                                   #(abs(dfsales_total_recent['wowtrans'])>0.2))]\n",
    "dfsales_total_recent_delete = dfsales_total_recent_delete.sort_values(['week_end_dt','location_id'],\n",
    "                                                                     ascending = [0,1])\n",
    "dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores.csv',index = False)\n",
    "print(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "test = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "print(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfsales_total_recent =  dfsales_total_recent[(dfsales_total_recent['gross_transaction_cnt']!=0)|\\\n",
    "#                                             (dfsales_total_recent['class_gross_sales_amt']!=0)]\n",
    "dfsales_total_recent = dfsales_total_recent[(dfsales_total_recent['Store_Category']!='Complete')|\\\n",
    "                                            ((dfsales_total_recent['Store_Category']=='Complete')&\\\n",
    "                                            (abs(dfsales_total_recent['yoysales'])<=0.2)|\\\n",
    "                                            (abs(dfsales_total_recent['yoytrans'])<=0.2))]#&\\\n",
    "                                            #(abs(dfsales_total_recent['wowsales'])<=0.2)&\\\n",
    "                                            #(abs(dfsales_total_recent['wowtrans'])<=0.2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dfweeklist2 = dfweeklist[['week_end_dt']]\n",
    "dfweeklist2['week_end_dt_8w'] = dfweeklist2['week_end_dt']+pd.DateOffset(-84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweeklist_12plus = dfsales[['week_end_dt']].drop_duplicates()\n",
    "dfweeklist_12plus = dfweeklist_12plus.sort_values('week_end_dt',ascending = False)\n",
    "dfweeklist_12plus.reset_index(drop = True,inplace = True)\n",
    "dfweeklist_12plus.reset_index(inplace = True)\n",
    "dfweeklist_12plus = dfweeklist_12plus[dfweeklist_12plus['index']<64]\n",
    "dfweeklist_12plus['weeklastyear'] = dfweeklist_12plus['week_end_dt'] + pd.DateOffset(-364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_12plus = pd.merge(dfsales_total,dfweeklist_12plus,on= ['week_end_dt'])\n",
    "\n",
    "dfsales_12plus_lastyear = pd.merge(dfsales_total,\n",
    "                                 dfweeklist_12plus[['weeklastyear']],\n",
    "                                 left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "\n",
    "dfsales_12plus_lastyear = dfsales_12plus_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "dfsales_12plus_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "\n",
    "dfsales_12plus = pd.merge(dfsales_12plus,dfsales_12plus_lastyear,\n",
    "                                on = ['location_id','weeklastyear'],how = 'left')\n",
    "dfsales_12plus.fillna(0,inplace = True)\n",
    "\n",
    "dfsales_12plus['Store_Category'] = np.where(dfsales_12plus['open_wk']>=dfsales_12plus['weeklastyear'],'New',\n",
    "                                         np.where((dfsales_12plus['gross_transaction_cnt_ly']==0)&(dfsales_12plus['class_gross_sales_amt_ly']==0),\n",
    "                                         'Converted',\n",
    "                                         np.where((dfsales_12plus['gross_transaction_cnt']==0)&(dfsales_12plus['class_gross_sales_amt']==0),\n",
    "                                        'Converted','Complete')))\n",
    "dfsales_12plus['yoysales'] = dfsales_12plus['class_gross_sales_amt']/dfsales_12plus['class_gross_sales_amt_ly'] - 1\n",
    "dfsales_12plus['yoytrans'] = dfsales_12plus['gross_transaction_cnt']/dfsales_12plus['gross_transaction_cnt_ly'] - 1\n",
    "dfsales_12plus = dfsales_12plus[(dfsales_12plus['Store_Category']!='Complete')|\\\n",
    "                                            ((dfsales_12plus['Store_Category']=='Complete')&\\\n",
    "                                            (abs(dfsales_12plus['yoysales'])<=0.2)|\\\n",
    "                                            (abs(dfsales_12plus['yoytrans'])<=0.2))]#&\\\n",
    "dfsales_12plus = dfsales_12plus[['location_id','week_end_dt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_rankingall = pd.DataFrame()\n",
    "for i in range(52):\n",
    "    cweekdate = dfweeklist2['week_end_dt'][i]\n",
    "    recentweek_last = dfweeklist2['week_end_dt_8w'][i]\n",
    "    dfsales_ranking = dfsales_total[dfsales_total['week_end_dt']>recentweek_last]\n",
    "    dfsales_ranking = dfsales_ranking[dfsales_ranking['week_end_dt']<=cweekdate]\n",
    "    dfsales_ranking = pd.merge(dfsales_ranking,dfsales_12plus,\n",
    "                           on = ['location_id', 'week_end_dt'])\n",
    "    dfsales_ranking = pd.merge(dfsales_ranking,\n",
    "                               dftraffic[['location_id', 'week_end_dt', 'fiscal_week_nbr','traffic_week']],\n",
    "                               on =['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "    dfsales_ranking.fillna(0,inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking[['location_id','class_gross_sales_amt','traffic_week']].groupby('location_id').sum()\n",
    "    dfsales_ranking['Rev/Traffic'] = dfsales_ranking['class_gross_sales_amt']/dfsales_ranking['traffic_week']\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.sort_values('class_gross_sales_amt',ascending = False)\n",
    "    dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.rename(columns = {'index':'rev_index'})\n",
    "    dfsales_ranking = dfsales_ranking.replace(np.inf, 0)\n",
    "    \n",
    "    dfsales_ranking = dfsales_ranking.sort_values('Rev/Traffic',ascending = False)\n",
    "    dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "    dfsales_ranking.reset_index(inplace = True)\n",
    "    dfsales_ranking = dfsales_ranking.rename(columns = {'index':'traffi_index'})\n",
    "    \n",
    "    cols = len(dfsales_ranking.index)\n",
    "    dfsales_ranking['Store_Revenue_Rank'] = np.where(dfsales_ranking['rev_index']/cols <=0.2,'H',\n",
    "                                                    np.where(dfsales_ranking['rev_index']/cols <=0.8,'M','L'))\n",
    "    dfsales_ranking['Store_Revenue/Traffic_Rank'] = np.where(dfsales_ranking['traffi_index']/cols <=0.2,'H',\n",
    "                                                    np.where(dfsales_ranking['traffi_index']/cols <=0.8,'M','L'))\n",
    "    dfsales_ranking['week_end_dt'] = cweekdate\n",
    "    dfsales_rankingall = dfsales_rankingall.append(dfsales_ranking,ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Store_Revenue_Rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Store_Revenue_Rank'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-50eadd8161f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mdfsales_rankingall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'week_end_dt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Store_Revenue_Rank'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Store_Revenue/Traffic_Rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 on = ['location_id','week_end_dt'],how = 'left')\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdfsales_total_recent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store_Revenue_Rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdfsales_total_recent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store_Revenue/Traffic_Rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Store_Revenue_Rank'"
     ]
    }
   ],
   "source": [
    "dfsales_total_recent.fillna(0,inplace = True)\n",
    "dfsales_total_recent.reset_index(drop = True,inplace = True)\n",
    "dfsales_total_recent = pd.merge(dfsales_total_recent,\n",
    "                                dfsales_rankingall[['location_id','week_end_dt','Store_Revenue_Rank','Store_Revenue/Traffic_Rank']],\n",
    "                                on = ['location_id','week_end_dt'],how = 'left')\n",
    "dfsales_total_recent['Store_Revenue_Rank'].fillna('NA',inplace = True)\n",
    "dfsales_total_recent['Store_Revenue/Traffic_Rank'].fillna('NA',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_total_recent['AOV'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['gross_transaction_cnt']\n",
    "dfsales_total_recent['AOV_ly'] = dfsales_total_recent['class_gross_sales_amt_ly']/dfsales_total_recent['gross_transaction_cnt_ly']\n",
    "dfsales_total_recent['Trans/Traffic'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['traffic_week']\n",
    "dfsales_total_recent['Trans/Traffic_ly'] = dfsales_total_recent['gross_transaction_cnt_ly']/dfsales_total_recent['traffic_week_ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsales_total_recent['Store_Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'AOV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AOV'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-59896b13421e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'YoYDiff'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcolumnheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumnheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf_complete\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AOV'"
     ]
    }
   ],
   "source": [
    "df_complete = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Complete']\n",
    "df_complete.reset_index(drop = True, inplace = True)\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "columnheader = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    columnheader = columnheader + [i,a,b]\n",
    "    df_complete[b] = df_complete[i]/df_complete[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dfstore['open_dtwd']\n",
    "del dfstore['open_wk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dma = pd.read_csv('/home/jubauser1/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfstore['zip_cd'] = dfstore['zip_cd'].str[0:5]\n",
    "dfstore = pd.merge(dfstore,dma,on = 'zip_cd',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete = df_complete[columnheader]\n",
    "df_complete = pd.merge(df_complete,dfstore,on='location_id',how = 'left')\n",
    "df_complete = df_complete.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = dfsales_total_recent[dfsales_total_recent['Store_Category']=='New']\n",
    "df_new = df_new[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                 'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                 'Trans/Traffic','on_hand','Store_Category']]\n",
    "df_new = pd.merge(df_new,dfstore,on='location_id',how = 'left')\n",
    "df_new = df_new.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "del df_new['Store_Category']\n",
    "df_new.to_csv(outputpath + 'output2_new.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Complete', 'Converted', 'New'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsales_total_recent['Store_Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_converted = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Converted']\n",
    "del df_converted['fiscal_week_nbr']\n",
    "df_converted['week_end_dt'] = np.where(df_converted['week_end_dt']=='1970-01-01',\n",
    "                                       df_converted['weeklastyear'] + pd.DateOffset(364),\n",
    "                                       df_converted['week_end_dt'])\n",
    "df_converted['weeklastyear'] = np.where(df_converted['weeklastyear']=='1970-01-01',\n",
    "                                       df_converted['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                       df_converted['weeklastyear'])\n",
    "\n",
    "df_converted = pd.merge(df_converted,dfweeklist[['week_end_dt','fiscal_week_nbr']],\n",
    "                       on = 'week_end_dt',how='left')\n",
    "df_converted = df_converted[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                 'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                 'Trans/Traffic','on_hand','Store_Category',\n",
    "                 'weeklastyear','gross_transaction_cnt_ly','class_gross_sales_amt_ly']]\n",
    "df_converted = pd.merge(df_converted,dfstore,on='location_id',how = 'left')\n",
    "df_converted = df_converted.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "\n",
    "\n",
    "\n",
    "del df_converted['Store_Category']\n",
    "df_converted.to_csv(outputpath + 'output3_converted.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tradearea = pd.read_csv('/home/jubauser1/BiglotsCode/OtherInput/revised_trade_area_5_mile_using_zip_center.csv',dtype = 'str')\n",
    "\n",
    "df_tradearea_all = pd.DataFrame()\n",
    "for i in (range(len(df_tradearea.index))):\n",
    "    a = df_tradearea['stores'][i]\n",
    "    b = df_tradearea['trade_area_code'][i]\n",
    "    a = a.replace('[','')\n",
    "    a = a.replace(']','')\n",
    "    for c in range(a.count(',')+1):\n",
    "        df_tradearea_all = df_tradearea_all.append({'location_id': (a.split(',')[c]), 'trade_area_code': b}, ignore_index=True)\n",
    "df_tradearea_all['location_id'] = df_tradearea_all['location_id'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tradearea_all.to_csv('/home/jubauser1/BiglotsCode/OtherInput/talist.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete = pd.merge(df_complete,df_tradearea_all,on ='location_id',how = 'left')\n",
    "df_complete['trade_area_code'].fillna('NA',inplace = True)\n",
    "df_complete.to_csv(outputpath + 'output1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete.to_csv(outputpath + 'output1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','trade_area_code']).sum()\n",
    "df_tadata.reset_index(inplace = True)\n",
    "\n",
    "df_tadata['AOV'] = df_tadata['class_gross_sales_amt']/df_tadata['gross_transaction_cnt']\n",
    "df_tadata['AOV_ly'] = df_tadata['class_gross_sales_amt_ly']/df_tadata['gross_transaction_cnt_ly']\n",
    "df_tadata['Trans/Traffic'] = df_tadata['gross_transaction_cnt']/df_tadata['traffic_week']\n",
    "df_tadata['Trans/Traffic_ly'] = df_tadata['gross_transaction_cnt_ly']/df_tadata['traffic_week_ly']\n",
    "\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    df_tadata[b] = df_tadata[i]/df_tadata[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_taclass1 = df_complete[['location_id','trade_area_code','week_end_dt','Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank']]\n",
    "df_taclass1.reset_index(drop = True, inplace = True)\n",
    "df_taclass1['Number_of_HMStores'] = np.where(df_taclass1['Store_Revenue_Rank']!='L',1,0)\n",
    "df_taclass1['Number_of_HMStores_RevTrafRank'] = np.where(df_taclass1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "df_taclass1['Number of Stores'] = 1\n",
    "df_taclass1 = df_taclass1.groupby(['trade_area_code','week_end_dt']).sum()\n",
    "df_taclass1.reset_index(inplace = True)\n",
    "\n",
    "df_taclass2 = df_complete[['trade_area_code','week_end_dt','zip_cd']].drop_duplicates()\n",
    "df_taclass2 = df_taclass2.groupby(['trade_area_code','week_end_dt']).count()\n",
    "df_taclass2.columns = ['NumberofZipcodes']\n",
    "df_taclass2.reset_index(inplace = True)\n",
    "\n",
    "df_taclass3 = df_complete[['trade_area_code','state_nm']].drop_duplicates(['trade_area_code'])\n",
    "\n",
    "df_tadata = pd.merge(df_tadata,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "df_tadata = pd.merge(df_tadata,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "df_tadata = pd.merge(df_tadata,df_taclass3,on =['trade_area_code'])\n",
    "\n",
    "df_tadata = df_tadata.sort_values(['trade_area_code','week_end_dt'],ascending = [1,0])\n",
    "df_tadata.to_csv(outputpath + 'output4.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dmadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','DMA']).sum()\n",
    "df_dmadata.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata['AOV'] = df_dmadata['class_gross_sales_amt']/df_dmadata['gross_transaction_cnt']\n",
    "df_dmadata['AOV_ly'] = df_dmadata['class_gross_sales_amt_ly']/df_dmadata['gross_transaction_cnt_ly']\n",
    "df_dmadata['Trans/Traffic'] = df_dmadata['gross_transaction_cnt']/df_dmadata['traffic_week']\n",
    "df_dmadata['Trans/Traffic_ly'] = df_dmadata['gross_transaction_cnt_ly']/df_dmadata['traffic_week_ly']\n",
    "\n",
    "metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "              'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "              'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "for i in metricslist:\n",
    "    a = i+'_ly'\n",
    "    b = i+ 'YoYDiff'\n",
    "    df_dmadata[b] = df_dmadata[i]/df_dmadata[a] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_dmadata1 = df_complete[['location_id','DMA','week_end_dt','Store_Revenue_Rank',\n",
    "                           'Store_Revenue/Traffic_Rank']]\n",
    "df_dmadata1.reset_index(drop = True, inplace = True)\n",
    "df_dmadata1['Number_of_HMStores'] = np.where(df_dmadata1['Store_Revenue_Rank']!='L',1,0)\n",
    "df_dmadata1['Number_of_HMStores_RevTrafRank'] = np.where(df_dmadata1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "df_dmadata1['Number of Stores'] = 1\n",
    "df_dmadata1 = df_dmadata1.groupby(['DMA','week_end_dt']).sum()\n",
    "df_dmadata1.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata2 = df_complete[['DMA','week_end_dt','zip_cd']].drop_duplicates()\n",
    "df_dmadata2 = df_dmadata2.groupby(['DMA','week_end_dt']).count()\n",
    "df_dmadata2.columns = ['NumberofZipcodes']\n",
    "df_dmadata2.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata4 = df_complete[['DMA','week_end_dt','trade_area_code']].drop_duplicates()\n",
    "df_dmadata4 = df_dmadata4.groupby(['DMA','week_end_dt']).count()\n",
    "df_dmadata4.columns = ['NumberofTAs']\n",
    "df_dmadata4.reset_index(inplace = True)\n",
    "\n",
    "df_dmadata3 = df_complete[['DMA','state_nm']].drop_duplicates(['DMA'])\n",
    "\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata1,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata2,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata4,on =['DMA','week_end_dt'])\n",
    "df_dmadata = pd.merge(df_dmadata,df_dmadata3,on =['DMA'])\n",
    "\n",
    "df_dmadata = df_dmadata.sort_values(['DMA','week_end_dt'],ascending = [1,0])\n",
    "df_dmadata.to_csv(outputpath + 'output5.csv',index = False)\n",
    "del df_dmadata1,df_dmadata2,df_dmadata3,df_dmadata4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_storeweeklist = pd.merge(df_complete[['location_id', 'week_end_dt']],\n",
    "                            dfweeklist[['week_end_dt','weeklastyear']],on ='week_end_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsales_class1 = pd.merge(dfsales[['location_id', 'week_end_dt','class_code_id','class_gross_sales_amt']],\n",
    "                          df_storeweeklist,\n",
    "                          on = ['location_id', 'week_end_dt'])\n",
    "dfsales_class2 = pd.merge(dfsales[['location_id', 'week_end_dt', 'class_code_id','class_gross_sales_amt']],\n",
    "                          df_storeweeklist,\n",
    "                          left_on = ['location_id', 'week_end_dt'],\n",
    "                          right_on = ['location_id', 'weeklastyear'])\n",
    "\n",
    "del dfsales_class2['week_end_dt_x']\n",
    "dfsales_class2 = dfsales_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "\n",
    "dfsales_class1 = pd.merge(dfsales_class1,dfsales_class2,\n",
    "                          on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "dfsales_class1.fillna(0,inplace = True)\n",
    "del dfsales_class2\n",
    "del dfsales_class1['weeklastyear_x']\n",
    "del dfsales_class1['weeklastyear_y']\n",
    "dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_x':'class_gross_sales_amt'})\n",
    "dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_y':'class_gross_sales_amt_ly'})\n",
    "dfsales_class1['class_gross_sales_amt_yoy'] = dfsales_class1['class_gross_sales_amt']/dfsales_class1['class_gross_sales_amt_ly'] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfinventory_class1 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']], \n",
    "                              df_storeweeklist,\n",
    "                          on = ['location_id', 'week_end_dt'])\n",
    "dfinventory_class2 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']],\n",
    "                          df_storeweeklist,\n",
    "                          left_on = ['location_id', 'week_end_dt'],\n",
    "                          right_on = ['location_id', 'weeklastyear'])\n",
    "\n",
    "del dfinventory_class2['week_end_dt_x']\n",
    "dfinventory_class2 = dfinventory_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "\n",
    "dfinventory_class1 = pd.merge(dfinventory_class1,dfinventory_class2,\n",
    "                          on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "dfinventory_class1.fillna(0,inplace = True)\n",
    "del dfinventory_class2\n",
    "del dfinventory_class1['weeklastyear_x']\n",
    "del dfinventory_class1['weeklastyear_y']\n",
    "dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_x':'on_hand'})\n",
    "dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_y':'on_hand_ly'})\n",
    "dfinventory_class1['on_hand_yoy'] = dfinventory_class1['on_hand']/dfinventory_class1['on_hand_ly'] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tadetail = pd.merge(dfsales_class1,dfinventory_class1,\n",
    "                      on=['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "df_tadetail = pd.merge(df_tadetail,df_tradearea_all,on ='location_id',how = 'left')\n",
    "df_tadetail['trade_area_code'].fillna('NA',inplace = True)\n",
    "df_tadetail.fillna(0,inplace = True)\n",
    "\n",
    "df_tadetail = df_tadetail.groupby(['trade_area_code','week_end_dt','class_code_id']).sum()\n",
    "df_tadetail['class_gross_sales_amt_yoy'] = df_tadetail['class_gross_sales_amt']/df_tadetail['class_gross_sales_amt_ly'] -1\n",
    "df_tadetail['on_hand_yoy'] = df_tadetail['on_hand']/df_tadetail['on_hand_ly'] -1\n",
    "df_tadetail.reset_index(inplace = True)\n",
    "\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "df_tadetail = pd.merge(df_tadetail,df_taclass3,on =['trade_area_code'])\n",
    "df_tadetail = pd.merge(dfweeklist[['week_end_dt','fiscal_week_nbr']],df_tadetail,\n",
    "                      on ='week_end_dt')\n",
    "df_tadetail.to_csv(outputpath + 'output4_classdetail.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(outputpath+'BigLots_Weekly_Data_'+recentweek+'.xlsx',\n",
    "                            #engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_transaction_cnt']]\n",
    "test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "test.to_excel(writer,'Transactions', index=False)\n",
    "test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_gross_sales_amt']]\n",
    "test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "test.to_excel(writer,'Revenue', index=False)\n",
    "dftraffic = dftraffic[~dftraffic['location_id'].isin(closed_onlinestorelist)]\n",
    "dftraffic = dftraffic.sort_values(['location_id', 'week_end_dt'])\n",
    "dfinventory_total = dfinventory_total[~dfinventory_total['location_id'].isin(closed_onlinestorelist)]\n",
    "dfinventory_total = dfinventory_total.sort_values(['location_id', 'week_end_dt'])\n",
    "dftraffic.to_excel(writer,'Traffic', index=False)\n",
    "dfinventory_total.to_excel(writer,'Inventory', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_complete[df_complete['week_end_dt']==recentweek_date]\n",
    "df_complete_week.to_csv(outputpath + 'output1_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_tadetail_week = df_tadetail[df_tadetail['week_end_dt']==recentweek_date]\n",
    "df_tadetail_week.to_csv(outputpath + 'output4_classdetail_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_dmadata_week = df_dmadata[df_dmadata['week_end_dt']==recentweek_date]\n",
    "df_dmadata_week.to_csv(outputpath + 'output5_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_tadata_week = df_tadata[df_tadata['week_end_dt']==recentweek_date]\n",
    "df_tadata_week.to_csv(outputpath + 'output4_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_converted_week = df_converted[df_converted['week_end_dt']==recentweek_date]\n",
    "df_converted_week.to_csv(outputpath + 'output3_converted_' + recentweek + '.csv',index = False)\n",
    "\n",
    "df_new_week = df_new[df_new['week_end_dt']==recentweek_date]\n",
    "df_new_week.to_csv(outputpath + 'output2_new_' + recentweek + '.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_complete_week[['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'location_desc', 'open_dt',\n",
    "                   'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                   'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter(outputpath+'Output1Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_row(0,None, format2)\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format4)\n",
    "worksheet.set_column('W:W', None, format3)\n",
    "worksheet.set_column('Z:Z', None, format3)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('W:W', None, format4)\n",
    "worksheet.set_column('Z:Z', None, format3)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('E:E', 12, None)\n",
    "worksheet.set_column('F:F', 16, None)\n",
    "worksheet.set_column('O:Y', None, format1)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('W:W', None, format3)\n",
    "worksheet.set_column('Z:Z', None, format4)\n",
    "worksheet.set_column('U:V', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_tadata_week[['trade_area_code', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'Number_of_HMStores', 'Number_of_HMStores_RevTrafRank',\n",
    "                   'Number of Stores', 'NumberofZipcodes', 'state_nm',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "\n",
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter(outputpath+'Output4Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format4)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format3)\n",
    "worksheet.set_column('Q:Q', None, format4)\n",
    "worksheet.set_column('T:T', None, format3)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('I:S', None, format1)\n",
    "worksheet.set_column('K:K', None, format3)\n",
    "worksheet.set_column('N:N', None, format3)\n",
    "worksheet.set_column('Q:Q', None, format3)\n",
    "worksheet.set_column('T:T', None, format4)\n",
    "worksheet.set_column('O:P', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete_week = df_dmadata_week[['DMA', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                     'Number_of_HMStores',\n",
    "                     'Number_of_HMStores_RevTrafRank', 'Number of Stores',\n",
    "                     'NumberofZipcodes', 'NumberofTAs', 'state_nm',\n",
    "                   'class_gross_sales_amt',\n",
    "                   'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                   'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                   'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                   'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "\n",
    "df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "\n",
    "df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(outputpath+'Output5Tracker_'+recentweek+'.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            datetime_format='yyyy-mm-dd',\n",
    "                            date_format='yyyy-mm-dd')\n",
    "\n",
    "workbook  = writer.book\n",
    "\n",
    "format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "format2 = workbook.add_format({'text_wrap' : True})\n",
    "format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                               'bg_color': 'FF9999'})\n",
    "\n",
    "df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "worksheet = writer.sheets['TransactionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format4)\n",
    "worksheet.set_column('R:R', None, format3)\n",
    "worksheet.set_column('U:U', None, format3)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "worksheet = writer.sheets['ConversionTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format3)\n",
    "worksheet.set_column('R:R', None, format4)\n",
    "worksheet.set_column('U:U', None, format3)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "worksheet = writer.sheets['TrafficTracker']\n",
    "worksheet.set_column('B:B', 12, None)\n",
    "worksheet.set_column('J:T', None, format1)\n",
    "worksheet.set_column('L:L', None, format3)\n",
    "worksheet.set_column('O:O', None, format3)\n",
    "worksheet.set_column('R:R', None, format3)\n",
    "worksheet.set_column('U:U', None, format4)\n",
    "worksheet.set_column('P:Q', None, format3)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
