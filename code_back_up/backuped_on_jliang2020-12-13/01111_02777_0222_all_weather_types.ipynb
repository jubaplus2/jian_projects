{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import glob\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder=\"/home/jian/Projects/Big_Lots/Weather/Json_data/daily/api_response_20180524/\"\n",
    "json_list_all=glob.glob(folder+\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inclusion_stores=pd.read_excel(\"/home/jian/Projects/Big_Lots/Q1_Post/BL_Sales YoY_JL_20180514.xlsx\",sheetname=\"Inclusion Stores\",skiprows=1,dtype=str)\n",
    "store_zips=pd.read_excel(\"/home/jian/Projects/Big_Lots/Other_Input/all_store_DMA.xlsx\",dtype=str)[['location_id','zip']]\n",
    "store_zips['zip']=store_zips['zip'].apply(lambda x: x.zfill(5))\n",
    "inclusion_stores=pd.merge(inclusion_stores,store_zips,on=\"location_id\",how=\"left\")\n",
    "inclusion_stores=inclusion_stores[['location_id','zip']].rename(columns={\"zip\":\"zip_cd\"})\n",
    "inclusion_stores=inclusion_stores[inclusion_stores['location_id']!='1769']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_files=pd.DataFrame({\"file\":json_list_all},index=range(len(json_list_all)))\n",
    "df_files['Date']=df_files['file'].apply(lambda x: datetime.datetime.strptime(x[len(x)-15:len(x)-5],\"%Y-%m-%d\"))\n",
    "df_files=df_files[(df_files['Date']>=datetime.datetime(2018,2,4)) & (df_files['Date']<=datetime.datetime(2018,5,5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_weather_groups=[]\n",
    "all_weather_desc=[]\n",
    "all_weather_id=[]\n",
    "df_missing_weather=pd.DataFrame()\n",
    "for file in df_files['file']:\n",
    "    response=json.load(open(file,\"r\"))\n",
    "    for zip_cd in inclusion_stores['zip_cd']:\n",
    "        try:        \n",
    "            data_zip_weather=response[zip_cd]['weather']\n",
    "            for i in range(len(data_zip_weather)):            \n",
    "                weather_group=data_zip_weather[i]['main']\n",
    "                weather_desc=data_zip_weather[i]['description']\n",
    "                \n",
    "                all_weather_groups=list(set(all_weather_groups+[weather_group]))\n",
    "                all_weather_desc=list(set(all_weather_desc+[weather_desc]))\n",
    "                \n",
    "        except:\n",
    "            df_missing_weather=df_missing_weather.append(pd.DataFrame({\"Date\":file[len(file)-15:len(file)-5],\"zips\":zip_cd},index=[0]))\n",
    "            # print(\"zip data not available: \" + zip_cd+\" | \"+file[len(file)-15:len(file)-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_missing_weather=df_missing_weather.sort_values([\"Date\",\"zips\"])\n",
    "writer_folder=\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/\"+str(datetime.datetime.now().now().date())+\"/\"\n",
    "\n",
    "try:\n",
    "    os.stat(writer_folder)\n",
    "except:\n",
    "    os.makedirs(writer_folder)\n",
    "\n",
    "\n",
    "writer=pd.ExcelWriter(writer_folder+\"Q1_inclusion_store_all_weather_type_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"all_type_group\":all_weather_groups}).to_excel(writer,\"all_weather_group_list\",index=False)\n",
    "pd.DataFrame({\"all_type_desc\":all_weather_desc}).to_excel(writer,\"all_weather_desc_list\",index=False)\n",
    "df_missing_weather.to_excel(writer,\"missing_zips\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_rain_snow_df=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Q1_inclusion_store_all_weather_type_ranked_Server.xlsx\",sheetname=\"ranked rain snow\")\n",
    "\n",
    "ranked_rain=ranked_rain_snow_df.iloc[:,[4,5]]\n",
    "ranked_rain.columns=[['rain_desc','rank']]\n",
    "ranked_rain=ranked_rain.sort_values(\"rank\")\n",
    "\n",
    "ranked_snow=ranked_rain_snow_df.iloc[:,[0,1]]\n",
    "ranked_snow.columns=[['snow_desc','rank']]\n",
    "ranked_snow=ranked_snow[~pd.isnull(ranked_snow['snow_desc'])]\n",
    "ranked_snow=ranked_snow.sort_values(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rain_desc</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>light intensity drizzle rain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rain_desc  rank\n",
       "0  light intensity drizzle rain     1\n",
       "1       light intensity drizzle     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_rain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Temp, Rain Desc, Snow Desc, All Weather Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_rain=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"No_Rain\"]+ranked_rain['rain_desc'].tolist())\n",
    "df_output_snow=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"No_Snow\"]+ranked_snow['snow_desc'].tolist())\n",
    "df_output_temp=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"Temp\",\"Temp_Max\",\"Temp_Min\"])\n",
    "df_output_group=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\"]+all_weather_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2018-06-15 12:54:53.562853 finished of the date:  2018-02-22\n",
      "2 2018-06-15 12:55:10.471384 finished of the date:  2018-03-19\n",
      "3 2018-06-15 12:55:28.353622 finished of the date:  2018-02-18\n",
      "4 2018-06-15 12:55:47.503212 finished of the date:  2018-04-27\n",
      "5 2018-06-15 12:56:08.811191 finished of the date:  2018-04-07\n",
      "6 2018-06-15 12:56:31.170083 finished of the date:  2018-03-03\n",
      "7 2018-06-15 12:56:54.415408 finished of the date:  2018-03-15\n",
      "8 2018-06-15 12:57:18.629805 finished of the date:  2018-04-11\n",
      "9 2018-06-15 12:57:45.192664 finished of the date:  2018-02-14\n",
      "10 2018-06-15 12:58:13.032214 finished of the date:  2018-02-15\n",
      "11 2018-06-15 12:58:41.920075 finished of the date:  2018-04-10\n",
      "12 2018-06-15 12:59:12.659729 finished of the date:  2018-03-14\n",
      "13 2018-06-15 12:59:45.546037 finished of the date:  2018-03-02\n",
      "14 2018-06-15 13:00:19.001828 finished of the date:  2018-04-06\n",
      "15 2018-06-15 13:00:53.825515 finished of the date:  2018-03-22\n",
      "16 2018-06-15 13:01:28.999425 finished of the date:  2018-04-26\n",
      "17 2018-06-15 13:02:05.631164 finished of the date:  2018-04-30\n",
      "18 2018-06-15 13:02:46.778228 finished of the date:  2018-02-19\n",
      "19 2018-06-15 13:03:26.033020 finished of the date:  2018-03-18\n",
      "20 2018-06-15 13:04:09.694918 finished of the date:  2018-02-23\n",
      "21 2018-06-15 13:04:51.248240 finished of the date:  2018-04-01\n",
      "22 2018-06-15 13:05:37.022641 finished of the date:  2018-02-28\n",
      "23 2018-06-15 13:06:21.917242 finished of the date:  2018-03-05\n",
      "24 2018-06-15 13:07:11.526911 finished of the date:  2018-03-13\n",
      "25 2018-06-15 13:08:00.595343 finished of the date:  2018-04-17\n",
      "26 2018-06-15 13:08:51.341664 finished of the date:  2018-02-12\n",
      "27 2018-06-15 13:09:50.103703 finished of the date:  2018-02-04\n",
      "28 2018-06-15 13:10:46.673596 finished of the date:  2018-03-29\n",
      "29 2018-06-15 13:11:41.011853 finished of the date:  2018-03-09\n",
      "30 2018-06-15 13:12:40.801659 finished of the date:  2018-02-24\n",
      "31 2018-06-15 13:13:38.299721 finished of the date:  2018-04-21\n",
      "32 2018-06-15 13:14:37.138579 finished of the date:  2018-03-25\n",
      "33 2018-06-15 13:15:37.060487 finished of the date:  2018-02-08\n",
      "34 2018-06-15 13:16:39.229589 finished of the date:  2018-02-09\n",
      "35 2018-06-15 13:17:44.683224 finished of the date:  2018-03-24\n",
      "36 2018-06-15 13:18:48.188741 finished of the date:  2018-04-20\n",
      "37 2018-06-15 13:19:58.946642 finished of the date:  2018-02-25\n",
      "38 2018-06-15 13:21:08.153586 finished of the date:  2018-03-08\n",
      "39 2018-06-15 13:22:21.648464 finished of the date:  2018-03-28\n",
      "40 2018-06-15 13:23:31.931713 finished of the date:  2018-02-05\n",
      "41 2018-06-15 13:24:45.651178 finished of the date:  2018-05-01\n",
      "42 2018-06-15 13:26:00.690169 finished of the date:  2018-02-13\n",
      "43 2018-06-15 13:27:21.161045 finished of the date:  2018-03-12\n",
      "44 2018-06-15 13:28:39.144290 finished of the date:  2018-03-04\n",
      "45 2018-06-15 13:29:59.528282 finished of the date:  2018-02-06\n",
      "46 2018-06-15 13:31:20.818437 finished of the date:  2018-05-02\n",
      "47 2018-06-15 13:32:53.900952 finished of the date:  2018-02-10\n",
      "48 2018-06-15 13:34:34.314703 finished of the date:  2018-04-15\n",
      "49 2018-06-15 13:36:04.868073 finished of the date:  2018-03-11\n",
      "50 2018-06-15 13:37:36.671227 finished of the date:  2018-03-07\n",
      "51 2018-06-15 13:39:13.058564 finished of the date:  2018-04-03\n",
      "52 2018-06-15 13:40:51.157494 finished of the date:  2018-03-27\n",
      "53 2018-06-15 13:42:28.885371 finished of the date:  2018-04-23\n",
      "54 2018-06-15 13:44:03.269445 finished of the date:  2018-03-31\n",
      "55 2018-06-15 13:45:42.198014 finished of the date:  2018-04-19\n",
      "56 2018-06-15 13:47:22.456882 finished of the date:  2018-02-26\n",
      "57 2018-06-15 13:49:01.487857 finished of the date:  2018-04-18\n",
      "58 2018-06-15 13:50:45.735607 finished of the date:  2018-03-30\n",
      "59 2018-06-15 13:52:33.268937 finished of the date:  2018-04-22\n",
      "60 2018-06-15 13:54:19.280267 finished of the date:  2018-03-26\n",
      "61 2018-06-15 13:56:06.640870 finished of the date:  2018-04-02\n",
      "62 2018-06-15 13:58:02.100904 finished of the date:  2018-03-06\n",
      "63 2018-06-15 13:59:56.644186 finished of the date:  2018-03-10\n",
      "64 2018-06-15 14:01:57.684473 finished of the date:  2018-04-14\n",
      "65 2018-06-15 14:04:07.381104 finished of the date:  2018-02-11\n",
      "66 2018-06-15 14:06:09.603976 finished of the date:  2018-05-03\n",
      "67 2018-06-15 14:08:24.139465 finished of the date:  2018-02-07\n",
      "68 2018-06-15 14:10:52.634137 finished of the date:  2018-03-21\n",
      "69 2018-06-15 14:13:09.402209 finished of the date:  2018-04-25\n",
      "70 2018-06-15 14:15:22.323995 finished of the date:  2018-02-20\n",
      "71 2018-06-15 14:17:33.209351 finished of the date:  2018-04-09\n",
      "72 2018-06-15 14:22:23.419973 finished of the date:  2018-05-04\n",
      "73 2018-06-15 14:24:44.750679 finished of the date:  2018-04-29\n",
      "74 2018-06-15 14:31:52.294507 finished of the date:  2018-02-16\n",
      "75 2018-06-15 14:39:11.162662 finished of the date:  2018-04-13\n",
      "76 2018-06-15 14:41:52.096737 finished of the date:  2018-03-17\n",
      "77 2018-06-15 14:44:40.797575 finished of the date:  2018-03-01\n",
      "78 2018-06-15 14:48:50.766613 finished of the date:  2018-04-05\n",
      "79 2018-06-15 14:56:02.372468 finished of the date:  2018-04-04\n",
      "80 2018-06-15 14:58:31.533773 finished of the date:  2018-03-16\n",
      "81 2018-06-15 15:01:00.218276 finished of the date:  2018-04-12\n",
      "82 2018-06-15 15:03:42.501383 finished of the date:  2018-02-17\n",
      "83 2018-06-15 15:06:14.968790 finished of the date:  2018-04-28\n",
      "84 2018-06-15 15:08:51.517549 finished of the date:  2018-05-05\n",
      "85 2018-06-15 15:11:27.443150 finished of the date:  2018-04-08\n",
      "86 2018-06-15 15:14:20.494128 finished of the date:  2018-02-21\n",
      "87 2018-06-15 15:17:12.312955 finished of the date:  2018-04-24\n",
      "88 2018-06-15 15:20:05.546288 finished of the date:  2018-03-20\n"
     ]
    }
   ],
   "source": [
    "k_count=0\n",
    "for file in df_files['file']:\n",
    "    date=datetime.datetime.strptime(file[len(file)-15:len(file)-5],\"%Y-%m-%d\").date()\n",
    "    response=json.load(open(file,\"r\"))\n",
    "        \n",
    "    for zip_cd in inclusion_stores['zip_cd'].unique().tolist():\n",
    "        df_rain=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"No_Rain\"]+ranked_rain['rain_desc'].tolist(),index=[0])\n",
    "        df_snow=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"No_Snow\"]+ranked_snow['snow_desc'].tolist(),index=[0])\n",
    "        df_temp=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"Temp\",\"Temp_Max\",\"Temp_Min\"],index=[0])\n",
    "        \n",
    "        df_group=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\"]+all_weather_groups,index=[0])\n",
    "        \n",
    "        \n",
    "        df_rain['zip_cd']=zip_cd\n",
    "        df_snow['zip_cd']=zip_cd\n",
    "        df_temp['zip_cd']=zip_cd\n",
    "        df_group['zip_cd']=zip_cd\n",
    "\n",
    "        df_rain['Date']=date\n",
    "        df_snow['Date']=date\n",
    "        df_temp['Date']=date\n",
    "        df_group['Date']=date\n",
    "                                       \n",
    "                                       \n",
    "                                       \n",
    "        \n",
    "        if zip_cd in list(response.keys()):\n",
    "            df_rain[\"Collected\"]=\"Recorded\"\n",
    "            df_snow[\"Collected\"]=\"Recorded\"\n",
    "            df_temp[\"Collected\"]=\"Recorded\"\n",
    "            df_group[\"Collected\"]=\"Recorded\"\n",
    "            \n",
    "            temp=response[zip_cd]['main']['temp']*9/5 - 459.67\n",
    "            temp_Max=response[zip_cd]['main']['temp_max']*9/5 - 459.67\n",
    "            temp_Min=response[zip_cd]['main']['temp_min']*9/5 - 459.67\n",
    "\n",
    "            df_temp['Temp']=temp\n",
    "            df_temp['Temp_Max']=temp_Max\n",
    "            df_temp['Temp_Min']=temp_Min\n",
    "\n",
    "            for desc_rain in ranked_rain['rain_desc'].tolist():\n",
    "                for j in range(len(response[zip_cd]['weather'])):\n",
    "                    if response[zip_cd]['weather'][j]['description']==desc_rain:\n",
    "                        df_rain[desc_rain]=1\n",
    "            for desc_snow in ranked_snow['snow_desc'].tolist():\n",
    "                for j in range(len(response[zip_cd]['weather'])):\n",
    "                    if response[zip_cd]['weather'][j]['description']==desc_snow:\n",
    "                        df_snow[desc_snow]=1\n",
    "            for w_group in all_weather_groups:\n",
    "                for j in range(len(response[zip_cd]['weather'])):\n",
    "                    if response[zip_cd]['weather'][j]['main']==w_group:\n",
    "                        df_group[w_group]=1\n",
    "        else:\n",
    "            df_rain[\"Collected\"]=\"Not_recorded\"\n",
    "            df_snow[\"Collected\"]=\"Not_recorded\"\n",
    "            df_temp[\"Collected\"]=\"Not_recorded\"\n",
    "            df_group[\"Collected\"]=\"Not_recorded\"\n",
    "            \n",
    "        df_output_rain=df_output_rain.append(df_rain)\n",
    "        df_output_snow=df_output_snow.append(df_snow)\n",
    "        df_output_temp=df_output_temp.append(df_temp) \n",
    "        df_output_group=df_output_group.append(df_group)\n",
    "    k_count=k_count+1\n",
    "    print(k_count, datetime.datetime.now(),\"finished of the date: \",date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_rain['sum']=df_output_rain[ranked_rain['rain_desc'].tolist()].sum(axis=1)\n",
    "df_output_snow['sum']=df_output_snow[ranked_snow['snow_desc'].tolist()].sum(axis=1)\n",
    "df_output_rain['No_Rain']=np.where(pd.isnull(df_output_rain['sum']),1,np.nan)\n",
    "df_output_snow['No_Snow']=np.where(pd.isnull(df_output_snow['sum']),1,np.nan)\n",
    "\n",
    "df_output_rain['No_Rain']=np.where(df_output_rain['Collected']==\"Not_recorded\",np.nan,df_output_rain['No_Rain'])\n",
    "df_output_snow['No_Snow']=np.where(df_output_snow['Collected']==\"Not_recorded\",np.nan,df_output_snow['No_Snow'])\n",
    "\n",
    "del df_output_rain['sum']\n",
    "del df_output_snow['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_temp_store=df_output_temp.copy()\n",
    "df_output_temp_store=pd.merge(df_output_temp_store,inclusion_stores,on=\"zip_cd\",how=\"left\")\n",
    "\n",
    "df_output_rain_store=df_output_rain.copy()\n",
    "df_output_rain_store=pd.merge(df_output_rain_store,inclusion_stores,on=\"zip_cd\",how=\"left\")\n",
    "\n",
    "df_output_snow_store=df_output_snow.copy()\n",
    "df_output_snow_store=pd.merge(df_output_snow_store,inclusion_stores,on=\"zip_cd\",how=\"left\")\n",
    "\n",
    "df_output_group_store=df_output_group.copy()\n",
    "df_output_group_store=pd.merge(df_output_group_store,inclusion_stores,on=\"zip_cd\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_Snow_Rain_Desc_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "# df_output_temp_store.to_excel(writer,\"Temp\",index=False)\n",
    "# df_output_rain_store.to_excel(writer,\"Rain\",index=False)\n",
    "# df_output_snow_store.to_excel(writer,\"Snow\",index=False)\n",
    "# df_output_group_store.to_excel(writer,\"Group\",index=False)\n",
    "# writer.save()\n",
    "# save later after get the avg difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average Tempreture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder=\"/home/jian/Projects/Big_Lots/Weather/Json_data/daily/api_response_20180524/\"\n",
    "json_list_all=glob.glob(folder+\"*.json\")\n",
    "\n",
    "df_files_avg_temp=pd.DataFrame({\"file\":json_list_all},index=range(len(json_list_all)))\n",
    "df_files_avg_temp['Date']=df_files_avg_temp['file'].apply(lambda x: datetime.datetime.strptime(x[len(x)-15:len(x)-5],\"%Y-%m-%d\"))\n",
    "df_files_avg_temp=df_files_avg_temp[(df_files_avg_temp['Date']>=datetime.datetime(2018,2,4)-datetime.timedelta(days=14)) & (df_files_avg_temp['Date']<=datetime.datetime(2018,5,5)+datetime.timedelta(days=14))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2018-06-15 15:20:10.428053 finished of the date:  2018-02-22\n",
      "2 2018-06-15 15:20:14.626936 finished of the date:  2018-01-24\n",
      "3 2018-06-15 15:20:18.868339 finished of the date:  2018-03-19\n",
      "4 2018-06-15 15:20:23.196550 finished of the date:  2018-02-18\n",
      "5 2018-06-15 15:20:27.637010 finished of the date:  2018-04-27\n",
      "6 2018-06-15 15:20:32.168912 finished of the date:  2018-04-07\n",
      "7 2018-06-15 15:20:36.748828 finished of the date:  2018-03-03\n",
      "8 2018-06-15 15:20:41.290279 finished of the date:  2018-03-15\n",
      "9 2018-06-15 15:20:45.952536 finished of the date:  2018-01-28\n",
      "10 2018-06-15 15:20:50.636525 finished of the date:  2018-04-11\n",
      "11 2018-06-15 15:20:55.361571 finished of the date:  2018-02-14\n",
      "12 2018-06-15 15:21:00.132767 finished of the date:  2018-05-06\n",
      "13 2018-06-15 15:21:04.952514 finished of the date:  2018-02-02\n",
      "14 2018-06-15 15:21:09.824042 finished of the date:  2018-02-03\n",
      "15 2018-06-15 15:21:14.754695 finished of the date:  2018-05-07\n",
      "16 2018-06-15 15:21:19.746058 finished of the date:  2018-05-11\n",
      "17 2018-06-15 15:21:24.788215 finished of the date:  2018-02-15\n",
      "18 2018-06-15 15:21:29.918474 finished of the date:  2018-04-10\n",
      "19 2018-06-15 15:21:35.106235 finished of the date:  2018-01-29\n",
      "20 2018-06-15 15:21:40.424619 finished of the date:  2018-03-14\n",
      "21 2018-06-15 15:21:45.799123 finished of the date:  2018-03-02\n",
      "22 2018-06-15 15:21:51.219358 finished of the date:  2018-04-06\n",
      "23 2018-06-15 15:21:56.646032 finished of the date:  2018-03-22\n",
      "24 2018-06-15 15:22:02.208782 finished of the date:  2018-04-26\n",
      "25 2018-06-15 15:22:07.852056 finished of the date:  2018-04-30\n",
      "26 2018-06-15 15:22:13.499526 finished of the date:  2018-02-19\n",
      "27 2018-06-15 15:22:19.204645 finished of the date:  2018-03-18\n",
      "28 2018-06-15 15:22:24.903207 finished of the date:  2018-01-25\n",
      "29 2018-06-15 15:22:30.608996 finished of the date:  2018-02-23\n",
      "30 2018-06-15 15:22:36.391972 finished of the date:  2018-04-01\n",
      "31 2018-06-15 15:22:42.260570 finished of the date:  2018-02-28\n",
      "32 2018-06-15 15:22:48.265462 finished of the date:  2018-03-05\n",
      "33 2018-06-15 15:22:54.197491 finished of the date:  2018-03-13\n",
      "34 2018-06-15 15:23:00.299879 finished of the date:  2018-04-17\n",
      "35 2018-06-15 15:23:06.456775 finished of the date:  2018-02-12\n",
      "36 2018-06-15 15:23:12.671614 finished of the date:  2018-05-16\n",
      "37 2018-06-15 15:23:18.942637 finished of the date:  2018-02-04\n",
      "38 2018-06-15 15:23:25.211421 finished of the date:  2018-03-29\n",
      "39 2018-06-15 15:23:31.562077 finished of the date:  2018-03-09\n",
      "40 2018-06-15 15:23:38.005630 finished of the date:  2018-02-24\n",
      "41 2018-06-15 15:23:44.500397 finished of the date:  2018-01-22\n",
      "42 2018-06-15 15:23:51.057204 finished of the date:  2018-04-21\n",
      "43 2018-06-15 15:23:57.692194 finished of the date:  2018-03-25\n",
      "44 2018-06-15 15:24:04.402848 finished of the date:  2018-02-08\n",
      "45 2018-06-15 15:24:11.160370 finished of the date:  2018-02-09\n",
      "46 2018-06-15 15:24:17.955134 finished of the date:  2018-03-24\n",
      "47 2018-06-15 15:24:24.780698 finished of the date:  2018-04-20\n",
      "48 2018-06-15 15:24:31.681965 finished of the date:  2018-01-23\n",
      "49 2018-06-15 15:24:38.533857 finished of the date:  2018-02-25\n",
      "50 2018-06-15 15:24:45.389397 finished of the date:  2018-03-08\n",
      "51 2018-06-15 15:24:52.303387 finished of the date:  2018-03-28\n",
      "52 2018-06-15 15:24:59.371471 finished of the date:  2018-02-05\n",
      "53 2018-06-15 15:25:06.552209 finished of the date:  2018-05-01\n",
      "54 2018-06-15 15:25:13.671157 finished of the date:  2018-05-17\n",
      "55 2018-06-15 15:25:20.979174 finished of the date:  2018-02-13\n",
      "56 2018-06-15 15:25:28.351216 finished of the date:  2018-03-12\n",
      "57 2018-06-15 15:25:35.834090 finished of the date:  2018-03-04\n",
      "58 2018-06-15 15:25:43.362374 finished of the date:  2018-02-06\n",
      "59 2018-06-15 15:25:51.030568 finished of the date:  2018-05-02\n",
      "60 2018-06-15 15:25:58.807610 finished of the date:  2018-05-14\n",
      "61 2018-06-15 15:26:06.670648 finished of the date:  2018-02-10\n",
      "62 2018-06-15 15:26:14.615373 finished of the date:  2018-04-15\n",
      "63 2018-06-15 15:26:22.597078 finished of the date:  2018-03-11\n",
      "64 2018-06-15 15:26:30.664276 finished of the date:  2018-03-07\n",
      "65 2018-06-15 15:26:38.839327 finished of the date:  2018-04-03\n",
      "66 2018-06-15 15:26:47.012520 finished of the date:  2018-03-27\n",
      "67 2018-06-15 15:26:55.203575 finished of the date:  2018-04-23\n",
      "68 2018-06-15 15:27:03.496918 finished of the date:  2018-05-18\n",
      "69 2018-06-15 15:27:11.854287 finished of the date:  2018-03-31\n",
      "70 2018-06-15 15:27:20.276634 finished of the date:  2018-04-19\n",
      "71 2018-06-15 15:27:28.838337 finished of the date:  2018-02-26\n",
      "72 2018-06-15 15:27:37.456214 finished of the date:  2018-01-21\n",
      "73 2018-06-15 15:27:46.162474 finished of the date:  2018-04-18\n",
      "74 2018-06-15 15:27:54.935343 finished of the date:  2018-03-30\n",
      "75 2018-06-15 15:28:03.803882 finished of the date:  2018-05-19\n",
      "76 2018-06-15 15:28:12.688080 finished of the date:  2018-04-22\n",
      "77 2018-06-15 15:28:21.773882 finished of the date:  2018-03-26\n",
      "78 2018-06-15 15:28:30.977320 finished of the date:  2018-04-02\n",
      "79 2018-06-15 15:28:40.292856 finished of the date:  2018-03-06\n",
      "80 2018-06-15 15:28:49.673189 finished of the date:  2018-03-10\n",
      "81 2018-06-15 15:28:59.179850 finished of the date:  2018-04-14\n",
      "82 2018-06-15 15:29:08.754045 finished of the date:  2018-02-11\n",
      "83 2018-06-15 15:29:18.275414 finished of the date:  2018-05-15\n",
      "84 2018-06-15 15:29:28.215905 finished of the date:  2018-05-03\n",
      "85 2018-06-15 15:29:38.223534 finished of the date:  2018-02-07\n",
      "86 2018-06-15 15:29:48.442475 finished of the date:  2018-03-21\n",
      "87 2018-06-15 15:29:58.787743 finished of the date:  2018-04-25\n",
      "88 2018-06-15 15:30:09.315589 finished of the date:  2018-05-08\n",
      "89 2018-06-15 15:30:19.911776 finished of the date:  2018-01-26\n",
      "90 2018-06-15 15:30:30.659516 finished of the date:  2018-02-20\n",
      "91 2018-06-15 15:30:41.450575 finished of the date:  2018-01-30\n",
      "92 2018-06-15 15:30:52.311703 finished of the date:  2018-04-09\n",
      "93 2018-06-15 15:31:03.311570 finished of the date:  2018-05-04\n",
      "94 2018-06-15 15:31:14.465259 finished of the date:  2018-04-29\n",
      "95 2018-06-15 15:31:25.779275 finished of the date:  2018-05-12\n",
      "96 2018-06-15 15:31:37.344169 finished of the date:  2018-02-16\n",
      "97 2018-06-15 15:31:49.042796 finished of the date:  2018-04-13\n",
      "98 2018-06-15 15:32:00.922464 finished of the date:  2018-03-17\n",
      "99 2018-06-15 15:32:12.921984 finished of the date:  2018-03-01\n",
      "100 2018-06-15 15:32:25.053002 finished of the date:  2018-04-05\n",
      "101 2018-06-15 15:32:37.363649 finished of the date:  2018-04-04\n",
      "102 2018-06-15 15:32:49.841496 finished of the date:  2018-03-16\n",
      "103 2018-06-15 15:33:02.406544 finished of the date:  2018-04-12\n",
      "104 2018-06-15 15:33:15.120498 finished of the date:  2018-02-17\n",
      "105 2018-06-15 15:33:28.015663 finished of the date:  2018-05-13\n",
      "106 2018-06-15 15:33:41.024932 finished of the date:  2018-04-28\n",
      "107 2018-06-15 15:33:54.270159 finished of the date:  2018-05-05\n",
      "108 2018-06-15 15:34:07.755534 finished of the date:  2018-02-01\n",
      "109 2018-06-15 15:34:21.463887 finished of the date:  2018-04-08\n",
      "110 2018-06-15 15:34:35.152121 finished of the date:  2018-01-31\n",
      "111 2018-06-15 15:34:49.039812 finished of the date:  2018-02-21\n",
      "112 2018-06-15 15:35:03.210461 finished of the date:  2018-01-27\n",
      "113 2018-06-15 15:35:17.558026 finished of the date:  2018-05-09\n",
      "114 2018-06-15 15:35:32.058175 finished of the date:  2018-04-24\n",
      "115 2018-06-15 15:35:46.736628 finished of the date:  2018-03-20\n"
     ]
    }
   ],
   "source": [
    "df_output_temp_avg=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"Temp\",\"Temp_Max\",\"Temp_Min\"])\n",
    "k_count=0\n",
    "for file in df_files_avg_temp['file']:\n",
    "    date=datetime.datetime.strptime(file[len(file)-15:len(file)-5],\"%Y-%m-%d\").date()\n",
    "    response=json.load(open(file,\"r\"))\n",
    "        \n",
    "    for zip_cd in inclusion_stores['zip_cd'].unique().tolist():\n",
    "        df_temp_avg=pd.DataFrame(columns=[\"zip_cd\",\"Date\",\"Collected\",\"Temp\",\"Temp_Max\",\"Temp_Min\"],index=[0])\n",
    "        df_temp_avg['zip_cd']=zip_cd\n",
    "        df_temp_avg['Date']=date\n",
    "        \n",
    "        if zip_cd in list(response.keys()):\n",
    "            df_temp_avg[\"Collected\"]=\"Recorded\"\n",
    "            \n",
    "            temp=response[zip_cd]['main']['temp']*9/5 - 459.67\n",
    "            temp_Max=response[zip_cd]['main']['temp_max']*9/5 - 459.67\n",
    "            temp_Min=response[zip_cd]['main']['temp_min']*9/5 - 459.67\n",
    "\n",
    "            df_temp_avg['Temp']=temp\n",
    "            df_temp_avg['Temp_Max']=temp_Max\n",
    "            df_temp_avg['Temp_Min']=temp_Min\n",
    "\n",
    "        else:\n",
    "            df_temp_avg[\"Collected\"]=\"Not_recorded\"\n",
    "\n",
    "        df_output_temp_avg=df_output_temp_avg.append(df_temp_avg)\n",
    "    k_count=k_count+1\n",
    "    print(k_count,datetime.datetime.now(),\"finished of the date: \",date)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_temp_avg=df_output_temp_avg.sort_values([\"zip_cd\",\"Date\"])\n",
    "\n",
    "df_output_temp_avg_agg=pd.DataFrame()\n",
    "\n",
    "for zip_cd,group in df_output_temp_avg.groupby(['zip_cd']):\n",
    "    df=group.copy()\n",
    "    df.reset_index()\n",
    "    i=0\n",
    "    for date_Q1 in df_files['Date']:\n",
    "        df_day=df[(df['Date']>=date_Q1.date()-datetime.timedelta(days=14)) & (df['Date']<=date_Q1.date()+datetime.timedelta(days=14))]\n",
    "        df_day_avg_temp=df_day['Temp'].mean()\n",
    "        df_day_avg_temp_max=df_day['Temp_Max'].mean()\n",
    "        df_day_avg_temp_min=df_day['Temp_Min'].mean()\n",
    "        df_app=pd.DataFrame({\"zip_cd\":zip_cd,\"Date\":date_Q1.date(),\"Avg_Temp\":df_day_avg_temp,\n",
    "                            \"Avg_Temp_Max\":df_day_avg_temp_max,\"Avg_Temp_Min\":df_day_avg_temp_min},index=[i])\n",
    "        i=i+1\n",
    "        \n",
    "        df_output_temp_avg_agg=df_output_temp_avg_agg.append(df_app)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_temp_store=pd.merge(df_output_temp_store,df_output_temp_avg_agg,on=[\"Date\",\"zip_cd\"],how=\"left\")\n",
    "df_output_temp_store['Diff_Temp']=df_output_temp_store['Temp']-df_output_temp_store['Avg_Temp']\n",
    "df_output_temp_store['Diff_Temp_Max']=df_output_temp_store['Temp_Max']-df_output_temp_store['Avg_Temp_Max']\n",
    "df_output_temp_store['Diff_Temp_Min']=df_output_temp_store['Temp_Min']-df_output_temp_store['Avg_Temp_Min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_temp_store.to_excel(writer,\"Temp\",index=False)\n",
    "df_output_rain_store.to_excel(writer,\"Rain\",index=False)\n",
    "df_output_snow_store.to_excel(writer,\"Snow\",index=False)\n",
    "df_output_group_store.to_excel(writer,\"Group\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the Severity Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "rain_snow_weight=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Q1_inclusion_store_all_weather_type_ranked.xlsx\",sheetname=\"ranked rain snow\")\n",
    "\n",
    "rain_weight=rain_snow_weight.iloc[:,[4,6]]\n",
    "rain_weight.columns=[['rain_desc','weight']]\n",
    "rain_weight['weight']=rain_weight['weight'].astype(int)\n",
    "rain_weight=rain_weight.sort_values(\"weight\")\n",
    "\n",
    "snow_weight=rain_snow_weight.iloc[:,[0,2]]\n",
    "snow_weight.columns=[['snow_desc','weight']]\n",
    "snow_weight=snow_weight[~pd.isnull(snow_weight['snow_desc'])]\n",
    "snow_weight['weight']=snow_weight['weight'].astype(int)\n",
    "snow_weight=snow_weight.sort_values(\"weight\")\n",
    "\n",
    "group_weight=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Q1_inclusion_store_all_weather_type_ranked.xlsx\",sheetname=\"all_weather_group_list\")\n",
    "group_weight['Severity']=group_weight['Severity'].astype(int)\n",
    "group_weight=group_weight[['all_type_group','Severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_weight_dict=group_weight[['all_type_group', 'Severity']].set_index('all_type_group').T.to_dict()\n",
    "rain_weight_dict=rain_weight[['rain_desc', 'weight']].set_index('rain_desc').T.to_dict()\n",
    "snow_weight_dict=snow_weight[['snow_desc', 'weight']].set_index('snow_desc').T.to_dict()\n",
    "rain_weight_dict.update({\"No_Rain\":{\"weight\":0}})\n",
    "snow_weight_dict.update({\"No_Snow\":{\"weight\":0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in list(group_weight_dict.keys()):\n",
    "    df_output_group_store[col]=df_output_group_store[col]*group_weight_dict[col]['Severity']\n",
    "for col in list(rain_weight_dict.keys()):\n",
    "    df_output_rain_store[col]=df_output_rain_store[col]*rain_weight_dict[col]['weight']\n",
    "for col in list(snow_weight_dict.keys()):\n",
    "    df_output_snow_store[col]=df_output_snow_store[col]*snow_weight_dict[col]['weight']\n",
    "    \n",
    "df_output_group_store['Severity']=df_output_group_store[list(group_weight_dict.keys())].max(axis=1)\n",
    "df_output_rain_store['Severity']=df_output_rain_store[list(rain_weight_dict.keys())].max(axis=1)\n",
    "df_output_snow_store['Severity']=df_output_snow_store[list(snow_weight_dict.keys())].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_output_rain_store[\\'Severity\\'][df_output_rain_store[\\'Collected\\']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\\ndf_output_snow_store[\\'Severity\\'][df_output_snow_store[\\'Collected\\']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\\ndf_output_group_store[\\'Severity\\'][df_output_group_store[\\'Collected\\']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_output_rain_store['Severity'][df_output_rain_store['Collected']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\n",
    "df_output_snow_store['Severity'][df_output_snow_store['Collected']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\n",
    "df_output_group_store['Severity'][df_output_group_store['Collected']==\"Not_recorded\"]=\"Not_recorded\" or np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_Snow_Rain_Severity_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "df_output_temp_store.to_excel(writer,\"Temp\",index=False)\n",
    "df_output_rain_store.to_excel(writer,\"Rain\",index=False)\n",
    "df_output_snow_store.to_excel(writer,\"Snow\",index=False)\n",
    "df_output_group_store.to_excel(writer,\"Group\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>Date</th>\n",
       "      <th>Collected</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Temp_Max</th>\n",
       "      <th>Temp_Min</th>\n",
       "      <th>location_id</th>\n",
       "      <th>Avg_Temp</th>\n",
       "      <th>Avg_Temp_Max</th>\n",
       "      <th>Avg_Temp_Min</th>\n",
       "      <th>Diff_Temp</th>\n",
       "      <th>Diff_Temp_Max</th>\n",
       "      <th>Diff_Temp_Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43232</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>46.004</td>\n",
       "      <td>48.2</td>\n",
       "      <td>44.6</td>\n",
       "      <td>1</td>\n",
       "      <td>44.580071</td>\n",
       "      <td>46.592857</td>\n",
       "      <td>42.414286</td>\n",
       "      <td>1.423929</td>\n",
       "      <td>1.607143</td>\n",
       "      <td>2.185714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30906</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>79.502</td>\n",
       "      <td>80.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3</td>\n",
       "      <td>66.513071</td>\n",
       "      <td>69.221429</td>\n",
       "      <td>63.757143</td>\n",
       "      <td>12.988929</td>\n",
       "      <td>11.378571</td>\n",
       "      <td>13.242857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zip_cd        Date Collected    Temp  Temp_Max  Temp_Min location_id  \\\n",
       "0  43232  2018-02-22  Recorded  46.004      48.2      44.6           1   \n",
       "1  30906  2018-02-22  Recorded  79.502      80.6      77.0           3   \n",
       "\n",
       "    Avg_Temp  Avg_Temp_Max  Avg_Temp_Min  Diff_Temp  Diff_Temp_Max  \\\n",
       "0  44.580071     46.592857     42.414286   1.423929       1.607143   \n",
       "1  66.513071     69.221429     63.757143  12.988929      11.378571   \n",
       "\n",
       "   Diff_Temp_Min  \n",
       "0       2.185714  \n",
       "1      13.242857  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_temp_store.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted by Daily Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abandoned to use the other method: consistant weight by weekday over the Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraffic_folder=\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Daily Traffic/\"\\ntraffic_files=glob.glob(traffic_folder+\"*.txt\")\\n\\nfile=traffic_files[0]\\ntraffic_1=pd.read_csv(file,sep=\"|\")\\ntraffic_Q1=pd.DataFrame()\\nfor file in traffic_files:\\n    df=pd.read_csv(file,sep=\"|\",dtype=str)\\n    df[\\'week_end_dt\\']=df[\\'week_end_dt\\'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\\n    df[\\'weekly_traffic\\']=0\\n    col_date_list=[]\\n    for i in range(1,8):\\n        day_num=str(i)\\n        col=[x for x in df.columns.tolist() if day_num in x][0]\\n        df[col]=df[col].astype(float)\\n        df[\\'weekly_traffic\\']=df[\\'weekly_traffic\\']+df[col]\\n        df_app=df[[\"location_id\",\"week_end_dt\"]+[col]]\\n        df_app.columns=[\"location_id\",\"week_end_dt\",\"Traffic\"]\\n        df_app[\\'Date\\']=df_app[\\'week_end_dt\\'].unique()[0]-datetime.timedelta(days=(7-i))\\n        traffic_Q1=traffic_Q1.append(df_app)     \\n\\ntraffic_Q1_0=traffic_Q1[traffic_Q1[\\'Traffic\\']==0]\\nweight_0=pd.DataFrame()\\nfor store,group in traffic_Q1_0.groupby(\\'location_id\\'):\\n    exclude_week=group[\\'week_end_dt\\'].unique().tolist()\\n    df=traffic_Q1[(traffic_Q1[\\'location_id\\']==store) & (~traffic_Q1[\\'week_end_dt\\'].isin(exclude_week))]\\n    df[\\'weekday\\']=df[\\'Date\\'].apply(lambda x: x.weekday())\\n    df=df.groupby([\"location_id\",\"weekday\"])[\\'Traffic\\'].mean().to_frame().reset_index()\\n    sum_traffic=df[\\'Traffic\\'].sum()\\n    df[\\'Weight_by_Traffic\\']=df[\\'Traffic\\'].apply(lambda x: x/sum_traffic)\\n    weight_0=weight_0.append(df)\\ndel weight_0[\\'Traffic\\']\\n\\n\\ndef weighted_severity(df):\\n    result=pd.merge(df,traffic_Q1,on=[\"location_id\",\"Date\"],how=\"left\")\\n    result=pd.merge(result,recorded_traffic_weight,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\\n    result[\\'week_end_date_str\\']=result[\\'week_end_dt\\'].astype(str)\\n    result[\\'key\\']=result[\\'location_id\\']+\"|\"+result[\\'week_end_date_str\\']\\n\\n    result_0=result[result[\\'Traffic\\']==0]\\n\\n    \\n    \\n    result_1=result[~result[\\'key\\'].isin(result_0[\\'key\\'])]\\n    result_1[\\'Weight_by_Traffic\\']=result_1[\\'Traffic\\']/result_1[\\'weekly_traffic\\']\\n    result_1_col=result_1.columns.tolist()\\n    \\n    df_0=result[result[\\'key\\'].isin(result_0[\\'key\\'])]\\n    df_0[\\'weekday\\']=df_0[\\'Date\\'].apply(lambda x:x.weekday())\\n    df_0=pd.merge(df_0,weight_0,on=[\"location_id\",\"weekday\"],how=\"left\")\\n    df_0=df_0[result_1_col]\\n    result_1[\\'Weight_by_Traffic\\']=result_1[\\'Traffic\\']/result_1[\\'weekly_traffic\\']\\n    result=result_1.append(df_0)\\n    \\n    result[\\'Weighted_Severity\\']=result[\\'Severity\\']*result[\\'Weight_by_Traffic\\']\\n    \\n    del result[\\'week_end_date_str\\']\\n    return result\\ndf_output_rain_store_weight=weighted_severity(df_output_rain_store)\\ndf_output_snow_store_weight=weighted_severity(df_output_snow_store)\\ndf_output_group_store_weight=weighted_severity(df_output_group_store)\\n\\n\\ndf_output_temp_store_weight=df_output_temp_store.copy()\\ndf_output_temp_store_weight[\\'Date_str\\']=df_output_temp_store_weight[\\'Date\\'].astype(str)\\ndf_output_temp_store_weight[\\'key\\']=df_output_temp_store_weight[\\'location_id\\']+\"|\"+df_output_temp_store_weight[\\'Date_str\\']\\nkey_weight=df_output_snow_store_weight[[\\'Date\\',\\'location_id\\',\"Traffic\",\\'weekly_traffic\\',\\'Weight_by_Traffic\\']]\\nkey_weight[\\'Date_str\\']=key_weight[\\'Date\\'].astype(str)\\nkey_weight[\\'key\\']=key_weight[\\'location_id\\']+\"|\"+key_weight[\\'Date_str\\']\\ndel key_weight[\\'Date_str\\']\\ndel key_weight[\\'Date\\']\\ndel key_weight[\\'location_id\\']\\ndf_output_temp_store_weight=pd.merge(df_output_temp_store_weight,key_weight,on=\"key\",how=\"left\")\\ndel df_output_temp_store_weight[\\'key\\']\\ndf_output_temp_store_weight[\\'Weighted_Diff_Temp\\']=df_output_temp_store_weight[\\'Diff_Temp\\']*df_output_temp_store_weight[\\'Weight_by_Traffic\\']\\ndf_output_temp_store_weight[\\'Weighted_Diff_Temp_Max\\']=df_output_temp_store_weight[\\'Diff_Temp_Max\\']*df_output_temp_store_weight[\\'Weight_by_Traffic\\']\\ndf_output_temp_store_weight[\\'Weighted_Diff_Temp_Min\\']=df_output_temp_store_weight[\\'Diff_Temp_Min\\']*df_output_temp_store_weight[\\'Weight_by_Traffic\\']\\n\\n\\nweighted_writer=pd.ExcelWriter(writer_folder+\"Q1_Weighted_Snow_Rain_Desc_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\\ndf_output_temp_store_weight.to_excel(weighted_writer,\"Weighted_Temp\",index=False)\\ndf_output_rain_store_weight.to_excel(weighted_writer,\"Weighted_Rain\",index=False)\\ndf_output_snow_store_weight.to_excel(weighted_writer,\"Weighted_Snow\",index=False)\\ndf_output_group_store_weight.to_excel(weighted_writer,\"Weighted_Group\",index=False)\\nweighted_writer.save()\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "traffic_folder=\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Daily Traffic/\"\n",
    "traffic_files=glob.glob(traffic_folder+\"*.txt\")\n",
    "\n",
    "file=traffic_files[0]\n",
    "traffic_1=pd.read_csv(file,sep=\"|\")\n",
    "traffic_Q1=pd.DataFrame()\n",
    "for file in traffic_files:\n",
    "    df=pd.read_csv(file,sep=\"|\",dtype=str)\n",
    "    df['week_end_dt']=df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "    df['weekly_traffic']=0\n",
    "    col_date_list=[]\n",
    "    for i in range(1,8):\n",
    "        day_num=str(i)\n",
    "        col=[x for x in df.columns.tolist() if day_num in x][0]\n",
    "        df[col]=df[col].astype(float)\n",
    "        df['weekly_traffic']=df['weekly_traffic']+df[col]\n",
    "        df_app=df[[\"location_id\",\"week_end_dt\"]+[col]]\n",
    "        df_app.columns=[\"location_id\",\"week_end_dt\",\"Traffic\"]\n",
    "        df_app['Date']=df_app['week_end_dt'].unique()[0]-datetime.timedelta(days=(7-i))\n",
    "        traffic_Q1=traffic_Q1.append(df_app)     \n",
    "\n",
    "traffic_Q1_0=traffic_Q1[traffic_Q1['Traffic']==0]\n",
    "weight_0=pd.DataFrame()\n",
    "for store,group in traffic_Q1_0.groupby('location_id'):\n",
    "    exclude_week=group['week_end_dt'].unique().tolist()\n",
    "    df=traffic_Q1[(traffic_Q1['location_id']==store) & (~traffic_Q1['week_end_dt'].isin(exclude_week))]\n",
    "    df['weekday']=df['Date'].apply(lambda x: x.weekday())\n",
    "    df=df.groupby([\"location_id\",\"weekday\"])['Traffic'].mean().to_frame().reset_index()\n",
    "    sum_traffic=df['Traffic'].sum()\n",
    "    df['Weight_by_Traffic']=df['Traffic'].apply(lambda x: x/sum_traffic)\n",
    "    weight_0=weight_0.append(df)\n",
    "del weight_0['Traffic']\n",
    "\n",
    "\n",
    "def weighted_severity(df):\n",
    "    result=pd.merge(df,traffic_Q1,on=[\"location_id\",\"Date\"],how=\"left\")\n",
    "    result=pd.merge(result,recorded_traffic_weight,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "    result['week_end_date_str']=result['week_end_dt'].astype(str)\n",
    "    result['key']=result['location_id']+\"|\"+result['week_end_date_str']\n",
    "\n",
    "    result_0=result[result['Traffic']==0]\n",
    "\n",
    "    \n",
    "    \n",
    "    result_1=result[~result['key'].isin(result_0['key'])]\n",
    "    result_1['Weight_by_Traffic']=result_1['Traffic']/result_1['weekly_traffic']\n",
    "    result_1_col=result_1.columns.tolist()\n",
    "    \n",
    "    df_0=result[result['key'].isin(result_0['key'])]\n",
    "    df_0['weekday']=df_0['Date'].apply(lambda x:x.weekday())\n",
    "    df_0=pd.merge(df_0,weight_0,on=[\"location_id\",\"weekday\"],how=\"left\")\n",
    "    df_0=df_0[result_1_col]\n",
    "    result_1['Weight_by_Traffic']=result_1['Traffic']/result_1['weekly_traffic']\n",
    "    result=result_1.append(df_0)\n",
    "    \n",
    "    result['Weighted_Severity']=result['Severity']*result['Weight_by_Traffic']\n",
    "    \n",
    "    del result['week_end_date_str']\n",
    "    return result\n",
    "df_output_rain_store_weight=weighted_severity(df_output_rain_store)\n",
    "df_output_snow_store_weight=weighted_severity(df_output_snow_store)\n",
    "df_output_group_store_weight=weighted_severity(df_output_group_store)\n",
    "\n",
    "\n",
    "df_output_temp_store_weight=df_output_temp_store.copy()\n",
    "df_output_temp_store_weight['Date_str']=df_output_temp_store_weight['Date'].astype(str)\n",
    "df_output_temp_store_weight['key']=df_output_temp_store_weight['location_id']+\"|\"+df_output_temp_store_weight['Date_str']\n",
    "key_weight=df_output_snow_store_weight[['Date','location_id',\"Traffic\",'weekly_traffic','Weight_by_Traffic']]\n",
    "key_weight['Date_str']=key_weight['Date'].astype(str)\n",
    "key_weight['key']=key_weight['location_id']+\"|\"+key_weight['Date_str']\n",
    "del key_weight['Date_str']\n",
    "del key_weight['Date']\n",
    "del key_weight['location_id']\n",
    "df_output_temp_store_weight=pd.merge(df_output_temp_store_weight,key_weight,on=\"key\",how=\"left\")\n",
    "del df_output_temp_store_weight['key']\n",
    "df_output_temp_store_weight['Weighted_Diff_Temp']=df_output_temp_store_weight['Diff_Temp']*df_output_temp_store_weight['Weight_by_Traffic']\n",
    "df_output_temp_store_weight['Weighted_Diff_Temp_Max']=df_output_temp_store_weight['Diff_Temp_Max']*df_output_temp_store_weight['Weight_by_Traffic']\n",
    "df_output_temp_store_weight['Weighted_Diff_Temp_Min']=df_output_temp_store_weight['Diff_Temp_Min']*df_output_temp_store_weight['Weight_by_Traffic']\n",
    "\n",
    "\n",
    "weighted_writer=pd.ExcelWriter(writer_folder+\"Q1_Weighted_Snow_Rain_Desc_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "df_output_temp_store_weight.to_excel(weighted_writer,\"Weighted_Temp\",index=False)\n",
    "df_output_rain_store_weight.to_excel(weighted_writer,\"Weighted_Rain\",index=False)\n",
    "df_output_snow_store_weight.to_excel(weighted_writer,\"Weighted_Snow\",index=False)\n",
    "df_output_group_store_weight.to_excel(weighted_writer,\"Weighted_Group\",index=False)\n",
    "weighted_writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q1 Weekday weight applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inclusion_YoY=pd.read_excel(\"/home/jian/Projects/Big_Lots/Q1_Post/BL_Sales YoY_JL_20180514.xlsx\",sheetname=\"Inclusion Stores\",skiprows=1,dtype=str)\n",
    "inclusion_YoY=inclusion_YoY.iloc[:,0:29]\n",
    "\n",
    "for col in inclusion_YoY.columns.tolist()[3:]:\n",
    "    inclusion_YoY[col]=inclusion_YoY[col].astype(float)\n",
    "inclusion_YoY['Sales_2017']=inclusion_YoY[inclusion_YoY.columns.tolist()[3:16]].sum(axis=1)\n",
    "inclusion_YoY['Sales_2018']=inclusion_YoY[inclusion_YoY.columns.tolist()[16:29]].sum(axis=1)\n",
    "inclusion_YoY['YoY']=(inclusion_YoY['Sales_2018']-inclusion_YoY['Sales_2017'])/inclusion_YoY['Sales_2017']\n",
    "inclusion_YoY=inclusion_YoY[['location_id','DMA','Q1 Campaign Type','Sales_2017','Sales_2018','YoY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "traffic_folder=\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Daily Traffic/\"\n",
    "traffic_files=glob.glob(traffic_folder+\"*.txt\")\n",
    "\n",
    "traffic_Q1=pd.DataFrame()\n",
    "\n",
    "for file in traffic_files:\n",
    "    df=pd.read_csv(file,sep=\"|\",dtype=str)\n",
    "    df['week_end_dt']=df['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "    df['weekly_traffic']=0\n",
    "    col_date_list=[]\n",
    "    for i in range(1,8):\n",
    "        day_num=str(i)\n",
    "        col=[x for x in df.columns.tolist() if day_num in x][0]\n",
    "        df[col]=df[col].astype(float)\n",
    "        df['weekly_traffic']=df['weekly_traffic']+df[col]\n",
    "        df_app=df[[\"location_id\",\"week_end_dt\"]+[col]]\n",
    "        df_app['weekday']=col\n",
    "        df_app.columns=[\"location_id\",\"week_end_dt\",\"Traffic\",\"weekday\"]\n",
    "        df_app['Date']=df_app['week_end_dt'].unique()[0]-datetime.timedelta(days=(7-i))\n",
    "        traffic_Q1=traffic_Q1.append(df_app)\n",
    "traffic_Q1=traffic_Q1[traffic_Q1['Traffic']>0]\n",
    "df_date_weekday=traffic_Q1[['Date','weekday']].drop_duplicates()\n",
    "traffic_Q1=traffic_Q1[traffic_Q1['location_id'].isin(inclusion_YoY['location_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weight_weekday_store=traffic_Q1.groupby(['location_id','weekday'])['Traffic'].mean().to_frame().reset_index()\n",
    "df_weight_store=df_weight_weekday_store.groupby(['location_id'])['Traffic'].sum().to_frame().reset_index()\n",
    "df_weight_store=df_weight_store.rename(columns={\"Traffic\":\"weekly_traffic\"})\n",
    "df_weight_weekday_store=pd.merge(df_weight_weekday_store,df_weight_store,on=\"location_id\",how=\"left\")\n",
    "df_weight_weekday_store['weight']=df_weight_weekday_store['Traffic']/df_weight_weekday_store['weekly_traffic']\n",
    "df_weight_weekday_store['weekday_py']=np.where(df_weight_weekday_store['weekday']==\"traffic_day_1\",6,\n",
    "                                               df_weight_weekday_store['weekday'].apply(lambda x: int(x[len(x)-1:])-2)                                              \n",
    "                                              )\n",
    "overall_weekday=traffic_Q1.groupby(['weekday'])['Traffic'].mean().to_frame().reset_index()\n",
    "overall_weekday['weight']=overall_weekday['Traffic'].apply(lambda x: x/overall_weekday['Traffic'].sum())\n",
    "\n",
    "\n",
    "overall_weekday['weekday_py']=np.where(overall_weekday['weekday']==\"traffic_day_1\",6,\n",
    "                                               overall_weekday['weekday'].apply(lambda x: int(x[len(x)-1:])-2)                                              \n",
    "                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weather_desc_by_day_table(df_input,desc_list):\n",
    "    df_snow_desc_long=pd.DataFrame()\n",
    "    for col in desc_list:\n",
    "        df=df_input[['location_id','zip_cd','Date','Collected','Severity']+[col]]\n",
    "        df=df.rename(columns={col:\"nan_index\"})\n",
    "        df['desc']=col\n",
    "        df_snow_desc_long=df_snow_desc_long.append(df)\n",
    "    df_snow_desc_long_Not_Record=df_snow_desc_long[df_snow_desc_long['Collected']==\"Not_recorded\"][['location_id','zip_cd','Date']].drop_duplicates()\n",
    "    df_snow_desc_long_no_nan=df_snow_desc_long[~pd.isnull(df_snow_desc_long['nan_index'])]\n",
    "\n",
    "    df_snow_desc_long_Not_Record['key']=df_snow_desc_long_Not_Record['location_id']+\"|\"+df_snow_desc_long_Not_Record['Date'].apply(lambda x: str(x-datetime.timedelta(days=1)))\n",
    "    df_snow_desc_long_Not_Record_Key=df_snow_desc_long_no_nan.copy()\n",
    "    df_snow_desc_long_Not_Record_Key['key']=df_snow_desc_long_Not_Record_Key['location_id']+\"|\"+df_snow_desc_long_Not_Record_Key['Date'].apply(lambda x: str(x))\n",
    "    df_snow_desc_long_Not_Record_Fill=df_snow_desc_long_Not_Record_Key[df_snow_desc_long_Not_Record_Key['key'].isin(df_snow_desc_long_Not_Record['key'])]\n",
    "    del df_snow_desc_long_Not_Record_Fill['key']\n",
    "    df_snow_desc_long_Not_Record_Fill['Date']=df_snow_desc_long_Not_Record_Fill['Date'].apply(lambda x: x+datetime.timedelta(days=1))\n",
    "\n",
    "    df_snow_desc_long=df_snow_desc_long_no_nan.append(df_snow_desc_long_Not_Record_Fill)\n",
    "    df_snow_desc_long['weekday_py']=df_snow_desc_long['Date'].apply(lambda x: x.weekday())\n",
    "    \n",
    "    df_snow_desc_long_in=pd.merge(df_snow_desc_long,df_weight_weekday_store[['location_id','weekday_py','weight']],\n",
    "                                  on=['location_id','weekday_py'],how=\"left\")\n",
    "    df_snow_desc_long_out=df_snow_desc_long_in[pd.isnull(df_snow_desc_long_in['weight'])]\n",
    "    # 4012 only\n",
    "    df_snow_desc_long_in=df_snow_desc_long_in[~pd.isnull(df_snow_desc_long_in['weight'])]\n",
    "    del df_snow_desc_long_out['weight']\n",
    "    df_snow_desc_long_out=pd.merge(df_snow_desc_long_out,overall_weekday[['weekday_py','weight']],\n",
    "                                  on=['weekday_py'],how=\"left\")\n",
    "    \n",
    "    result=df_snow_desc_long_in.append(df_snow_desc_long_out)\n",
    "    result['weighted_severity']=result['Severity']*result['weight']\n",
    "    del result['nan_index']\n",
    "    del result['weekday_py']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "desc_list_snow=['No_Snow']+ranked_snow['snow_desc'].tolist()\n",
    "df_snow_desc_long=weather_desc_by_day_table(df_output_snow_store,desc_list_snow)\n",
    "\n",
    "desc_list_rain=['No_Rain']+ranked_rain['rain_desc'].tolist()\n",
    "df_rain_desc_long=weather_desc_by_day_table(df_output_rain_store,desc_list_rain)\n",
    "\n",
    "desc_list_group=all_weather_groups\n",
    "df_weather_group_long=weather_desc_by_day_table(df_output_group_store,desc_list_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>Date</th>\n",
       "      <th>Collected</th>\n",
       "      <th>Severity</th>\n",
       "      <th>desc</th>\n",
       "      <th>weight</th>\n",
       "      <th>weighted_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>14043</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Snow</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>819</td>\n",
       "      <td>14424</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Snow</td>\n",
       "      <td>0.124264</td>\n",
       "      <td>0.621318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id zip_cd        Date Collected  Severity  desc    weight  \\\n",
       "0         391  14043  2018-02-22  Recorded       5.0  Snow  0.125000   \n",
       "1         819  14424  2018-02-22  Recorded       5.0  Snow  0.124264   \n",
       "\n",
       "   weighted_severity  \n",
       "0           0.625000  \n",
       "1           0.621318  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_group_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weather_group_rank=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/weather_desc_drop_dup_rank.xlsx\",sheetname=\"weather_group_rank\")\n",
    "rain_desc_rank=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/weather_desc_drop_dup_rank.xlsx\",sheetname=\"rain_desc_rank\")\n",
    "snow_desc_rank=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/weather_desc_drop_dup_rank.xlsx\",sheetname=\"snow_desc_rank\")\n",
    "\n",
    "df_rain_desc_long_dedup=pd.merge(df_rain_desc_long,rain_desc_rank,on=\"desc\",how='left')\n",
    "df_snow_desc_long_dedup=pd.merge(df_snow_desc_long,snow_desc_rank,on=\"desc\",how='left')\n",
    "df_weather_group_long_dedup=pd.merge(df_weather_group_long,weather_group_rank,on=\"desc\",how='left')\n",
    "\n",
    "df_rain_desc_long_dedup=df_rain_desc_long_dedup.sort_values('rank',ascending=False).drop_duplicates(['location_id','Date'])\n",
    "df_snow_desc_long_dedup=df_snow_desc_long_dedup.sort_values('rank',ascending=False).drop_duplicates(['location_id','Date'])\n",
    "df_weather_group_long_dedup=df_weather_group_long_dedup.sort_values('rank',ascending=False).drop_duplicates(['location_id','Date'])\n",
    "\n",
    "df_rain_desc_long_dedup.to_csv(writer_folder+\"df_rain_desc_long_by_day.csv\",index=False)\n",
    "df_snow_desc_long_dedup.to_csv(writer_folder+\"df_snow_desc_long_by_day.csv\",index=False)\n",
    "df_weather_group_long_dedup.to_csv(writer_folder+\"df_weather_group_long_by_day.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather_group_long_dedup['zip_cd']=df_weather_group_long_dedup['zip_cd'].apply(lambda x: x.zfill(5))\n",
    "df_rain_desc_long_dedup['zip_cd']=df_rain_desc_long_dedup['zip_cd'].apply(lambda x: x.zfill(5))\n",
    "df_snow_desc_long_dedup['zip_cd']=df_snow_desc_long_dedup['zip_cd'].apply(lambda x: x.zfill(5))\n",
    "\n",
    "\n",
    "df_weather_group_long_dedup['weight']=df_weather_group_long_dedup['weight'].astype(float)\n",
    "df_rain_desc_long_dedup['weight']=df_rain_desc_long_dedup['weight'].astype(float)\n",
    "df_snow_desc_long_dedup['weight']=df_snow_desc_long_dedup['weight'].astype(float)\n",
    "\n",
    "df_weather_group_long_dedup['weighted_severity']=df_weather_group_long_dedup['weighted_severity'].astype(float)\n",
    "df_rain_desc_long_dedup['weighted_severity']=df_rain_desc_long_dedup['weighted_severity'].astype(float)\n",
    "df_snow_desc_long_dedup['weighted_severity']=df_snow_desc_long_dedup['weighted_severity'].astype(float)\n",
    "\n",
    "df_date_week_end=traffic_Q1[['week_end_dt','Date']].drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inclusion_YoY_long=pd.read_excel(\"/home/jian/Projects/Big_Lots/Q1_Post/BL_Sales YoY_JL_20180514.xlsx\",sheetname=\"Inclusion Stores\",skiprows=1,dtype=str)\n",
    "inclusion_YoY_long=inclusion_YoY_long.iloc[:,0:29]\n",
    "inclusion_long_sales=pd.DataFrame()\n",
    "for col in inclusion_YoY_long.columns.tolist()[3:]:\n",
    "    inclusion_YoY_long[col]=inclusion_YoY_long[col].astype(float)\n",
    "    df_long=inclusion_YoY_long[['location_id','DMA','Q1 Campaign Type']+[col]]\n",
    "    df_long=df_long.rename(columns={col:\"sales\"})\n",
    "    df_long['week_end_dt']=datetime.datetime.strptime(col,\"%Y-%m-%d\").date()\n",
    "    inclusion_long_sales=inclusion_long_sales.append(df_long)\n",
    "inclusion_long_sales_2017=inclusion_long_sales[inclusion_long_sales['week_end_dt']<=datetime.date(2017,12,1)]\n",
    "inclusion_long_sales_2017=inclusion_long_sales_2017.rename(columns={\"sales\":\"sales_weekly_2017\"})\n",
    "inclusion_long_sales_2017['week_end_dt']=inclusion_long_sales_2017['week_end_dt'].apply(lambda x: x+datetime.timedelta(days=52*7))\n",
    "inclusion_long_sales_2018=inclusion_long_sales[inclusion_long_sales['week_end_dt']>=datetime.date(2017,12,1)]\n",
    "inclusion_long_sales_2018=inclusion_long_sales_2018.rename(columns={\"sales\":\"sales_weekly_2018\"})\n",
    "\n",
    "inclusion_long_sales=pd.merge(inclusion_long_sales_2017,inclusion_long_sales_2018,on=[\"location_id\",'DMA','Q1 Campaign Type','week_end_dt'],how=\"left\")\n",
    "\n",
    "inclusion_long_sales=inclusion_long_sales[['location_id','DMA','Q1 Campaign Type','week_end_dt','sales_weekly_2017','sales_weekly_2018']]\n",
    "inclusion_long_sales=pd.merge(df_date_week_end,inclusion_long_sales,on=\"week_end_dt\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rain_desc_long_dedup=pd.merge(df_rain_desc_long_dedup,inclusion_long_sales,on=[\"Date\",\"location_id\"],how=\"left\")\n",
    "df_snow_desc_long_dedup=pd.merge(df_snow_desc_long_dedup,inclusion_long_sales,on=[\"Date\",\"location_id\"],how=\"left\")\n",
    "df_weather_group_long_dedup=pd.merge(df_weather_group_long_dedup,inclusion_long_sales,on=[\"Date\",\"location_id\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_week_count=df_weather_group_long_dedup.groupby(['location_id','week_end_dt'])['Date'].count().to_frame().reset_index()\n",
    "store_week_count=store_week_count.rename(columns={\"Date\":\"Recorded_Days\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_group_P_95_peak_days=[datetime.date(2018,2,10),datetime.date(2018,2,24),datetime.date(2018,3,24),\n",
    "                          datetime.date(2018,4,7),datetime.date(2018,4,8)]\n",
    "weather_group_P_5_peak_days=[datetime.date(2018,3,3),datetime.date(2018,3,24),\n",
    "                          datetime.date(2018,4,7),datetime.date(2018,4,8),datetime.date(2018,4,21)]\n",
    "rain_desc_P_95_peak_days=[datetime.date(2018,2,10),datetime.date(2018,2,24),datetime.date(2018,3,24),\n",
    "                          datetime.date(2018,4,7),datetime.date(2018,4,21)]\n",
    "rain_desc_P_5_peak_days=[datetime.date(2018,3,3),datetime.date(2018,3,31),datetime.date(2018,4,7),\n",
    "                          datetime.date(2018,4,8),datetime.date(2018,4,21)]\n",
    "snow_desc_P_95_peak_days=[datetime.date(2018,2,17),datetime.date(2018,2,24),datetime.date(2018,3,24),\n",
    "                          datetime.date(2018,4,7),datetime.date(2018,4,8)]\n",
    "snow_desc_P_5_peak_days=[datetime.date(2018,3,10),datetime.date(2018,4,7),datetime.date(2018,4,8),\n",
    "                          datetime.date(2018,4,20),datetime.date(2018,4,21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rain_desc_long_dedup=pd.merge(df_rain_desc_long_dedup,store_week_count,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "df_snow_desc_long_dedup=pd.merge(df_snow_desc_long_dedup,store_week_count,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "df_weather_group_long_dedup=pd.merge(df_weather_group_long_dedup,store_week_count,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "\n",
    "df_rain_desc_long_dedup['weighted_2_severity']=df_rain_desc_long_dedup['weighted_severity']/df_rain_desc_long_dedup['Recorded_Days']*7\n",
    "df_snow_desc_long_dedup['weighted_2_severity']=df_snow_desc_long_dedup['weighted_severity']/df_snow_desc_long_dedup['Recorded_Days']*7\n",
    "df_weather_group_long_dedup['weighted_2_severity']=df_weather_group_long_dedup['weighted_severity']/df_weather_group_long_dedup['Recorded_Days']*7\n",
    "\n",
    "df_rain_desc_long_dedup['weighted_2_rank']=df_rain_desc_long_dedup['rank']/df_rain_desc_long_dedup['Recorded_Days']*7*df_rain_desc_long_dedup['weight']\n",
    "df_snow_desc_long_dedup['weighted_2_rank']=df_snow_desc_long_dedup['rank']/df_snow_desc_long_dedup['Recorded_Days']*7*df_snow_desc_long_dedup['weight']\n",
    "df_weather_group_long_dedup['weighted_2_rank']=df_weather_group_long_dedup['rank']/df_weather_group_long_dedup['Recorded_Days']*7*df_weather_group_long_dedup['weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>Date</th>\n",
       "      <th>Collected</th>\n",
       "      <th>Severity</th>\n",
       "      <th>desc</th>\n",
       "      <th>weight</th>\n",
       "      <th>weighted_severity</th>\n",
       "      <th>rank</th>\n",
       "      <th>week_end_dt</th>\n",
       "      <th>DMA</th>\n",
       "      <th>Q1 Campaign Type</th>\n",
       "      <th>sales_weekly_2017</th>\n",
       "      <th>sales_weekly_2018</th>\n",
       "      <th>Recorded_Days</th>\n",
       "      <th>weighted_2_severity</th>\n",
       "      <th>weighted_2_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>14043</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Snow</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>BUFFALO</td>\n",
       "      <td>Balance</td>\n",
       "      <td>64079.62</td>\n",
       "      <td>82102.04</td>\n",
       "      <td>7</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4636</td>\n",
       "      <td>60107</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Snow</td>\n",
       "      <td>0.110874</td>\n",
       "      <td>0.55437</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-03-10</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>Balance</td>\n",
       "      <td>43586.96</td>\n",
       "      <td>47875.77</td>\n",
       "      <td>7</td>\n",
       "      <td>0.55437</td>\n",
       "      <td>1.330489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id zip_cd        Date Collected  Severity  desc    weight  \\\n",
       "0         391  14043  2018-02-22  Recorded       5.0  Snow  0.125000   \n",
       "1        4636  60107  2018-03-06  Recorded       5.0  Snow  0.110874   \n",
       "\n",
       "   weighted_severity  rank week_end_dt      DMA Q1 Campaign Type  \\\n",
       "0            0.62500    12  2018-02-24  BUFFALO          Balance   \n",
       "1            0.55437    12  2018-03-10  CHICAGO          Balance   \n",
       "\n",
       "   sales_weekly_2017  sales_weekly_2018  Recorded_Days  weighted_2_severity  \\\n",
       "0           64079.62           82102.04              7              0.62500   \n",
       "1           43586.96           47875.77              7              0.55437   \n",
       "\n",
       "   weighted_2_rank  \n",
       "0         1.500000  \n",
       "1         1.330489  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_group_long_dedup.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_to_weekly(df):\n",
    "    df_weighted_rank=df.groupby([\"location_id\",\"week_end_dt\"])['weighted_2_rank'].sum().to_frame().reset_index()\n",
    "    df_weighted_severity=df.groupby([\"location_id\",\"week_end_dt\"])['weighted_2_severity'].sum().to_frame().reset_index()\n",
    "    df_YoY=df[['location_id','zip_cd','week_end_dt','DMA','Q1 Campaign Type','sales_weekly_2017','sales_weekly_2018']].drop_duplicates()\n",
    "    result=pd.merge(df_YoY,df_weighted_rank,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "    result=pd.merge(result,df_weighted_severity,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "    result['YoY_by_week']=(result['sales_weekly_2018']-result['sales_weekly_2017'])/result['sales_weekly_2017']\n",
    "    result['location_id']=result['location_id'].astype(int)\n",
    "    result=result.sort_values(['location_id','week_end_dt'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_week_rain_desc=agg_to_weekly(df_rain_desc_long_dedup)\n",
    "agg_week_snow_desc=agg_to_weekly(df_snow_desc_long_dedup)\n",
    "agg_week_weather_group=agg_to_weekly(df_weather_group_long_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_to_Q1(df):\n",
    "    # all 13 weeks\n",
    "    df_weighted_rank=df.groupby([\"location_id\"])['weighted_2_rank'].sum().to_frame().reset_index()\n",
    "    df_weighted_severity=df.groupby([\"location_id\"])['weighted_2_severity'].sum().to_frame().reset_index()\n",
    "    df_YoY=df.groupby(['location_id','zip_cd','DMA','Q1 Campaign Type'])['sales_weekly_2017','sales_weekly_2018'].sum().reset_index()\n",
    "    result=pd.merge(df_YoY,df_weighted_rank,on=[\"location_id\"],how=\"left\")\n",
    "    result=pd.merge(result,df_weighted_severity,on=[\"location_id\"],how=\"left\")\n",
    "    result['YoY_by_week']=(result['sales_weekly_2018']-result['sales_weekly_2017'])/result['sales_weekly_2017']\n",
    "    result['location_id']=result['location_id'].astype(int)\n",
    "    result=result.sort_values('location_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_Q1_rain_desc=agg_to_Q1(df_rain_desc_long_dedup)\n",
    "agg_Q1_snow_desc=agg_to_Q1(df_snow_desc_long_dedup)\n",
    "agg_Q1_weather_group=agg_to_Q1(df_weather_group_long_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer_Bruce=pd.ExcelWriter(writer_folder+\"BL_Q1 weather aggregation.xlsx\",engine=\"xlsxwriter\")\n",
    "agg_Q1_rain_desc.to_excel(writer_Bruce,\"agg_Q1_rain_desc\",index=False)\n",
    "agg_Q1_snow_desc.to_excel(writer_Bruce,\"agg_Q1_snow_desc\",index=False)\n",
    "agg_Q1_weather_group.to_excel(writer_Bruce,\"agg_Q1_weather_group\",index=False)\n",
    "\n",
    "agg_week_rain_desc.to_excel(writer_Bruce,\"agg_week_rain_desc\",index=False)\n",
    "agg_week_snow_desc.to_excel(writer_Bruce,\"agg_week_snow_desc\",index=False)\n",
    "agg_week_weather_group.to_excel(writer_Bruce,\"agg_week_weather_group\",index=False)\n",
    "writer_Bruce.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126, 8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_joint_score[weather_joint_score['joint_weather_score']<1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_joint_score=agg_week_weather_group[['week_end_dt','location_id','weighted_2_severity']]\n",
    "weather_joint_score=weather_joint_score.rename(columns={\"weighted_2_severity\":\"joint_weather_score\"})\n",
    "\n",
    "rain_joint_score=agg_week_rain_desc[['week_end_dt','location_id','weighted_2_severity']]\n",
    "rain_joint_score=rain_joint_score.rename(columns={\"weighted_2_severity\":\"rain_score\"})\n",
    "\n",
    "snow_joint_score=agg_week_snow_desc[['week_end_dt','location_id','weighted_2_severity',\n",
    "                                    'sales_weekly_2017','sales_weekly_2018']]\n",
    "snow_joint_score=snow_joint_score.rename(columns={\"weighted_2_severity\":\"snow_score\"})\n",
    "\n",
    "weather_joint_score=pd.merge(weather_joint_score,rain_joint_score,on=['week_end_dt','location_id'])\n",
    "weather_joint_score=pd.merge(weather_joint_score,snow_joint_score,on=['week_end_dt','location_id'])\n",
    "weather_joint_score.to_csv(writer_folder+\"BL_joint weather data to create matrix_JL_\"+str(datetime.datetime.now().date())+\".csv\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_end_dt</th>\n",
       "      <th>location_id</th>\n",
       "      <th>joint_weather_score</th>\n",
       "      <th>rain_score</th>\n",
       "      <th>snow_score</th>\n",
       "      <th>sales_weekly_2017</th>\n",
       "      <th>sales_weekly_2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.801694</td>\n",
       "      <td>0.599658</td>\n",
       "      <td>0.601018</td>\n",
       "      <td>65529.31</td>\n",
       "      <td>63152.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.072440</td>\n",
       "      <td>1.155775</td>\n",
       "      <td>0.399772</td>\n",
       "      <td>72052.29</td>\n",
       "      <td>65248.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_end_dt  location_id  joint_weather_score  rain_score  snow_score  \\\n",
       "0  2018-02-10            1             2.801694    0.599658    0.601018   \n",
       "1  2018-02-17            1             3.072440    1.155775    0.399772   \n",
       "\n",
       "   sales_weekly_2017  sales_weekly_2018  \n",
       "0           65529.31           63152.41  \n",
       "1           72052.29           65248.17  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_joint_score.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer_excel_tabs=pd.ExcelWriter(writer_folder+\"BL_joint weather bin tabs\"+str(datetime.datetime.now().date())+\".xlsx\",engine='xlsxwriter')\n",
    "# Use the 13 weeks overal jont_weather_score percentile to create 5 bins\n",
    "weather_joint_score['joint_score_bin']=np.where(weather_joint_score['joint_weather_score']>=np.percentile(weather_joint_score['joint_weather_score'],80),\"Severity_5\",\n",
    "                                               np.where(weather_joint_score['joint_weather_score']>=np.percentile(weather_joint_score['joint_weather_score'],60),\"Severity_4\",\n",
    "                                                    np.where(weather_joint_score['joint_weather_score']>=np.percentile(weather_joint_score['joint_weather_score'],40),\"Severity_3\",\n",
    "                                                        np.where(weather_joint_score['joint_weather_score']>=np.percentile(weather_joint_score['joint_weather_score'],20),\"Severity_2\",\n",
    "                                                            'Severity_1')\n",
    "                                                            )\n",
    "                                                       )\n",
    "                                               )\n",
    "tab_frame=pd.DataFrame({\"joint_score_bin\":['Severity_1','Severity_2','Severity_3','Severity_4','Severity_5']},index=[x for x in range(5)])\n",
    "k=0\n",
    "for week_end_dt,group in weather_joint_score.groupby(['week_end_dt']):\n",
    "    #1\n",
    "    df=group[['location_id','joint_score_bin','week_end_dt']]\n",
    "\n",
    "    df_store_count=df.groupby(['joint_score_bin'])['location_id'].count().to_frame().reset_index()\n",
    "    df_store_count=df_store_count.rename(columns={\"location_id\":str(week_end_dt)})\n",
    "    \n",
    "    df_store_list=df.groupby(['joint_score_bin'])['location_id'].apply(list).to_frame().reset_index()\n",
    "    df_store_list=df_store_list.rename(columns={\"location_id\":str(week_end_dt)})\n",
    "    #2\n",
    "    df_score=group[['joint_weather_score','joint_score_bin','week_end_dt']]\n",
    "    \n",
    "    df_score_sum=df_score.groupby(['joint_score_bin'])['joint_weather_score'].sum().to_frame().reset_index()\n",
    "    df_score_sum=df_score_sum.rename(columns={\"joint_weather_score\":str(week_end_dt)})\n",
    "    \n",
    "    df_score_avg=df_score.groupby(['joint_score_bin'])['joint_weather_score'].mean().to_frame().reset_index()\n",
    "    df_score_avg=df_score_avg.rename(columns={\"joint_weather_score\":str(week_end_dt)})\n",
    "    \n",
    "    df_score_list=df_score.groupby(['joint_score_bin'])['joint_weather_score'].apply(list).to_frame().reset_index()\n",
    "    df_score_list=df_score_list.rename(columns={\"joint_weather_score\":str(week_end_dt)})\n",
    "    #3\n",
    "    df_sales=group[['sales_weekly_2017','sales_weekly_2018','joint_score_bin','week_end_dt']]\n",
    "    \n",
    "    df_sales_2017=df_sales.groupby(['joint_score_bin'])['sales_weekly_2017'].sum().to_frame().reset_index()\n",
    "    df_sales_2017=df_sales_2017.rename(columns={\"sales_weekly_2017\":str(week_end_dt)})\n",
    "    \n",
    "    df_sales_2018=df_sales.groupby(['joint_score_bin'])['sales_weekly_2018'].sum().to_frame().reset_index()\n",
    "    df_sales_2018=df_sales_2018.rename(columns={\"sales_weekly_2018\":str(week_end_dt)})\n",
    "    \n",
    "    \n",
    "    \n",
    "    if k==0:\n",
    "        tab_store_count=pd.merge(tab_frame,df_store_count,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_store_list=pd.merge(tab_frame,df_store_list,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_sum=pd.merge(tab_frame,df_score_sum,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_avg=pd.merge(tab_frame,df_score_avg,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_list=pd.merge(tab_frame,df_score_list,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_sales_2017=pd.merge(tab_frame,df_sales_2017,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_sales_2018=pd.merge(tab_frame,df_sales_2018,on=\"joint_score_bin\",how=\"left\")\n",
    "        \n",
    "    else:\n",
    "        tab_store_count=pd.merge(tab_store_count,df_store_count,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_store_list=pd.merge(tab_store_list,df_store_list,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_sum=pd.merge(tab_score_sum,df_score_sum,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_avg=pd.merge(tab_score_avg,df_score_avg,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_score_list=pd.merge(tab_score_list,df_score_list,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_sales_2017=pd.merge(tab_sales_2017,df_sales_2017,on=\"joint_score_bin\",how=\"left\")\n",
    "        tab_sales_2018=pd.merge(tab_sales_2018,df_sales_2018,on=\"joint_score_bin\",how=\"left\")\n",
    "        \n",
    "    k=k+1\n",
    "tab_sales_YoY=tab_sales_2017.copy()\n",
    "for i in range(len(tab_sales_2018)):\n",
    "    for j in range(1,tab_sales_2018.shape[1]):\n",
    "        tab_sales_YoY.iloc[i,j]=(tab_sales_2018.iloc[i,j]-tab_sales_2017.iloc[i,j])/tab_sales_2017.iloc[i,j]\n",
    "        \n",
    "tab_store_count.to_excel(writer_excel_tabs,\"tab_store_count\",index=False)\n",
    "tab_store_list.to_excel(writer_excel_tabs,\"tab_store_list\",index=False)\n",
    "tab_score_sum.to_excel(writer_excel_tabs,\"joint_weather_score_sum\",index=False)\n",
    "tab_score_avg.to_excel(writer_excel_tabs,\"joint_weather_score_avg\",index=False)\n",
    "tab_score_list.to_excel(writer_excel_tabs,\"joint_weather_score_list\",index=False)\n",
    "tab_sales_2017.to_excel(writer_excel_tabs,\"sales_2017_sum\",index=False)\n",
    "tab_sales_2018.to_excel(writer_excel_tabs,\"sales_2018_sum\",index=False)\n",
    "tab_sales_YoY.to_excel(writer_excel_tabs,\"Q1_sales_YoY\",index=False)\n",
    "writer_excel_tabs.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joint_score_bin</th>\n",
       "      <th>2018-02-10</th>\n",
       "      <th>2018-02-17</th>\n",
       "      <th>2018-02-24</th>\n",
       "      <th>2018-03-03</th>\n",
       "      <th>2018-03-10</th>\n",
       "      <th>2018-03-17</th>\n",
       "      <th>2018-03-24</th>\n",
       "      <th>2018-03-31</th>\n",
       "      <th>2018-04-07</th>\n",
       "      <th>2018-04-14</th>\n",
       "      <th>2018-04-21</th>\n",
       "      <th>2018-04-28</th>\n",
       "      <th>2018-05-05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Severity_1</td>\n",
       "      <td>9467267.80</td>\n",
       "      <td>11480055.93</td>\n",
       "      <td>20958127.32</td>\n",
       "      <td>822711.03</td>\n",
       "      <td>25612909.0</td>\n",
       "      <td>25048291.02</td>\n",
       "      <td>18175136.73</td>\n",
       "      <td>25398323.37</td>\n",
       "      <td>23496063.98</td>\n",
       "      <td>34059408.63</td>\n",
       "      <td>3525406.54</td>\n",
       "      <td>21597513.18</td>\n",
       "      <td>38083220.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Severity_2</td>\n",
       "      <td>7194888.56</td>\n",
       "      <td>12474278.63</td>\n",
       "      <td>19478614.96</td>\n",
       "      <td>30172171.13</td>\n",
       "      <td>23559537.9</td>\n",
       "      <td>11567865.82</td>\n",
       "      <td>17016245.91</td>\n",
       "      <td>23439362.19</td>\n",
       "      <td>16440673.39</td>\n",
       "      <td>27602285.21</td>\n",
       "      <td>26275629.35</td>\n",
       "      <td>18860569.06</td>\n",
       "      <td>21691325.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  joint_score_bin  2018-02-10   2018-02-17   2018-02-24   2018-03-03  \\\n",
       "0      Severity_1  9467267.80  11480055.93  20958127.32    822711.03   \n",
       "1      Severity_2  7194888.56  12474278.63  19478614.96  30172171.13   \n",
       "\n",
       "   2018-03-10   2018-03-17   2018-03-24   2018-03-31   2018-04-07  \\\n",
       "0  25612909.0  25048291.02  18175136.73  25398323.37  23496063.98   \n",
       "1  23559537.9  11567865.82  17016245.91  23439362.19  16440673.39   \n",
       "\n",
       "    2018-04-14   2018-04-21   2018-04-28   2018-05-05  \n",
       "0  34059408.63   3525406.54  21597513.18  38083220.74  \n",
       "1  27602285.21  26275629.35  18860569.06  21691325.81  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_sales_2017.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Done of here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append YoY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_temp_YoY=df_output_temp_store[['location_id','Date','Diff_Temp']].pivot(index=\"location_id\",columns=\"Date\",values=\"Diff_Temp\").reset_index()\n",
    "delta_temp_Max_YoY=df_output_temp_store[['location_id','Date','Diff_Temp_Max']].pivot(index=\"location_id\",columns=\"Date\",values=\"Diff_Temp_Max\").reset_index()\n",
    "delta_temp_Min_YoY=df_output_temp_store[['location_id','Date','Diff_Temp_Min']].pivot(index=\"location_id\",columns=\"Date\",values=\"Diff_Temp_Min\").reset_index()\n",
    "\n",
    "delta_temp_YoY=pd.merge(delta_temp_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "delta_temp_Max_YoY=pd.merge(delta_temp_Max_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "delta_temp_Min_YoY=pd.merge(delta_temp_Min_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "\n",
    "rain_YoY=df_output_rain_store[[\"location_id\",\"Date\",\"Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Severity\").reset_index()\n",
    "snow_YoY=df_output_snow_store[[\"location_id\",\"Date\",\"Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Severity\").reset_index()\n",
    "rain_YoY=pd.merge(rain_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "snow_YoY=pd.merge(snow_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "\n",
    "Weather_group_YoY=df_output_group_store[[\"location_id\",\"Date\",\"Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Severity\").reset_index()\n",
    "Weather_group_YoY=pd.merge(Weather_group_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "\n",
    "writer=pd.ExcelWriter(writer_folder+\"Q1_inclusion_store_all_weather_type_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "delta_temp_YoY.to_excel(writer,\"delta_temp_YoY\",index=False)\n",
    "delta_temp_Max_YoY.to_excel(writer,\"delta_temp_Max_YoY\",index=False)\n",
    "delta_temp_Min_YoY.to_excel(writer,\"delta_temp_Min_YoY\",index=False)\n",
    "\n",
    "rain_YoY.to_excel(writer,\"rain_YoY\",index=False)\n",
    "snow_YoY.to_excel(writer,\"snow_YoY\",index=False)\n",
    "Weather_group_YoY.to_excel(writer,\"weather_group_YoY\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New method to use the weekday average\n",
    "weighted_delta_temp_YoY=df_output_temp_store_weight[['location_id','Date','Weighted_Diff_Temp']].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Diff_Temp\").reset_index()\n",
    "weighted_delta_temp_Max_YoY=df_output_temp_store_weight[['location_id','Date','Weighted_Diff_Temp_Max']].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Diff_Temp_Max\").reset_index()\n",
    "weighted_delta_temp_Min_YoY=df_output_temp_store_weight[['location_id','Date','Weighted_Diff_Temp_Min']].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Diff_Temp_Min\").reset_index()\n",
    "\n",
    "weighted_delta_temp_YoY=pd.merge(weighted_delta_temp_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "weighted_delta_temp_Max_YoY=pd.merge(weighted_delta_temp_Max_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "weighted_delta_temp_Min_YoY=pd.merge(weighted_delta_temp_Min_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_rain_YoY=df_output_rain_store_weight[[\"location_id\",\"Date\",\"Weighted_Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Severity\").reset_index()\n",
    "weighted_snow_YoY=df_output_snow_store_weight[[\"location_id\",\"Date\",\"Weighted_Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Severity\").reset_index()\n",
    "weighted_rain_YoY=pd.merge(weighted_rain_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n",
    "weighted_snow_YoY=pd.merge(weighted_snow_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_Weather_group_YoY=df_output_group_store_weight[[\"location_id\",\"Date\",\"Weighted_Severity\"]].pivot(index=\"location_id\",columns=\"Date\",values=\"Weighted_Severity\").reset_index()\n",
    "weighted_Weather_group_YoY=pd.merge(weighted_Weather_group_YoY,inclusion_YoY,on=\"location_id\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_weighted_inclusion_store_all_weather_type_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "weighted_delta_temp_YoY.to_excel(writer,\"delta_temp_YoY\",index=False)\n",
    "weighted_delta_temp_Max_YoY.to_excel(writer,\"delta_temp_Max_YoY\",index=False)\n",
    "weighted_delta_temp_Min_YoY.to_excel(writer,\"delta_temp_Min_YoY\",index=False)\n",
    "\n",
    "weighted_rain_YoY.to_excel(writer,\"rain_YoY\",index=False)\n",
    "weighted_snow_YoY.to_excel(writer,\"snow_YoY\",index=False)\n",
    "weighted_Weather_group_YoY.to_excel(writer,\"weather_group_YoY\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>Date</th>\n",
       "      <th>Collected</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Temp_Max</th>\n",
       "      <th>Temp_Min</th>\n",
       "      <th>location_id</th>\n",
       "      <th>Avg_Temp</th>\n",
       "      <th>Avg_Temp_Max</th>\n",
       "      <th>Avg_Temp_Min</th>\n",
       "      <th>Diff_Temp</th>\n",
       "      <th>Diff_Temp_Max</th>\n",
       "      <th>Diff_Temp_Min</th>\n",
       "      <th>week_end_dt</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>weekly_traffic</th>\n",
       "      <th>Weight_by_Traffic</th>\n",
       "      <th>Weighted_Diff_Temp</th>\n",
       "      <th>Weighted_Diff_Temp_Max</th>\n",
       "      <th>Weighted_Diff_Temp_Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43232</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>46.004</td>\n",
       "      <td>48.2</td>\n",
       "      <td>44.6</td>\n",
       "      <td>1</td>\n",
       "      <td>44.580071</td>\n",
       "      <td>46.592857</td>\n",
       "      <td>42.414286</td>\n",
       "      <td>1.423929</td>\n",
       "      <td>1.607143</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>681.0</td>\n",
       "      <td>4479.0</td>\n",
       "      <td>0.152043</td>\n",
       "      <td>0.216498</td>\n",
       "      <td>0.244355</td>\n",
       "      <td>0.332322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30906</td>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>Recorded</td>\n",
       "      <td>79.502</td>\n",
       "      <td>80.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3</td>\n",
       "      <td>66.513071</td>\n",
       "      <td>69.221429</td>\n",
       "      <td>63.757143</td>\n",
       "      <td>12.988929</td>\n",
       "      <td>11.378571</td>\n",
       "      <td>13.242857</td>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>723.0</td>\n",
       "      <td>5346.0</td>\n",
       "      <td>0.135241</td>\n",
       "      <td>1.756640</td>\n",
       "      <td>1.538853</td>\n",
       "      <td>1.790981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zip_cd        Date Collected    Temp  Temp_Max  Temp_Min location_id  \\\n",
       "0  43232  2018-02-22  Recorded  46.004      48.2      44.6           1   \n",
       "1  30906  2018-02-22  Recorded  79.502      80.6      77.0           3   \n",
       "\n",
       "    Avg_Temp  Avg_Temp_Max  Avg_Temp_Min  Diff_Temp  Diff_Temp_Max  \\\n",
       "0  44.580071     46.592857     42.414286   1.423929       1.607143   \n",
       "1  66.513071     69.221429     63.757143  12.988929      11.378571   \n",
       "\n",
       "   Diff_Temp_Min week_end_dt  Traffic  weekly_traffic  Weight_by_Traffic  \\\n",
       "0       2.185714  2018-02-24    681.0          4479.0           0.152043   \n",
       "1      13.242857  2018-02-24    723.0          5346.0           0.135241   \n",
       "\n",
       "   Weighted_Diff_Temp  Weighted_Diff_Temp_Max  Weighted_Diff_Temp_Min  \n",
       "0            0.216498                0.244355                0.332322  \n",
       "1            1.756640                1.538853                1.790981  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for date,group in df_output_temp_store_weight.groupby(\"Date\"):\n",
    "    df=group[[\"Date\",\"location_id\",\"Diff_Temp\",\"Diff_Temp_Max\",\"Diff_Temp_Min\"]]\n",
    "    df_95_temp=df[df['Diff_Temp']>=np.percentile(df['Diff_Temp'],95)]\n",
    "    df_75_temp=df[(df['Diff_Temp']>=np.percentile(df['Diff_Temp'],75))&(df['Diff_Temp']<np.percentile(df['Diff_Temp'],95))]\n",
    "    df_50_temp=df[(df['Diff_Temp']>=np.percentile(df['Diff_Temp'],50))&(df['Diff_Temp']<np.percentile(df['Diff_Temp'],75))]\n",
    "    df_25_temp=df[(df['Diff_Temp']>=np.percentile(df['Diff_Temp'],25))&(df['Diff_Temp']<np.percentile(df['Diff_Temp'],50))]\n",
    "    df_5_temp=df[df['Diff_Temp']<np.percentile(df['Diff_Temp'],5)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_week_end_df=df_output_rain_store_weight[['Date','week_end_dt']].drop_duplicates()\n",
    "date_week_end_df=date_week_end_df[~pd.isnull(date_week_end_df['week_end_dt'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>week_end_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-22</td>\n",
       "      <td>2018-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>2018-03-19</td>\n",
       "      <td>2018-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date week_end_dt\n",
       "0     2018-02-22  2018-02-24\n",
       "1382  2018-03-19  2018-03-24"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_week_end_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Percentile_Value_With_0(data_frame,weather):\n",
    "    k_date=0\n",
    "    result_p=pd.DataFrame()\n",
    "\n",
    "    for date,group in data_frame.groupby(\"Date\"):\n",
    "        df=group[['zip_cd','Date','Collected','location_id','Weighted_Severity']]\n",
    "        df=df[df['Collected']==\"Recorded\"]\n",
    "        P100=np.nanpercentile(df['Weighted_Severity'],100)\n",
    "        P95=np.nanpercentile(df['Weighted_Severity'],95)\n",
    "        P75=np.nanpercentile(df['Weighted_Severity'],75)\n",
    "        P50=np.nanpercentile(df['Weighted_Severity'],50)\n",
    "        P25=np.nanpercentile(df['Weighted_Severity'],25)\n",
    "        P5=np.nanpercentile(df['Weighted_Severity'],5)\n",
    "        P0=np.nanpercentile(df['Weighted_Severity'],0)\n",
    "\n",
    "        df_app=pd.DataFrame({\"Date\":date,weather+\"_Severity_Max\":P100,weather+\"_Severity_P95\":P95,weather+\"_Severity_P75\":P75,\n",
    "                            weather+\"_Severity_P50\":P50,weather+\"_Severity_P25\":P25,weather+\"_Severity_P5\":P5,weather+\"_Severity_Min\":P0},\n",
    "                            index=[k_date])\n",
    "        \n",
    "        k_date=k_date+1\n",
    "        result_p=result_p.append(df_app)\n",
    "    result_p=pd.merge(result_p,date_week_end_df,on=\"Date\",how=\"left\")\n",
    "    result_p=result_p[['Date',weather+\"_Severity_Max\",weather+\"_Severity_P95\",weather+\"_Severity_P75\",\n",
    "                          weather+\"_Severity_P50\",weather+\"_Severity_P25\",weather+\"_Severity_P5\",weather+\"_Severity_Min\",\"week_end_dt\"]]\n",
    "        \n",
    "    return result_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain_chart_With_0=Percentile_Value_With_0(df_output_rain_store_weight,\"Rain\")\n",
    "snow_chart_With_0=Percentile_Value_With_0(df_output_snow_store_weight,\"Snow\")\n",
    "group_chart_With_0=Percentile_Value_With_0(df_output_group_store_weight,\"Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Snow_Severity_Max</th>\n",
       "      <th>Snow_Severity_P95</th>\n",
       "      <th>Snow_Severity_P75</th>\n",
       "      <th>Snow_Severity_P50</th>\n",
       "      <th>Snow_Severity_P25</th>\n",
       "      <th>Snow_Severity_P5</th>\n",
       "      <th>Snow_Severity_Min</th>\n",
       "      <th>week_end_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>0.462653</td>\n",
       "      <td>0.313744</td>\n",
       "      <td>0.18295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>0.355082</td>\n",
       "      <td>0.221446</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Snow_Severity_Max  Snow_Severity_P95  Snow_Severity_P75  \\\n",
       "0  2018-02-04           0.462653           0.313744            0.18295   \n",
       "1  2018-02-05           0.355082           0.221446            0.00000   \n",
       "\n",
       "   Snow_Severity_P50  Snow_Severity_P25  Snow_Severity_P5  Snow_Severity_Min  \\\n",
       "0                0.0                0.0               0.0                0.0   \n",
       "1                0.0                0.0               0.0                0.0   \n",
       "\n",
       "  week_end_dt  \n",
       "0  2018-02-10  \n",
       "1  2018-02-10  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_chart_With_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Percentile_Value_No_0(data_frame,weather):\n",
    "    k_date=0\n",
    "    result_p=pd.DataFrame()\n",
    "\n",
    "    for date,group in data_frame.groupby(\"Date\"):\n",
    "        df=group[['zip_cd','Date','Collected','location_id','Weighted_Severity']]\n",
    "        df=df[df['Collected']==\"Recorded\"]\n",
    "        df=df[df['Weighted_Severity']>0]\n",
    "        P100=np.nanpercentile(df['Weighted_Severity'],100)\n",
    "        P95=np.nanpercentile(df['Weighted_Severity'],95)\n",
    "        P75=np.nanpercentile(df['Weighted_Severity'],75)\n",
    "        P50=np.nanpercentile(df['Weighted_Severity'],50)\n",
    "        P25=np.nanpercentile(df['Weighted_Severity'],25)\n",
    "        P5=np.nanpercentile(df['Weighted_Severity'],5)\n",
    "        P0=np.nanpercentile(df['Weighted_Severity'],0)\n",
    "\n",
    "        df_app=pd.DataFrame({\"Date\":date,weather+\"_Severity_Max\":P100,weather+\"_Severity_P95\":P95,weather+\"_Severity_P75\":P75,\n",
    "                            weather+\"_Severity_P50\":P50,weather+\"_Severity_P25\":P25,weather+\"_Severity_P5\":P5,weather+\"_Severity_Min\":P0},\n",
    "                            index=[k_date])\n",
    "        \n",
    "        k_date=k_date+1\n",
    "        result_p=result_p.append(df_app)\n",
    "    result_p=pd.merge(result_p,date_week_end_df,on=\"Date\",how=\"left\")\n",
    "    result_p=result_p[['Date',weather+\"_Severity_Max\",weather+\"_Severity_P95\",weather+\"_Severity_P75\",\n",
    "                          weather+\"_Severity_P50\",weather+\"_Severity_P25\",weather+\"_Severity_P5\",weather+\"_Severity_Min\",\"week_end_dt\"]]\n",
    "    return result_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain_chart_No_0=Percentile_Value_No_0(df_output_rain_store_weight,\"Rain\")\n",
    "snow_chart_No_0=Percentile_Value_No_0(df_output_snow_store_weight,\"Snow\")\n",
    "group_chart_No_0=Percentile_Value_No_0(df_output_group_store_weight,\"Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer_chart_P=pd.ExcelWriter(writer_folder+\"Q1_Weather_Percentile_Chart_by_day_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "rain_chart_No_0.to_excel(writer_chart_P,\"Rain_No_0\",index=False)\n",
    "snow_chart_No_0.to_excel(writer_chart_P,\"Snow_No_0\",index=False)\n",
    "group_chart_No_0.to_excel(writer_chart_P,\"group_No_0\",index=False)\n",
    "rain_chart_With_0.to_excel(writer_chart_P,\"Rain_With_0\",index=False)\n",
    "snow_chart_With_0.to_excel(writer_chart_P,\"Snow_With_0\",index=False)\n",
    "group_chart_With_0.to_excel(writer_chart_P,\"group_With_0\",index=False)\n",
    "writer_chart_P.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_long=pd.read_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_2018-06-02.csv\",dtype=str)\n",
    "sales_long=sales_long[['location_id','week_end_date','sales']].rename(columns={\"week_end_date\":\"week_end_dt\"})\n",
    "sales_long['location_id']=sales_long['location_id'].astype(str)\n",
    "sales_long['sales']=sales_long['sales'].astype(float)\n",
    "sales_long['week_end_dt']=sales_long['week_end_dt'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_long_2018Q1=sales_long[(sales_long['week_end_dt']>=datetime.datetime(2018,2,10).date()) & (sales_long['week_end_dt']<=datetime.datetime(2018,5,5).date())]\n",
    "sales_long_2017Q1=sales_long[(sales_long['week_end_dt']>=datetime.datetime(2017,2,11).date()) & (sales_long['week_end_dt']<=datetime.datetime(2017,5,6).date())]\n",
    "\n",
    "sales_long_2017Q1=sales_long_2017Q1.rename(columns={\"sales\":\"weekly_sales_2017\"})\n",
    "sales_long_2018Q1=sales_long_2018Q1.rename(columns={\"sales\":\"weekly_sales_2018\"})\n",
    "\n",
    "sales_long_2017Q1['week_end_dt']=sales_long_2017Q1['week_end_dt'].apply(lambda x: x+datetime.timedelta(days=52*7))\n",
    "\n",
    "sales_long_weekly=pd.merge(sales_long_2018Q1,sales_long_2017Q1,on=[\"location_id\",\"week_end_dt\"],how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_long_weekly['YoY']=(sales_long_weekly['weekly_sales_2018']-sales_long_weekly['weekly_sales_2017'])/sales_long_weekly['weekly_sales_2017']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>week_end_dt</th>\n",
       "      <th>weekly_sales_2018</th>\n",
       "      <th>weekly_sales_2017</th>\n",
       "      <th>YoY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>63152.41</td>\n",
       "      <td>65529.31</td>\n",
       "      <td>-0.036272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>65248.17</td>\n",
       "      <td>72052.29</td>\n",
       "      <td>-0.094433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id week_end_dt  weekly_sales_2018  weekly_sales_2017       YoY\n",
       "0           1  2018-02-10           63152.41           65529.31 -0.036272\n",
       "1           1  2018-02-17           65248.17           72052.29 -0.094433"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_long_weekly.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolledup_for_rain_weighted_severity=df_output_rain_store_weight.groupby(['location_id','week_end_dt'])['Weighted_Severity'].mean().to_frame().reset_index()\n",
    "rolledup_for_rain_weighted_severity=pd.merge(rolledup_for_rain_weighted_severity,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "\n",
    "rolledup_for_snow_weighted_severity=df_output_snow_store_weight.groupby(['location_id','week_end_dt'])['Weighted_Severity'].mean().to_frame().reset_index()\n",
    "rolledup_for_snow_weighted_severity=pd.merge(rolledup_for_snow_weighted_severity,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "\n",
    "rolledup_for_group_weighted_severity=df_output_group_store_weight.groupby(['location_id','week_end_dt'])['Weighted_Severity'].mean().to_frame().reset_index()\n",
    "rolledup_for_group_weighted_severity=pd.merge(rolledup_for_group_weighted_severity,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_weighted_inclusion_store_by_week_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "rolledup_for_rain_weighted_severity.to_excel(writer,\"rain\",index=False)\n",
    "rolledup_for_snow_weighted_severity.to_excel(writer,\"snow\",index=False)\n",
    "rolledup_for_group_weighted_severity.to_excel(writer,\"group\",index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_rain_by_week_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "rolledup_for_rain_weighted_severity.to_excel(writer,\"rain_data\",index=False)\n",
    "for week_end in rolledup_for_rain_weighted_severity['week_end_dt'].unique():\n",
    "    df=rolledup_for_rain_weighted_severity[rolledup_for_rain_weighted_severity['week_end_dt']==week_end][['location_id','week_end_dt','Weighted_Severity','YoY',\"weekly_sales_2018\",\"weekly_sales_2017\"]]\n",
    "    df['location_id']=df['location_id'].astype(int)\n",
    "    df=df.sort_values(\"location_id\")\n",
    "    df.to_excel(writer,str(week_end),index=False)\n",
    "writer.save()   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_snow_by_week_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "rolledup_for_snow_weighted_severity.to_excel(writer,\"snow_data\",index=False)\n",
    "for week_end in rolledup_for_snow_weighted_severity['week_end_dt'].unique():\n",
    "    df=rolledup_for_snow_weighted_severity[rolledup_for_snow_weighted_severity['week_end_dt']==week_end][['location_id','week_end_dt','Weighted_Severity','YoY',\"weekly_sales_2018\",\"weekly_sales_2017\"]]\n",
    "    df['location_id']=df['location_id'].astype(int)\n",
    "    df=df.sort_values(\"location_id\")\n",
    "    df.to_excel(writer,str(week_end),index=False)\n",
    "writer.save()   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(writer_folder+\"Q1_group_by_week_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "rolledup_for_group_weighted_severity.to_excel(writer,\"group_data\",index=False)\n",
    "for week_end in rolledup_for_group_weighted_severity['week_end_dt'].unique():\n",
    "    df=rolledup_for_group_weighted_severity[rolledup_for_group_weighted_severity['week_end_dt']==week_end][['location_id','week_end_dt','Weighted_Severity','YoY',\"weekly_sales_2018\",\"weekly_sales_2017\"]]\n",
    "    df['location_id']=df['location_id'].astype(int)\n",
    "    df=df.sort_values(\"location_id\")\n",
    "    df.to_excel(writer,str(week_end),index=False)\n",
    "writer.save() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rolledup_for_group_weighted_severity['week_end_dt'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_rain_store_weight_weekly_sales=pd.merge(df_output_rain_store_weight,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "df_output_snow_store_weight_weekly_sales=pd.merge(df_output_snow_store_weight,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")\n",
    "df_output_group_store_weight_weekly_sales=pd.merge(df_output_group_store_weight,sales_long_weekly,on=[\"location_id\",\"week_end_dt\"],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Severity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-398-568b3c230a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0magg_YoY_rain_store_weight_weekly_sales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_output_rain_store_weight_weekly_sales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_output_rain_store_weight_weekly_sales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Collected'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"Recorded\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'week_end_dt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weekly_sales_2018'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weekly_sales_2017'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magg_YoY_rain_store_weight_weekly_sales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_YoY_rain_store_weight_weekly_sales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"week_end_dt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Severity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weekly_sales_2018\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekly_sales_2017\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m   4414\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   4415\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4416\u001b[0;31m                        **kwargs)\n\u001b[0m\u001b[1;32m   4417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4418\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                                     \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, mutated)\u001b[0m\n\u001b[1;32m   2688\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Severity'"
     ]
    }
   ],
   "source": [
    "\n",
    "agg_count_rain_store_weight_weekly_sales=df_output_rain_store_weight_weekly_sales[df_output_rain_store_weight_weekly_sales['Collected']==\"Recorded\"][['location_id','Date','Severity']]\n",
    "agg_count_rain_store_weight_weekly_sales=agg_count_rain_store_weight_weekly_sales.groupby(['Date','Severity'])['location_id'].count().to_frame().reset_index()\n",
    "agg_count_rain_store_weight_weekly_sales=agg_count_rain_store_weight_weekly_sales.rename(columns={\"location_id\":\"store_count\"})\n",
    "\n",
    "agg_YoY_rain_store_weight_weekly_sales=df_output_rain_store_weight_weekly_sales[df_output_rain_store_weight_weekly_sales['Collected']==\"Recorded\"][['location_id','week_end_dt','weekly_sales_2018','weekly_sales_2017']].drop_duplicates()\n",
    "agg_YoY_rain_store_weight_weekly_sales=agg_YoY_rain_store_weight_weekly_sales.groupby([\"week_end_dt\",\"Severity\"])[\"weekly_sales_2018\",\"weekly_sales_2017\"].sum().to_frame().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_rain_store_weight_weekly_sales.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=df_output_rain_store_weight_weekly_sales.copy()\n",
    "test['rain_max']=test[ranked_rain['rain_desc'].tolist()].ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output_rain_store_weight_weekly_sales.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
