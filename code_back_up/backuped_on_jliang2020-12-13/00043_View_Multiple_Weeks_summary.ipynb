{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:26:39.626870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import sqlalchemy\n",
    "import json\n",
    "config=json.load(open(\"/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/config.json\",\"r\"))\n",
    "username=config['username']\n",
    "password=config['password']\n",
    "database=config['database']\n",
    "\n",
    "BL_engine=sqlalchemy.create_engine(\n",
    "            \"mysql+pymysql://%s:%s@localhost/%s\" % (username, password, database))\n",
    "print(datetime.datetime.now())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-05-02_2020-08-24/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-05-09_2020-08-23/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-05-16_2020-08-23/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-05-23_2020-08-22/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-05-30_2020-08-21/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-06-06_2020-08-20/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-06-13_2020-08-17/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-06-20_2020-08-17/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-06-27_2020-08-16/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-07-04_2020-08-22/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-07-11_2020-08-16/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-07-18_2020-08-21/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-07-25_2020-08-27/',\n",
       " '/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/output_No_DCM_2020-07-25_2020-08-29/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_output='/home/jian/Projects/Big_Lots/Predictive_Model/extract_from_MySQL/'\n",
    "list_folder_output_by_week=os.listdir(folder_output)\n",
    "list_folder_output_by_week=[folder_output+x+\"/\" for x in list_folder_output_by_week if \"output_No_DCM_\" in x]\n",
    "list_folder_output_by_week.sort()\n",
    "list_folder_output_by_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folder_output_by_week=[x for x in list_folder_output_by_week if \"/output_No_DCM_2020-07-25_\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22009ae341a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfolder_unsubsription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/mnt/clients/juba/hqjubaapp02/sharefolder/biglots_data/Email_Subscription_Files/Unsubs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist_unsubsription_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_unsubsription\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlist_unsubsription_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_unsubsription_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"iber_File_Refresh__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist_unsubsription_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_unsubsription_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "folder_unsubsription='/home/sharefolder/biglots_data/unsubscribe/'\n",
    "list_unsubsription_files=glob.glob(folder_unsubsription+\"*.csv\")\n",
    "list_unsubsription_files=[x for x in list_unsubsription_files if \"iber_File_Refresh__\" in x]\n",
    "list_unsubsription_files.sort()\n",
    "list_unsubsription_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-02 trans_1_only_DV3_2020-05-02 2020-09-02 14:46:45.274861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-02 trans_2_plus_DV2_2020-05-02 2020-09-02 14:47:04.616662\n",
      "2020-05-09 trans_1_only_DV3_2020-05-09 2020-09-02 14:51:26.529120\n",
      "2020-05-09 trans_2_plus_DV2_2020-05-09 2020-09-02 14:51:45.379915\n",
      "2020-05-16 trans_1_only_DV3_2020-05-16 2020-09-02 14:56:09.992297\n",
      "2020-05-16 trans_2_plus_DV2_2020-05-16 2020-09-02 14:56:28.605826\n",
      "2020-05-23 trans_1_only_DV3_2020-05-23 2020-09-02 15:00:58.437651\n",
      "2020-05-23 trans_2_plus_DV2_2020-05-23 2020-09-02 15:01:17.629468\n",
      "2020-05-30 trans_1_only_DV3_2020-05-30 2020-09-02 15:05:49.109233\n",
      "2020-05-30 trans_2_plus_DV2_2020-05-30 2020-09-02 15:06:08.525740\n",
      "2020-06-06 trans_1_only_DV3_2020-06-06 2020-09-02 15:10:11.367364\n",
      "2020-06-06 trans_2_plus_DV2_2020-06-06 2020-09-02 15:10:30.339112\n",
      "2020-06-13 trans_1_only_DV3_2020-06-13 2020-09-02 15:15:31.739010\n",
      "2020-06-13 trans_2_plus_DV2_2020-06-13 2020-09-02 15:15:51.330823\n",
      "2020-06-20 trans_1_only_DV3_2020-06-20 2020-09-02 15:19:50.595787\n",
      "2020-06-20 trans_2_plus_DV2_2020-06-20 2020-09-02 15:20:08.924349\n",
      "2020-06-27 trans_1_only_DV3_2020-06-27 2020-09-02 15:23:53.371739\n",
      "2020-06-27 trans_2_plus_DV2_2020-06-27 2020-09-02 15:24:12.373650\n",
      "2020-07-04 trans_1_only_DV3_2020-07-04 2020-09-02 15:28:05.662450\n",
      "2020-07-04 trans_2_plus_DV2_2020-07-04 2020-09-02 15:28:26.879356\n",
      "2020-07-11 trans_1_only_DV3_2020-07-11 2020-09-02 15:32:17.202664\n",
      "2020-07-11 trans_2_plus_DV2_2020-07-11 2020-09-02 15:32:38.412180\n",
      "2020-07-18 trans_1_only_DV3_2020-07-18 2020-09-02 15:36:16.583107\n",
      "2020-07-18 trans_2_plus_DV2_2020-07-18 2020-09-02 15:36:36.595263\n"
     ]
    }
   ],
   "source": [
    "df_output_train_summary=pd.DataFrame()\n",
    "df_output_test_summary=pd.DataFrame()\n",
    "df_output_both_summary=pd.DataFrame()\n",
    "\n",
    "for weekly_folder in list_folder_output_by_week:\n",
    "    files=os.listdir(weekly_folder)\n",
    "    \n",
    "    file_trans_1_only_DV3=[weekly_folder+x for x in files if \"BL_LRModeling_NoDCM_trans_1_only_DV3\" in x][0]\n",
    "    file_trans_2_plus_DV2=[weekly_folder+x for x in files if \"BL_LRModeling_NoDCM_trans_2_plus_DV2\" in x][0]\n",
    "    week_end_dt=file_trans_1_only_DV3.split(\"_JL_\")[0][-10:]\n",
    "    week_end_dt=datetime.datetime.strptime(week_end_dt,\"%Y-%m-%d\").date()\n",
    "    \n",
    "    df_unsub_files=pd.DataFrame({\"file_path\":list_unsubsription_files})\n",
    "    df_unsub_files['date']=df_unsub_files['file_path'].apply(lambda x: x.split(\"ile_Refresh__\")[1][:8])\n",
    "    df_unsub_files['date']=pd.to_datetime(df_unsub_files['date']).dt.date\n",
    "    df_unsub_files['day_diff']=abs(df_unsub_files['date']-week_end_dt)\n",
    "    path_unsub=df_unsub_files[df_unsub_files['day_diff']==df_unsub_files['day_diff'].min()]['file_path'].values.tolist()[0]\n",
    "    list_unsunsribe_ids=pd.read_csv(path_unsub,\n",
    "                             dtype=str,usecols=['customersummary_c_primaryscnhash'])['customersummary_c_primaryscnhash'].unique().tolist()\n",
    "    \n",
    "    sql_str_start=\"'\"+str(week_end_dt-datetime.timedelta(days=13))+\"'\"\n",
    "    sql_str_end=\"'\"+str(week_end_dt)+\"'\"\n",
    "    df_ids_shoppers_past_2_weeks=pd.read_sql(\"select distinct customer_id_hashed as customer_id_hashed from Pred_POS_Department where transaction_dt between %s and %s and customer_id_hashed is not null;\"%(sql_str_start,sql_str_end),con=BL_engine)\n",
    "    df_ids_shoppers_past_2_weeks['past_shoppers']=\"shopper_in_past_2_weeks\"\n",
    "    df_week_train=pd.DataFrame()\n",
    "    df_week_test=pd.DataFrame()\n",
    "    df_week_both=pd.DataFrame()\n",
    "    \n",
    "    # train\n",
    "    for file in [file_trans_1_only_DV3,file_trans_2_plus_DV2]:\n",
    "        print(week_end_dt,file.split(\"BL_LRModeling_NoDCM_\")[1][:27],datetime.datetime.now())\n",
    "        type_model=file.split(\"BL_LRModeling_NoDCM_\")[1][:12]\n",
    "        DV_n=file.split(type_model)[1][1:4]\n",
    "        excel_file=pd.ExcelFile(file)\n",
    "        file_ids_train=[weekly_folder+x for x in files if \"df_train\" in x and type_model in x][0]\n",
    "        file_ids_test=[weekly_folder+x for x in files if \"df_test\" in x and type_model in x][0]\n",
    "        ### train\n",
    "        df_ids_train=pd.read_csv(file_ids_train)\n",
    "        df_ids_train['subscription_lables']=np.where(df_ids_train['customer_id_hashed'].isin(list_unsunsribe_ids),\"unsub\",\"default_subs\")\n",
    "\n",
    "        df_ids_train=pd.merge(df_ids_train,df_ids_shoppers_past_2_weeks,on=\"customer_id_hashed\",how=\"left\")\n",
    "        count_target_ids_shoppered_past2weeks_train=df_ids_train[(df_ids_train['selection_label']==\"target\") & (df_ids_train['past_shoppers']==\"shopper_in_past_2_weeks\")]\n",
    "        count_target_ids_shoppered_past2weeks_train=len(count_target_ids_shoppered_past2weeks_train)\n",
    "        count_nonselect_ids_shoppered_past2weeks_train=df_ids_train[(df_ids_train['selection_label']==\"nonselect\") & (df_ids_train['past_shoppers']==\"shopper_in_past_2_weeks\")]\n",
    "        count_nonselect_ids_shoppered_past2weeks_train=len(count_nonselect_ids_shoppered_past2weeks_train)\n",
    "        ### test\n",
    "        df_ids_test=pd.read_csv(file_ids_test)\n",
    "        df_ids_test['subscription_lables']=np.where(df_ids_test['customer_id_hashed'].isin(list_unsunsribe_ids),\"unsub\",\"default_subs\")\n",
    "\n",
    "        df_ids_test=pd.merge(df_ids_test,df_ids_shoppers_past_2_weeks,on=\"customer_id_hashed\",how=\"left\")\n",
    "        count_target_ids_shoppered_past2weeks_test=df_ids_test[(df_ids_test['selection_label']==\"target\") & (df_ids_test['past_shoppers']==\"shopper_in_past_2_weeks\")]\n",
    "        count_target_ids_shoppered_past2weeks_test=len(count_target_ids_shoppered_past2weeks_test)\n",
    "        count_nonselect_ids_shoppered_past2weeks_test=df_ids_test[(df_ids_test['selection_label']==\"nonselect\") & (df_ids_test['past_shoppers']==\"shopper_in_past_2_weeks\")]\n",
    "        count_nonselect_ids_shoppered_past2weeks_test=len(count_nonselect_ids_shoppered_past2weeks_test)\n",
    "        \n",
    "        # new to file past 4 weeks count\n",
    "        count_id_train_new_signs_past4weeks=df_ids_train[(df_ids_train['sign_up_label']==\"new_signs\")].shape[0]\n",
    "        count_id_train_new_signs_past4weeks_sub=df_ids_train[(df_ids_train['sign_up_label']==\"new_signs\") & (df_ids_train['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        count_id_train_new_signs_past4weeks_unsub=df_ids_train[(df_ids_train['sign_up_label']==\"new_signs\") & (df_ids_train['subscription_lables']==\"unsub\")].shape[0]\n",
    "        \n",
    "        count_id_test_new_signs_past4weeks=df_ids_test[(df_ids_test['sign_up_label']==\"new_signs\")].shape[0]\n",
    "        count_id_test_new_signs_past4weeks_sub=df_ids_test[(df_ids_test['sign_up_label']==\"new_signs\") & (df_ids_test['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        count_id_test_new_signs_past4weeks_unsub=df_ids_test[(df_ids_test['sign_up_label']==\"new_signs\") & (df_ids_test['subscription_lables']==\"unsub\")].shape[0]\n",
    "        \n",
    "        \n",
    "        # count\n",
    "        df=excel_file.parse(\"df_dataset_shape\")\n",
    "        train_records=df.loc[df['Unnamed: 0']==\"X_train\",\"records\"].values[0]\n",
    "        test_records=df.loc[df['Unnamed: 0']==\"X_test\",\"records\"].values[0]\n",
    "\n",
    "        # cutoff\n",
    "        df=excel_file.parse(\"select_score_matrix\")\n",
    "        cutoff=df.iloc[0,0]\n",
    "        # Media selection\n",
    "        # Train\n",
    "        df_train=excel_file.parse(\"train_id_summary\")\n",
    "        target_count_train=df_train[df_train['selection_label']==\"target\"]['customer_id_hashed'].sum()\n",
    "        nonselect_count_train=df_train[df_train['selection_label']==\"nonselect\"]['customer_id_hashed'].sum()\n",
    "        \n",
    "        target_count_train_new=df_train[(df_train['selection_label']==\"target\") & (df_train['sign_up_label']==\"new_signs\")]['customer_id_hashed'].sum()\n",
    "        target_count_train_existing=df_train[(df_train['selection_label']==\"target\") & (df_train['sign_up_label']==\"existing\")]['customer_id_hashed'].sum()\n",
    "        target_count_train_existing_sub=df_ids_train[(df_ids_train['selection_label']==\"target\") & (df_ids_train['sign_up_label']==\"existing\") & (df_ids_train['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        target_count_train_existing_unsub=df_ids_train[(df_ids_train['selection_label']==\"target\") & (df_ids_train['sign_up_label']==\"existing\") & (df_ids_train['subscription_lables']==\"unsub\")].shape[0]\n",
    "        nonselect_count_train_existing_sub=df_ids_train[(df_ids_train['selection_label']==\"nonselect\") & (df_ids_train['sign_up_label']==\"existing\") & (df_ids_train['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        nonselect_count_train_existing_unsub=df_ids_train[(df_ids_train['selection_label']==\"nonselect\") & (df_ids_train['sign_up_label']==\"existing\") & (df_ids_train['subscription_lables']==\"unsub\")].shape[0]        \n",
    "        \n",
    "        shoper_count_train_target=df_train[(df_train['selection_label']==\"target\") & (df_train['actual_shopping_label']==\"shopper\")]['customer_id_hashed'].sum()\n",
    "        shoper_count_train_nonselect=df_train[(df_train['selection_label']==\"nonselect\") & (df_train['actual_shopping_label']==\"shopper\")]['customer_id_hashed'].sum()\n",
    "\n",
    "        target_shop_rate_train=np.round(shoper_count_train_target/target_count_train,4)\n",
    "        nonselect_shop_rate_train=np.round(shoper_count_train_nonselect/nonselect_count_train,4)\n",
    "        target_shop_rate_P2W_train=np.round(count_target_ids_shoppered_past2weeks_train/target_count_train,4)\n",
    "        nonselect_shop_rate_P2W_train=np.round(count_nonselect_ids_shoppered_past2weeks_train/nonselect_count_train,4)\n",
    "        \n",
    "        # Test\n",
    "        df_test=excel_file.parse(\"test_id_summary\")\n",
    "        target_count_test=df_test[df_test['selection_label']==\"target\"]['customer_id_hashed'].sum()\n",
    "        nonselect_count_test=df_test[df_test['selection_label']==\"nonselect\"]['customer_id_hashed'].sum()\n",
    "        total_count_test=target_count_test+nonselect_count_test\n",
    "\n",
    "        target_count_test_new=df_test[(df_test['selection_label']==\"target\") & (df_test['sign_up_label']==\"new_signs\")]['customer_id_hashed'].sum()\n",
    "        target_count_test_existing=df_test[(df_test['selection_label']==\"target\") & (df_test['sign_up_label']==\"existing\")]['customer_id_hashed'].sum()\n",
    "        target_count_test_existing_sub=df_ids_test[(df_ids_test['selection_label']==\"target\") & (df_ids_test['sign_up_label']==\"existing\") &(df_ids_test['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        target_count_test_existing_unsub=df_ids_test[(df_ids_test['selection_label']==\"target\") & (df_ids_test['sign_up_label']==\"existing\") & (df_ids_test['subscription_lables']==\"unsub\")].shape[0]\n",
    "        nonselect_count_test_existing_sub=df_ids_test[(df_ids_test['selection_label']==\"nonselect\") & (df_ids_test['sign_up_label']==\"existing\") & (df_ids_test['subscription_lables']==\"default_subs\")].shape[0]\n",
    "        nonselect_count_test_existing_unsub=df_ids_test[(df_ids_test['selection_label']==\"nonselect\") & (df_ids_test['sign_up_label']==\"existing\") & (df_ids_test['subscription_lables']==\"unsub\")].shape[0]        \n",
    "        \n",
    "        \n",
    "        shoper_count_test_target=df_test[(df_test['selection_label']==\"target\") & (df_test['actual_shopping_label']==\"shopper\")]['customer_id_hashed'].sum()\n",
    "        shoper_count_test_nonselect=df_test[(df_test['selection_label']==\"nonselect\") & (df_test['actual_shopping_label']==\"shopper\")]['customer_id_hashed'].sum()\n",
    "        \n",
    "        target_shop_rate_test=np.round(shoper_count_test_target/target_count_test,4)\n",
    "        nonselect_shop_rate_test=np.round(shoper_count_test_nonselect/nonselect_count_test,4)\n",
    "        target_shop_rate_P2W_test=np.round(count_target_ids_shoppered_past2weeks_test/target_count_test,4)\n",
    "        nonselect_shop_rate_P2W_test=np.round(count_nonselect_ids_shoppered_past2weeks_test/nonselect_count_test,4)\n",
    "        \n",
    "        # Both\n",
    "        target_shop_rate_DV_both=np.round(sum([shoper_count_train_target,shoper_count_test_target])/sum([target_count_train,target_count_test]),4)\n",
    "        nonselect_shop_rate_DV_both=np.round(sum([shoper_count_train_nonselect,shoper_count_test_nonselect])/sum([nonselect_count_train,nonselect_count_test]),4)\n",
    "        target_shop_rate_P2W_both=np.round(sum([count_target_ids_shoppered_past2weeks_train,count_target_ids_shoppered_past2weeks_test])/sum([target_count_train,target_count_test]),4)\n",
    "        nonselect_shop_rate_P2W_both=np.round(sum([count_nonselect_ids_shoppered_past2weeks_train,count_nonselect_ids_shoppered_past2weeks_test])/sum([target_count_train,nonselect_count_test]),4)\n",
    "        \n",
    "        df_output_train=pd.DataFrame({\n",
    "            \"Total_Counts\":train_records,\n",
    "            \"cutoffs\":cutoff,\n",
    "            \"target_count\":target_count_train,\n",
    "            \"target_new_ids\":target_count_train_new,\n",
    "            \"past4W_new_ids_subset\":count_id_train_new_signs_past4weeks,\n",
    "            \"past4W_new_ids_subset_sub\":count_id_train_new_signs_past4weeks_sub,\n",
    "            \"past4W_new_ids_subset_unsub\":count_id_train_new_signs_past4weeks_unsub,\n",
    "            \"target_existing_ids\":target_count_train_existing,\n",
    "            \"target_existing_ids_sub\":target_count_train_existing_sub,\n",
    "            \"target_existing_ids_unsub\":target_count_train_existing_unsub,\n",
    "            \"target_shop_rate_DV\":target_shop_rate_train,\n",
    "            \"nonselect_shop_rate_DV\":nonselect_shop_rate_train,\n",
    "            \"target_shop_rate_Past2W\":target_shop_rate_P2W_train,\n",
    "            \"nonselect_shop_rate_Past2W\":nonselect_shop_rate_P2W_train,\n",
    "            'nonselect_count_existing_sub':nonselect_count_train_existing_sub,\n",
    "            'nonselect_count_existing_unsub':nonselect_count_train_existing_unsub\n",
    "        },index=[type_model]).T.rename(columns={type_model:week_end_dt}).reset_index()\n",
    "        \n",
    "        df_output_test=pd.DataFrame({\n",
    "            \"Total_Counts\":test_records,\n",
    "            \"cutoffs\":cutoff,\n",
    "            \"target_count\":target_count_test,\n",
    "            \"target_new_ids\":target_count_test_new,\n",
    "            \"past4W_new_ids_subset\":count_id_test_new_signs_past4weeks,\n",
    "            \"past4W_new_ids_subset_sub\":count_id_test_new_signs_past4weeks_sub,\n",
    "            \"past4W_new_ids_subset_unsub\":count_id_test_new_signs_past4weeks_unsub,\n",
    "            \"target_existing_ids\":target_count_test_existing,\n",
    "            \"target_existing_ids_sub\":target_count_test_existing_sub,\n",
    "            \"target_existing_ids_unsub\":target_count_test_existing_unsub,\n",
    "            \"target_shop_rate_DV\":target_shop_rate_test,\n",
    "            \"nonselect_shop_rate_DV\":nonselect_shop_rate_test,\n",
    "            \"target_shop_rate_Past2W\":target_shop_rate_P2W_test,\n",
    "            \"nonselect_shop_rate_Past2W\":nonselect_shop_rate_P2W_test,\n",
    "            'nonselect_count_existing_sub':nonselect_count_test_existing_sub,\n",
    "            'nonselect_count_existing_unsub':nonselect_count_test_existing_unsub\n",
    "        },index=[type_model]).T.rename(columns={type_model:week_end_dt}).reset_index()\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        # iterrate the cutoff of 1 trans\n",
    "        if type_model==\"trans_1_only\":\n",
    "            df_train_id_new=df_ids_train[df_ids_train['sign_up_label']==\"new_signs\"]\n",
    "            df_new_cutoff_train=pd.DataFrame()        \n",
    "            for i in range(1,6):\n",
    "                new_cutoff=np.round(cutoff/(1+i/10),4)\n",
    "                df_train_id_new['selection_label']=np.where(df_train_id_new['y_hat']>=new_cutoff,\"target\",\"nonselect\")\n",
    "                df=pd.DataFrame({week_end_dt:df_train_id_new['selection_label'].tolist().count(\"target\")},index=[\"count_new_target_at_%d0pctg_diminishing\"%i])\n",
    "                df_new_cutoff_train=df_new_cutoff_train.append(df)\n",
    "                \n",
    "            df_test_id_new=df_ids_test[df_ids_test['sign_up_label']==\"new_signs\"]\n",
    "            df_new_cutoff_test=pd.DataFrame()        \n",
    "            for i in range(1,6):\n",
    "                new_cutoff=np.round(cutoff/(1+i/10),4)\n",
    "                df_test_id_new['selection_label']=np.where(df_test_id_new['y_hat']>=new_cutoff,\"target\",\"nonselect\")\n",
    "                df=pd.DataFrame({week_end_dt:df_test_id_new['selection_label'].tolist().count(\"target\")},index=[\"count_new_target_at_%d0pctg_diminishing\"%i])\n",
    "                df_new_cutoff_test=df_new_cutoff_test.append(df)  \n",
    "            df_new_cutoff_train=df_new_cutoff_train.reset_index()\n",
    "            df_new_cutoff_test=df_new_cutoff_test.reset_index()\n",
    "            df_output_train=df_output_train.append(df_new_cutoff_train)\n",
    "            df_output_test=df_output_test.append(df_new_cutoff_test)\n",
    "            \n",
    "                \n",
    "        elif type_model==\"trans_2_plus\":\n",
    "            df_train_id_all=df_ids_train.copy()\n",
    "            df_update_cutoff_train=pd.DataFrame()        \n",
    "            for i in range(1,6):\n",
    "                updated_cutoff=np.round(cutoff/(1+i/10),4)\n",
    "                df_train_id_all['selection_label']=np.where(df_train_id_all['y_hat']>=updated_cutoff,\"target\",\"nonselect\")\n",
    "                df=pd.DataFrame({week_end_dt:df_train_id_all['selection_label'].tolist().count(\"target\")},index=[\"count_updated_target_at_%d0pctg_diminishing\"%i])\n",
    "                df_update_cutoff_train=df_update_cutoff_train.append(df)\n",
    "                \n",
    "            df_test_id_all=df_ids_test.copy()\n",
    "            df_update_cutoff_test=pd.DataFrame()        \n",
    "            for i in range(1,6):\n",
    "                updated_cutoff=np.round(cutoff/(1+i/10),4)\n",
    "                df_test_id_all['selection_label']=np.where(df_test_id_all['y_hat']>=updated_cutoff,\"target\",\"nonselect\")\n",
    "                df=pd.DataFrame({week_end_dt:df_test_id_all['selection_label'].tolist().count(\"target\")},index=[\"count_updated_target_at_%d0pctg_diminishing\"%i])\n",
    "                df_update_cutoff_test=df_update_cutoff_test.append(df)  \n",
    "            \n",
    "            df_update_cutoff_train=df_update_cutoff_train.reset_index()\n",
    "            df_update_cutoff_test=df_update_cutoff_test.reset_index()\n",
    "            \n",
    "            df_output_train=df_output_train.append(df_update_cutoff_train)\n",
    "            df_output_test=df_output_test.append(df_update_cutoff_test)\n",
    "            \n",
    "            \n",
    "        df_output_train['type_model']=type_model\n",
    "        df_output_train['DV_type']=DV_n\n",
    "        df_output_test['type_model']=type_model\n",
    "        df_output_test['DV_type']=DV_n\n",
    "        \n",
    "        df_output_train=df_output_train.set_index(['index','DV_type','type_model'])\n",
    "        df_output_test=df_output_test.set_index(['index','DV_type','type_model'])\n",
    "        \n",
    "        df_week_train=df_week_train.append(df_output_train)\n",
    "        df_week_test=df_week_test.append(df_output_test)\n",
    "        \n",
    "\n",
    "        df_output_total=pd.DataFrame({\n",
    "            \"Total_Counts\":train_records+test_records,\n",
    "            \"cutoffs\":cutoff,\n",
    "            \"target_count\":target_count_train+target_count_test,\n",
    "            \"target_new_ids\":target_count_train_new+target_count_test_new,\n",
    "            \"past4W_new_ids_subset\":count_id_train_new_signs_past4weeks+count_id_test_new_signs_past4weeks,\n",
    "            \"past4W_new_ids_subset_sub\":count_id_train_new_signs_past4weeks_sub+count_id_test_new_signs_past4weeks_sub,\n",
    "            \"past4W_new_ids_subset_unsub\":count_id_train_new_signs_past4weeks_unsub+count_id_test_new_signs_past4weeks_unsub,\n",
    "            \"target_existing_ids\":target_count_train_existing+target_count_test_existing,\n",
    "            \"target_existing_ids_sub\":target_count_train_existing_sub+target_count_test_existing_sub,\n",
    "            \"target_existing_ids_unsub\":target_count_train_existing_unsub+target_count_test_existing_unsub,\n",
    "            \"target_shop_rate_DV\":target_shop_rate_DV_both,\n",
    "            \"nonselect_shop_rate_DV\":nonselect_shop_rate_DV_both,\n",
    "            \"target_shop_rate_Past2W\":target_shop_rate_P2W_both,\n",
    "            \"nonselect_shop_rate_Past2W\":nonselect_shop_rate_P2W_both,\n",
    "            'nonselect_count_existing_sub':nonselect_count_train_existing_sub+nonselect_count_test_existing_sub,\n",
    "            'nonselect_count_existing_unsub':nonselect_count_train_existing_unsub+nonselect_count_test_existing_unsub\n",
    "        },index=[type_model]).T.rename(columns={type_model:week_end_dt}).reset_index()\n",
    "        df_output_total['type_model']=type_model\n",
    "        df_output_total['DV_type']=DV_n\n",
    "        df_output_total=df_output_total.set_index(['index','DV_type','type_model'])\n",
    "        df_week_both=df_week_both.append(df_output_total)\n",
    "        \n",
    "    \n",
    "    if len(df_output_train_summary)==0:\n",
    "        df_output_train_summary=df_week_train\n",
    "    else:\n",
    "        df_output_train_summary=pd.concat([df_output_train_summary,df_week_train],axis=1)\n",
    "        \n",
    "    if len(df_output_test_summary)==0:\n",
    "        df_output_test_summary=df_week_test\n",
    "    else:\n",
    "        df_output_test_summary=pd.concat([df_output_test_summary,df_week_test],axis=1)\n",
    "        \n",
    "    if len(df_output_both_summary)==0:\n",
    "        df_output_both_summary=df_week_both\n",
    "    else:\n",
    "        df_output_both_summary=pd.concat([df_output_both_summary,df_week_both],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jian/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_output_train_summary=df_output_train_summary.reset_index()\n",
    "df_output_test_summary=df_output_test_summary.reset_index()\n",
    "df_output_both_summary=df_output_both_summary.reset_index()\n",
    "\n",
    "df_train_new_update=df_output_train_summary[~df_output_train_summary['index'].isin(df_output_both_summary['index'].tolist())]\n",
    "df_test_new_update=df_output_test_summary[~df_output_test_summary['index'].isin(df_output_both_summary['index'].tolist())]\n",
    "df_train_new_update=df_train_new_update.set_index(['index','DV_type','type_model'])\n",
    "df_test_new_update=df_test_new_update.set_index(['index','DV_type','type_model'])\n",
    "\n",
    "df_both_new_update=df_train_new_update+df_test_new_update\n",
    "df_both_new_update=df_both_new_update.reset_index()\n",
    "df_output_both_summary=df_output_both_summary.append(df_both_new_update)\n",
    "df_order=df_output_train_summary[['index','DV_type','type_model']]\n",
    "df_order['order']=range(df_order.shape[0])\n",
    "\n",
    "df_output_both_summary=pd.merge(df_order,df_output_both_summary,on=['index','DV_type','type_model'],how=\"outer\")\n",
    "df_output_both_summary=df_output_both_summary.sort_values(\"order\")\n",
    "del df_output_both_summary['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(\"./BL_RL_modeling_summary_JL_%s.xlsx\"%str(datetime.datetime.now().date()),engine=\"xlsxwriter\")\n",
    "df_output_both_summary.to_excel(writer,\"overall\",index=True)\n",
    "df_output_train_summary.to_excel(writer,\"train\",index=True)\n",
    "df_output_test_summary.to_excel(writer,\"test\",index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
