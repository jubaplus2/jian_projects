{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Planner_Request/Storm_Snow_20190120'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_weather_folder=\"/home/jian/Projects/Big_Lots/Weather/Json_data/daily/\"\n",
    "forecast_weather_folder=\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"\n",
    "\n",
    "actual_list=[x for x in glob.glob(actual_weather_folder+\"*.json\") if str(datetime.datetime.now().date()-datetime.timedelta(days=1)) in x]\n",
    "forecast_list=[x for x in glob.glob(actual_weather_folder+\"*.json\") if str(datetime.datetime.now().date()-datetime.timedelta(days=1)) in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_open_last_week=pd.read_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_2019-01-12.csv\",dtype=str)\n",
    "store_open_last_week=store_open_last_week[store_open_last_week['week_end_date']==\"2019-01-12\"]\n",
    "store_open_last_week['sales']=store_open_last_week['sales'].astype(float)\n",
    "store_open_last_week=store_open_last_week[store_open_last_week['sales']>0]\n",
    "store_open_last_week=store_open_last_week[store_open_last_week['location_id']!=\"6990\"]\n",
    "\n",
    "len(store_open_last_week['location_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>zip_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>30906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>43402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id zip_cd\n",
       "0           3  30906\n",
       "1          30  43402"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_list_info=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190101-135843-638.txt\",dtype=str,sep=\"|\")\n",
    "store_list_info=store_list_info[store_list_info['location_id'].isin(store_open_last_week['location_id'].tolist())]\n",
    "store_list_info=store_list_info[['location_id','zip_cd']]\n",
    "store_list_info['zip_cd']=store_list_info['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "store_list_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_list=[]\n",
    "main_list=[]\n",
    "i_counters=0\n",
    "for json_actual in actual_list:\n",
    "    record_time=json_actual.split(\"/\")[len(json_actual.split(\"/\"))-1].split(\".\")[0]\n",
    "    respons=json.load(open(json_actual,\"r\"))\n",
    "    df_1_time_point=pd.DataFrame()\n",
    "    for zip_cd in store_list_info['zip_cd'].unique().tolist():\n",
    "        try:\n",
    "            data_for_a_zip=respons[zip_cd]\n",
    "            weather_main=data_for_a_zip['weather']\n",
    "            zip_weather_list=[]\n",
    "            for j in range(len(weather_main)):\n",
    "                zip_weather_list=zip_weather_list+[weather_main[j]['main']]\n",
    "            \n",
    "            df=pd.DataFrame({\"zip_cd\":zip_cd,\"Actual_\"+record_time:[zip_weather_list]},index=[i_counters])\n",
    "            i_counters+=1\n",
    "            df_1_time_point=df_1_time_point.append(df)\n",
    "        except:\n",
    "            empty_list=empty_list+[zip_cd]\n",
    "            df=pd.DataFrame({\"zip_cd\":zip_cd,\"Actual_\"+record_time:['Not_Recorded']},index=[i_counters])\n",
    "            i_counters+=1\n",
    "            df_1_time_point=df_1_time_point.append(df)\n",
    "            \n",
    "            \n",
    "    store_list_info=pd.merge(store_list_info,df_1_time_point,on=\"zip_cd\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_pred_json=json.load(open('/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/2019-01-20: 17.json',\"r\"))\n",
    "\n",
    "forecast_df=pd.DataFrame()\n",
    "i_counter=0\n",
    "empty_list=[]\n",
    "df_forecast=pd.DataFrame()\n",
    "for zip_cd in store_list_info['zip_cd'].unique().tolist():\n",
    "    try:\n",
    "        data_for_a_zip=latest_pred_json[zip_cd]\n",
    "        df=pd.DataFrame({\"zip_cd\":zip_cd},index=[i_counter])\n",
    "        i_counters+=1\n",
    "        \n",
    "        for i in range(40):\n",
    "            time_point_data=data_for_a_zip['list'][i]\n",
    "            dt_txt=datetime.datetime.utcfromtimestamp(time_point_data['dt']-60*60*5).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            weather_main=time_point_data['weather']\n",
    "            zip_weather_list=[]\n",
    "            for j in range(len(weather_main)):\n",
    "                zip_weather_list=zip_weather_list+[weather_main[j]['main']]\n",
    "                df[\"Predict_\"+dt_txt]=[zip_weather_list]\n",
    "        df_forecast=df_forecast.append(df)\n",
    "    except:\n",
    "        empty_list=empty_list+[zip_cd]\n",
    "        df=pd.DataFrame({\"zip_cd\":zip_cd},index=[i_counter])\n",
    "        i_counters+=1\n",
    "        df_forecast=df_forecast.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_list_info=pd.merge(store_list_info,df_forecast,on=\"zip_cd\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list_info['Snow_Sunday_Monday']=store_list_info['Actual_2019-01-20: 11']+store_list_info['Actual_2019-01-20: 14']+store_list_info['Actual_2019-01-20: 17']+\\\n",
    "                                        store_list_info['Predict_2019-01-20 19:00:00']+store_list_info['Predict_2019-01-20 22:00:00']+\\\n",
    "                                        store_list_info['Predict_2019-01-21 01:00:00']+store_list_info['Predict_2019-01-21 04:00:00']+\\\n",
    "                                        store_list_info['Predict_2019-01-21 07:00:00']+store_list_info['Predict_2019-01-21 10:00:00']+\\\n",
    "                                        store_list_info['Predict_2019-01-21 13:00:00']+store_list_info['Predict_2019-01-21 16:00:00']+\\\n",
    "                                        store_list_info['Predict_2019-01-21 19:00:00']+store_list_info['Predict_2019-01-21 22:00:00']\n",
    "store_list_info['Snow_Sunday_Monday']=store_list_info['Snow_Sunday_Monday'].fillna(\"Not_Recorded\")                    \n",
    "store_list_info['Snow_Sunday_Monday']=store_list_info['Snow_Sunday_Monday'].apply(lambda x: set(x))\n",
    "\n",
    "store_list_Snow_SundayMonday=store_list_info[store_list_info['Snow_Sunday_Monday'].apply(lambda x: \"Snow\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_impacted_T=df_impacted[df_impacted[\\'revenue_flag\\']==\"T\"]\\ndf_impacted_T_Not_Others_T=df_impacted_T[~df_impacted_T[\\'zip\\'].isin(df_Not_impacted[\"zip\"].unique().tolist())]\\n\\ndf_PST=df_Not_impacted_P_S.append(df_impacted_T_Not_Others_T)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_level_zips=pd.read_csv(\"/home/jian/Projects/Big_Lots/New_TA/zips_in_new_ta/sales_by_zip (Store level).csv\",dtype=str)\n",
    "\n",
    "store_level_zips=store_level_zips[['location_id','revenue_flag','zip']].drop_duplicates()\n",
    "\n",
    "store_level_zips=store_level_zips[store_level_zips['location_id'].isin(store_open_last_week['location_id'].unique().tolist())]\n",
    "\n",
    "df_impacted=store_level_zips[store_level_zips['location_id'].isin(store_list_Snow_SundayMonday['location_id'])]\n",
    "df_Not_impacted=store_level_zips[~store_level_zips['location_id'].isin(store_list_Snow_SundayMonday['location_id'])]\n",
    "\n",
    "df_impacted_P_S=df_impacted[df_impacted['revenue_flag'].isin([\"P\",\"S\"])]\n",
    "df_Not_impacted_P_S=df_Not_impacted[df_Not_impacted['revenue_flag'].isin([\"P\",\"S\"])]\n",
    "\n",
    "df_impacted_P_S_Not_Others_PS=df_impacted_P_S[~df_impacted_P_S['zip'].isin(df_Not_impacted_P_S[\"zip\"].unique().tolist())]\n",
    "'''\n",
    "df_impacted_T=df_impacted[df_impacted['revenue_flag']==\"T\"]\n",
    "df_impacted_T_Not_Others_T=df_impacted_T[~df_impacted_T['zip'].isin(df_Not_impacted[\"zip\"].unique().tolist())]\n",
    "\n",
    "df_PST=df_Not_impacted_P_S.append(df_impacted_T_Not_Others_T)\n",
    "'''\n",
    "# No need to change in this way because the whole TA mostly consider P/S zips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 10 miles zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_list_org=pd.read_table(\"/home/jian/BigLots/static_files/Store_list/MediaStormStores20190101-135843-638.txt\",dtype=str,sep=\"|\")\n",
    "\n",
    "\n",
    "new_store_list=[x for x in store_list_Snow_SundayMonday['location_id'].tolist() if x not in df_impacted_P_S_Not_Others_PS['location_id'].unique().tolist()]\n",
    "len(new_store_list)\n",
    "\n",
    "new_store_list_lat_lng=store_list_org[store_list_org['location_id'].isin(new_store_list)][['location_id','latitude_meas','longitude_meas']]\n",
    "new_store_list_lat_lng['latitude_meas']=new_store_list_lat_lng['latitude_meas'].astype(float)\n",
    "new_store_list_lat_lng['longitude_meas']=new_store_list_lat_lng['longitude_meas'].astype(float)\n",
    "new_store_list_lat_lng=new_store_list_lat_lng.reset_index()\n",
    "del new_store_list_lat_lng['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 3)\n",
      "(523, 3)\n"
     ]
    }
   ],
   "source": [
    "zip_centers=json.load(open(\"/home/jian/Docs/Geo_mapping/center_of_rentrak_zip.json\",\"r\"))\n",
    "from haversine import haversine\n",
    "all_new_store_zips_df=pd.DataFrame()\n",
    "for i in range(len(new_store_list_lat_lng)):\n",
    "    location=new_store_list_lat_lng['location_id'][i]\n",
    "    store_center=[new_store_list_lat_lng['latitude_meas'][i],new_store_list_lat_lng['longitude_meas'][i]]\n",
    "    store_df=pd.DataFrame()\n",
    "    for zip_cd in zip_centers.keys():\n",
    "        dist=haversine(zip_centers[zip_cd],store_center,miles=True)\n",
    "        if dist<=10:\n",
    "            df=pd.DataFrame({\"location_id\":location,\"zip\":zip_cd,\"miles\":dist},index=[0])\n",
    "            store_df=store_df.append(df)\n",
    "            \n",
    "    all_new_store_zips_df=all_new_store_zips_df.append(store_df)\n",
    "    \n",
    "print(all_new_store_zips_df.shape)\n",
    "all_new_store_zips_df_not_in_Others_PS=all_new_store_zips_df[~all_new_store_zips_df['zip'].isin(df_Not_impacted_P_S[\"zip\"].unique().tolist())]\n",
    "print(all_new_store_zips_df_not_in_Others_PS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_zip_exclude_unque=df_impacted_P_S_Not_Others_PS[['zip']].append(all_new_store_zips_df_not_in_Others_PS[['zip']]).drop_duplicates()\n",
    "\n",
    "del store_list_org['zip_cd']\n",
    "store_list_info=pd.merge(store_list_info,store_list_org,on=\"location_id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(\"/home/jian/Projects/Big_Lots/Analysis/2019_Q1/Planner_Request/Storm_Snow_20190120/BL_Zips_for_Snow_20190120-21_PS_newstore_10_miles_JL_\"+str(datetime.datetime.now().date())+\".xlsx\",engine=\"xlsxwriter\")\n",
    "\n",
    "df_zip_exclude_unque.to_excel(writer,\"Zips_Influced_to_Suppress\",index=False)\n",
    "\n",
    "store_list_Snow_SundayMonday=store_list_Snow_SundayMonday[['location_id','zip_cd','Snow_Sunday_Monday']+[x for x in store_list_Snow_SundayMonday.columns.tolist() if x not in ['location_id','zip_cd','Snow_Sunday_Monday']]]\n",
    "store_list_Snow_SundayMonday.to_excel(writer,\"stores_impacted\",index=False)\n",
    "\n",
    "df_impacted_P_S_Not_Others_PS.to_excel(writer,\"zips_by_store_PS\",index=False)\n",
    "all_new_store_zips_df_not_in_Others_PS.to_excel(writer,\"zips_new_stores_suppress\",index=False)\n",
    "all_new_store_zips_df.to_excel(writer,\"zips_new_stores_all\",index=False)\n",
    "store_list_info.to_excel(writer,\"all_open_store_weather\",index=False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
