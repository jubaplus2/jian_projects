{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biglots_weekly():\n",
    "# coding: utf-8\n",
    "# Weekly run, valid before Saturday\n",
    "# Directly Export from Jupyter\n",
    "    print(\"start celery\")\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import datetime\n",
    "    import paramiko\n",
    "    import glob\n",
    "    import logging\n",
    "    import gc\n",
    "    import smtplib\n",
    "    folderpath = '/home/jian/BiglotsCode/outputs/'\n",
    "    lastweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+10))\n",
    "    \n",
    "    Tuesday_StampDate_Str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1))\n",
    "    Tuesday_today_str =Tuesday_StampDate_Str[0:4]+Tuesday_StampDate_Str[5:7]+Tuesday_StampDate_Str[8:10]\n",
    "    thisweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3))\n",
    "    \n",
    "    today_str=str(datetime.datetime.now().date())\n",
    "    logging.basicConfig(filename='celery.log', level=logging.INFO)\n",
    "    \n",
    "    \n",
    "    # In[2]:\n",
    "    \n",
    "    recent_weekly_data_folder=\"/home/jian/BigLots/MediaStorm_\"+thisweeksdate+\"/\"\n",
    "    Simeng_recent_weekly_data_folder=\"/home/simeng/outputs_\"+thisweeksdate+\"/\"\n",
    "\n",
    "    Saturday_str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3-7))\n",
    "    \n",
    "    if not os.path.exists(\"/home/jian/BiglotsCode/outputs/Output_\"+thisweeksdate+\"/By_Zip_weather_forecast_for_Saturday_\"+Saturday_str+\".csv\"):\n",
    "        del Saturday_str\n",
    "    \n",
    "        try:\n",
    "            os.stat(recent_weekly_data_folder)\n",
    "        except:\n",
    "            os.mkdir(recent_weekly_data_folder)\n",
    "            \n",
    "        try:\n",
    "            os.stat(Simeng_recent_weekly_data_folder)\n",
    "        except:\n",
    "            os.mkdir(Simeng_recent_weekly_data_folder)\n",
    "        \n",
    "        logging.info(str(datetime.datetime.now())+\": Start Running\")\n",
    "        # In[3]:\n",
    "        \n",
    "        host = \"64.237.51.251\" #hard-coded\n",
    "        port = 22\n",
    "        transport = paramiko.Transport((host, port))\n",
    "         \n",
    "        password = \"bwRi3V6fgZsfJrMl\" #hard-coded\n",
    "        username = \"client\" #hard-coded\n",
    "        transport.connect(username = username, password = password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        \n",
    "        Client_Today_STR=str(datetime.datetime.now().date())\n",
    "        Client_Today_NUM_STR =Client_Today_STR[0:4]+Client_Today_STR[5:7]+Client_Today_STR[8:10]\n",
    "        \n",
    "        new_weekly_file_list=sftp.listdir(\"/mnt/drv5/biglots_data/\")\n",
    "        new_weekly_file_list=[\"/mnt/drv5/biglots_data/\"+x for x in new_weekly_file_list if Client_Today_NUM_STR in x]\n",
    "        \n",
    "        for new_weekly_file in new_weekly_file_list:\n",
    "            localpath=recent_weekly_data_folder+new_weekly_file.split(\"/\")[len(new_weekly_file.split(\"/\"))-1]\n",
    "            try:\n",
    "                os.stat(localpath)\n",
    "            except:\n",
    "                sftp.get(new_weekly_file,localpath)\n",
    "        \n",
    "        sftp.close()\n",
    "        logging.info(str(datetime.datetime.now())+\": Finished copying\")\n",
    "        # In[4]:\n",
    "        \n",
    "        newsalespath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormSalesWeekly' in x][0]\n",
    "        newtrafficpath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormTrafficWeekly' in x][0]\n",
    "        newinventorypath = [x for x in glob.glob(recent_weekly_data_folder+\"*.txt\") if 'MediaStormInventoryWeekly' in x][0]\n",
    "        \n",
    "        \n",
    "        # In[5]:\n",
    "        \n",
    "        closed_onlinestorelist = ['6990','145']\n",
    "        \n",
    "        \n",
    "        # In[6]:\n",
    "        \n",
    "        df_tradearea_all = pd.read_csv('/home/jian/BiglotsCode/OtherInput/New_TA_info.csv',dtype = 'str')\n",
    "        df_tradearea_all['trade_area_code']=df_tradearea_all['Ta_Info'].apply(lambda x: x.split(\" | \")[0])\n",
    "        df_tradearea_all=df_tradearea_all[['location_id','trade_area_code']]\n",
    "        \n",
    "        \n",
    "        # In[7]:\n",
    "        \n",
    "        dfsales = pd.read_csv(folderpath + 'combinedsales'+ lastweeksdate + '.csv',sep = '|',dtype = 'str')\n",
    "        df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "        a = df.columns\n",
    "        print(\"new sales data column header matches:\")\n",
    "        print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "               'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "        logging.info(\"new sales data column header matches:\")\n",
    "        logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "               'gross_transaction_cnt', 'class_code_id', 'subclass_id', 'subclass_gross_sales_amt'])\n",
    "        '''print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_sales_amt',\n",
    "               'gross_transaction_cnt', 'class_code_id', 'class_gross_sales_amt'])'''\n",
    "        df['subclass_gross_sales_amt']=df['subclass_gross_sales_amt'].astype(float)\n",
    "        df=df.groupby(['location_id','week_end_dt','fiscal_week_nbr','gross_sales_amt','gross_transaction_cnt','class_code_id'])['subclass_gross_sales_amt'].sum().to_frame().reset_index()\n",
    "        df=df.rename(columns={\"subclass_gross_sales_amt\":\"class_gross_sales_amt\"})\n",
    "        \n",
    "        \n",
    "        \n",
    "        dfsales = dfsales.append(df,ignore_index = True)\n",
    "        a = (len(dfsales.index))\n",
    "        dfsales = dfsales.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "               'class_code_id'])\n",
    "        b = (len(dfsales.index))\n",
    "        if a==b:\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(\"last week traffic data duplication deduped\")\n",
    "        \n",
    "        \n",
    "        # In[8]:\n",
    "        \n",
    "        recentweek = (max(dfsales['week_end_dt']))\n",
    "        recentweek\n",
    "        \n",
    "        \n",
    "        # In[9]:\n",
    "        \n",
    "        lastweeksdate\n",
    "        \n",
    "        \n",
    "        # In[10]:\n",
    "        \n",
    "        dfsales.to_csv(folderpath + 'combinedsales'+ recentweek + '.csv',index = False,sep = '|')\n",
    "        \n",
    "        \n",
    "        # In[11]:\n",
    "        \n",
    "        dfsales = dfsales[~dfsales['location_id'].isin(closed_onlinestorelist)]\n",
    "        \n",
    "        \n",
    "        # In[12]:\n",
    "        \n",
    "        outputpath = folderpath +'Output_' + recentweek +'/'\n",
    "        try:\n",
    "            os.stat(outputpath)\n",
    "        except:\n",
    "            os.mkdir(outputpath)\n",
    "        \n",
    "        \n",
    "        # In[13]:\n",
    "        \n",
    "        dfnodata = dfsales[(dfsales['class_gross_sales_amt'] == '?')&                   (dfsales['week_end_dt'] == recentweek)]\n",
    "        # dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "        print(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))\n",
    "        logging.info(\"stores with ? sales/transaction: \" + str(len(dfnodata.index)))\n",
    "        \n",
    "        \n",
    "        # In[14]:\n",
    "        \n",
    "        dfsales['week_end_dt'] = pd.to_datetime(dfsales['week_end_dt'])\n",
    "        dfsales = dfsales[dfsales['class_gross_sales_amt']!='?']\n",
    "        dfsales = dfsales.reset_index(drop = True)\n",
    "        \n",
    "        dfsales['gross_sales_amt'] = dfsales['gross_sales_amt'].astype('float')\n",
    "        dfsales['gross_transaction_cnt'] = dfsales['gross_transaction_cnt'].astype('float')\n",
    "        dfsales['class_gross_sales_amt'] = dfsales['class_gross_sales_amt'].astype('float')\n",
    "        \n",
    "        \n",
    "        # In[15]:\n",
    "        \n",
    "        dfweeklist = dfsales[['week_end_dt','fiscal_week_nbr']].drop_duplicates()\n",
    "        dfweeklist = dfweeklist.sort_values('week_end_dt',ascending = False)\n",
    "        dfweeklist.reset_index(drop = True,inplace = True)\n",
    "        dfweeklist.reset_index(inplace = True)\n",
    "        \n",
    "        dfweeklist_wow = dfweeklist.copy()\n",
    "        dfweeklist_wow['index'] = dfweeklist_wow['index'] - 1\n",
    "        dfweeklist_wow = dfweeklist_wow[['index','week_end_dt']]\n",
    "        dfweeklist_wow.columns = ['index','weeklastweek']\n",
    "        \n",
    "        dfweeklist = dfweeklist[dfweeklist['index']<104]\n",
    "        dfweeklist.reset_index(drop = True,inplace = True)\n",
    "        dfweeklist['year'] = np.ceil((dfweeklist['index'] + 1)/52)\n",
    "        \n",
    "        dfweeklist1 = dfweeklist[dfweeklist['year'] == 1]\n",
    "        dfweeklist1 = dfweeklist1[['index', 'week_end_dt', 'fiscal_week_nbr']]\n",
    "        dfweeklist2 = dfweeklist[dfweeklist['year'] == 2]\n",
    "        dfweeklist2 = dfweeklist2[['week_end_dt', 'fiscal_week_nbr']]\n",
    "        dfweeklist2.columns = ['weeklastyear', 'fiscal_week_nbr']\n",
    "        dfweeklist1['rank']=dfweeklist1['week_end_dt'].rank(ascending=True)\n",
    "        dfweeklist2['rank']=dfweeklist2['weeklastyear'].rank(ascending=True)\n",
    "        \n",
    "        del dfweeklist2['fiscal_week_nbr']\n",
    "        \n",
    "        dfweeklist = pd.merge(dfweeklist1,dfweeklist2,on ='rank' )\n",
    "        dfweeklist = pd.merge(dfweeklist,dfweeklist_wow,on ='index')\n",
    "        del dfweeklist1,dfweeklist2,dfweeklist_wow\n",
    "        \n",
    "        dfweeklist.to_csv(outputpath + 'weeklist.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[16]:\n",
    "        \n",
    "        recentweek_date = (max(dfsales['week_end_dt']))\n",
    "        \n",
    "        \n",
    "        # In[17]:\n",
    "        \n",
    "        dfcheck = dfsales[dfsales['week_end_dt'] == recentweek_date]\n",
    "        \n",
    "        \n",
    "        # In[18]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # In[19]:\n",
    "        \n",
    "        dfcheck_total1 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                                 'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "        a = (len(dfcheck_total1.index))\n",
    "        dfcheck_total1 = dfcheck_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "        b = (len(dfcheck_total1.index))\n",
    "        if a==b:\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(\"last week sales multiple gross sales/trasaction in the same store\")\n",
    "        \n",
    "        dfcheck_total2 = dfcheck[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                                 'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "        dfcheck_total2.reset_index(inplace = True)\n",
    "        \n",
    "        dfcheck_total = pd.merge(dfcheck_total1,dfcheck_total2,\n",
    "                                on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                                how = 'outer')\n",
    "        \n",
    "        del dfcheck_total1,dfcheck_total2\n",
    "        \n",
    "        dfcheck_zero = dfcheck_total[(dfcheck_total['class_gross_sales_amt']<=0)|                            (dfcheck_total['gross_transaction_cnt']<=0) ]\n",
    "        \n",
    "        dfcheck_zero.to_csv(outputpath + 'zerosales.csv',index = False)\n",
    "        print(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "        logging.info(\"stores with zero sales/transaction: \" + str(len(dfcheck_zero.index)))\n",
    "            \n",
    "            \n",
    "        del dfcheck_zero\n",
    "        \n",
    "        dfcheck_total['TotalDiff'] = dfcheck_total['gross_sales_amt']-dfcheck_total['class_gross_sales_amt']\n",
    "        dfcheck_total['TotalDiff'] = dfcheck_total['TotalDiff'].round()\n",
    "        dfcheck_totalnonmatch = dfcheck_total[dfcheck_total['TotalDiff']!=0]\n",
    "        print(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "        logging.info(\"stores gross sales can not match sum of class sales: \" + str(len(dfcheck_totalnonmatch.index)))\n",
    "        \n",
    "        dfcheck_totalnonmatch.to_csv(outputpath + 'totalnonmatch.csv',index = False)\n",
    "        del dfcheck_totalnonmatch\n",
    "        \n",
    "        dfcheck_zeroclass = dfcheck[(dfcheck['class_gross_sales_amt']==0)]\n",
    "        dfcheck_zeroclass.to_csv(outputpath + 'zeroclasssales.csv',index = False)\n",
    "        print(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "        logging.info(\"stores with zero class sales: \" + str(len(dfcheck_zeroclass.index)))\n",
    "        del dfcheck_zeroclass\n",
    "        del dfcheck\n",
    "        \n",
    "        \n",
    "        # In[20]:\n",
    "        \n",
    "        dfsales_total1 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                                 'gross_sales_amt','gross_transaction_cnt']].drop_duplicates()\n",
    "        dfsales_total1 = dfsales_total1.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "        \n",
    "        dfsales_total2 = dfsales[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                                 'class_gross_sales_amt']].groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "        dfsales_total2.reset_index(inplace = True)\n",
    "        \n",
    "        dfsales_total = pd.merge(dfsales_total1,dfsales_total2,\n",
    "                                on = ['location_id', 'week_end_dt', 'fiscal_week_nbr'] ,\n",
    "                                how = 'outer')\n",
    "        del dfsales_total1,dfsales_total2\n",
    "        \n",
    "        \n",
    "        # In[21]:\n",
    "        \n",
    "        dfstore = pd.read_table('/home/jian/BigLots/static_files/MediaStormStoreList_Nov15.txt',\n",
    "                                sep = '|',dtype = 'str')\n",
    "        dfstore['open_dt'] = pd.to_datetime(dfstore['open_dt'])\n",
    "        dfstore['open_dtwd'] = dfstore['open_dt'].dt.dayofweek\n",
    "        dfstore['open_wk'] = np.where(dfstore['open_dtwd']<=5,\n",
    "                               dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(5-x.weekday()))),\n",
    "                               dfstore['open_dt'].apply(lambda x:x+datetime.timedelta(days=(12-x.weekday()))))\n",
    "        \n",
    "        dma = pd.read_csv('/home/jian/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')\n",
    "        dfstore_exc = dfstore\n",
    "        dfstore_exc['zip_cd'] = dfstore_exc['zip_cd'].str[0:5]\n",
    "        dfstore_exc = pd.merge(dfstore_exc,dma,on = 'zip_cd',how = 'left')\n",
    "        \n",
    "        \n",
    "        # In[22]:\n",
    "        \n",
    "        dfstorematch = dfsales_total[['location_id']].drop_duplicates()\n",
    "        dfstorematch = pd.merge(dfstorematch,dfstore[['location_id','address_line_1']],\n",
    "                                on = 'location_id',how = 'left')\n",
    "        dfstorematch['address_line_1'].fillna('empty',inplace = True)\n",
    "        dfstorematch = dfstorematch[dfstorematch['address_line_1']=='empty']\n",
    "        print(\"stores w/o detailed info: \")\n",
    "        print(dfstorematch['location_id'].unique())\n",
    "        logging.info(\"stores w/o detailed info: \")\n",
    "        logging.info(dfstorematch['location_id'].unique())\n",
    "        \n",
    "        \n",
    "        # In[23]:\n",
    "        \n",
    "        len(dfstorematch['location_id'].unique())\n",
    "        \n",
    "        \n",
    "        # In[24]:\n",
    "        \n",
    "        old_stores=pd.read_excel(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",sheetname=\"old_stores\",dtype=str)\n",
    "        new_stores=pd.read_excel(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",sheetname=\"new_stores\",dtype=str)\n",
    "        \n",
    "        \n",
    "        last_week_closed_stores=old_stores['closed_store'].tolist()+new_stores['location_id'].tolist()\n",
    "        '''\n",
    "        ['61','290','455','1084','1230','1422','1550','1750','4479','5098','5177','824','5133','4099',\n",
    "                                 '4113','4165','4280','4362','1913','1967','1148','1182','280','388','507','5363','4675','5364','4677']\n",
    "        \n",
    "        # 5363 2018-04-28\n",
    "        # 4675 2018-06-02 not updated about TA info\n",
    "        # 5364 2018-06-16 not updated about TA info\n",
    "        # 4677 2018-06-23 not updated about TA info\n",
    "        # 5367 2018-07-28 not updated about TA info\n",
    "        '''\n",
    "        sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores)\n",
    "        logging.info(\"No New Stores: \"+ str(sorted(dfstorematch['location_id'].unique().tolist())==sorted(last_week_closed_stores)))\n",
    "        \n",
    "        \n",
    "        # In[25]:\n",
    "        \n",
    "        this_week_new_store_list=[x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores]\n",
    "        logging.info(\"New Stores:\")\n",
    "        logging.info([x for x in dfstorematch['location_id'].unique().tolist() if x not in last_week_closed_stores])\n",
    "        \n",
    "        new_stores_df=pd.DataFrame({\"location_id\":this_week_new_store_list,\"first_week_in_data\":[recentweek_date]*len(this_week_new_store_list)},index=[x for x in range(len(this_week_new_store_list))])\n",
    "        new_stores=new_stores.append(new_stores_df)\n",
    "        \n",
    "        writer=pd.ExcelWriter(\"/home/jian/BiglotsCode/OtherInput/BL_close_and_new_stores_20180801.xlsx\",engine='xlsxwriter')\n",
    "        old_stores.to_excel(writer,'old_stores',index=False)\n",
    "        new_stores.to_excel(writer,'new_stores',index=False)\n",
    "        writer.save()\n",
    "        # In[ ]:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # In[26]:\n",
    "        \n",
    "        len(dfstorematch['location_id'].unique())\n",
    "        del dfstorematch\n",
    "        \n",
    "        \n",
    "        # In[27]:\n",
    "        \n",
    "        dfsales_total = pd.merge(dfsales_total,dfstore[['location_id','open_wk']],\n",
    "                                on = 'location_id',how = 'left')\n",
    "        dfsales_total['open_wk'].fillna(datetime.datetime.strptime(str(20200101), '%Y%m%d').date(),inplace = True)\n",
    "        \n",
    "        \n",
    "        # In[28]:\n",
    "        \n",
    "        dftraffic = pd.read_csv(folderpath + 'combinedtraffic'+ lastweeksdate + '.csv',\n",
    "                       sep = '|',dtype = 'str')\n",
    "        \n",
    "        df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "        a = df.columns\n",
    "        print(\"new traffic data column header matches:\")\n",
    "        print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "               'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "               'traffic_day_6', 'traffic_day_7'])\n",
    "        logging.info(\"new traffic data column header matches:\")\n",
    "        logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'traffic_day_1',\n",
    "               'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "               'traffic_day_6', 'traffic_day_7'])\n",
    "            \n",
    "        dftraffic = dftraffic.append(df,ignore_index = True)\n",
    "        a = (len(dftraffic.index))\n",
    "        dftraffic = dftraffic.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr'])\n",
    "        b = (len(dftraffic.index))\n",
    "        if a==b:\n",
    "            print(\"\")\n",
    "            logging.info(\"\")\n",
    "        else:\n",
    "            print(\"last week traffic data duplication deduped\")\n",
    "            logging.info(\"last week traffic data duplication deduped\")\n",
    "        dftraffic.to_csv(folderpath + 'combinedtraffic'+ recentweek + '.csv',index = False,sep = '|')\n",
    "        \n",
    "        dftraffic['traffic_week'] = 0 \n",
    "        for i in ['traffic_day_1','traffic_day_2', 'traffic_day_3', 'traffic_day_4',\n",
    "                  'traffic_day_5', 'traffic_day_6', 'traffic_day_7']:\n",
    "            dftraffic[i] = dftraffic[i].astype('float')\n",
    "            dftraffic['traffic_week'] = dftraffic['traffic_week'] +dftraffic[i]\n",
    "        dftraffic['week_end_dt'] = pd.to_datetime(dftraffic['week_end_dt'])\n",
    "        \n",
    "        \n",
    "        # In[29]:\n",
    "        \n",
    "        dfinventory = pd.read_csv(folderpath + 'combinedinventory'+ lastweeksdate + '.csv',\n",
    "                       sep = '|',dtype = 'str')\n",
    "        \n",
    "        df = pd.read_csv(newinventorypath,sep = '|',dtype = 'str')\n",
    "        a = df.columns\n",
    "        print(\"new inventory data column header matches:\")\n",
    "        logging.info(\"new inventory data column header matches:\")\n",
    "        print(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "        logging.info(a == ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand'])\n",
    "        df.columns = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id','on_hand']\n",
    "        dfinventory = dfinventory.append(df,ignore_index = True)\n",
    "        a = (len(dfinventory.index))\n",
    "        dfinventory = dfinventory.drop_duplicates(['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_code_id'])\n",
    "        b = (len(dfinventory.index))\n",
    "        if a==b:\n",
    "            print(\"\")\n",
    "            logging.info(\"\")\n",
    "        else:\n",
    "            print(\"last week inventory data duplication deduped\")\n",
    "            logging.info(\"last week inventory data duplication deduped\")\n",
    "        \n",
    "        \n",
    "        # In[30]:\n",
    "        \n",
    "        dfinventory.to_csv(folderpath + 'combinedinventory'+ recentweek + '.csv',index = False,sep = '|')\n",
    "        \n",
    "        \n",
    "        # In[31]:\n",
    "        \n",
    "        dfinventory['week_end_dt'] = pd.to_datetime(dfinventory['week_end_dt'])\n",
    "        dfinventory['on_hand'] = dfinventory['on_hand'].astype('float')\n",
    "        dfinventory_total = dfinventory.groupby(['location_id', 'week_end_dt', 'fiscal_week_nbr']).sum()\n",
    "        dfinventory_total.reset_index(inplace = True)\n",
    "        \n",
    "        \n",
    "        # In[32]:\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total,\n",
    "                                        dfweeklist[['week_end_dt', 'fiscal_week_nbr','weeklastyear','weeklastweek']],\n",
    "                                        on= ['week_end_dt', 'fiscal_week_nbr'])\n",
    "        \n",
    "        dfsales_total_lastyear = pd.merge(dfsales_total,\n",
    "                                         dfweeklist[['weeklastyear']],\n",
    "                                         left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "        \n",
    "        dfsales_total_lastyear = dfsales_total_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "        dfsales_total_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastyear,\n",
    "                                        on = ['location_id','weeklastyear'],how = 'outer')\n",
    "        dfsales_total_recent.fillna(0,inplace = True)\n",
    "        \n",
    "        dfsales_total_recent['Store_Category'] = np.where(dfsales_total_recent['open_wk']>=dfsales_total_recent['weeklastyear'],'New',\n",
    "                                                 np.where((dfsales_total_recent['gross_transaction_cnt_ly']==0)&(dfsales_total_recent['class_gross_sales_amt_ly']==0),\n",
    "                                                 'Converted',\n",
    "                                                 np.where((dfsales_total_recent['gross_transaction_cnt']==0)&(dfsales_total_recent['class_gross_sales_amt']==0),\n",
    "                                                'Converted','Complete')))\n",
    "        \n",
    "        dfsales_total_lastweek = pd.merge(dfsales_total,\n",
    "                                         dfweeklist[['weeklastweek']],\n",
    "                                         left_on= 'week_end_dt',right_on = 'weeklastweek')\n",
    "        \n",
    "        dfsales_total_lastweek = dfsales_total_lastweek[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastweek']]\n",
    "        dfsales_total_lastweek.columns = ['location_id','gross_transaction_cnt_lw', 'class_gross_sales_amt_lw','weeklastweek']\n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dfsales_total_lastweek,\n",
    "                                        on = ['location_id','weeklastweek'],how = 'left')\n",
    "        dfsales_total_recent.fillna(0,inplace = True)\n",
    "        \n",
    "        dfsales_total_recent['week_end_dt'] = np.where(dfsales_total_recent['week_end_dt']=='1970-01-01',\n",
    "                                               dfsales_total_recent['weeklastyear'] + pd.DateOffset(364),\n",
    "                                               dfsales_total_recent['week_end_dt'])\n",
    "        dfsales_total_recent['weeklastyear'] = np.where(dfsales_total_recent['weeklastyear']=='1970-01-01',\n",
    "                                               dfsales_total_recent['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                               dfsales_total_recent['weeklastyear'])\n",
    "        \n",
    "        \n",
    "        # In[33]:\n",
    "        \n",
    "        dfallstorelist = dfstore[~dfstore['location_id'].isin(closed_onlinestorelist)]\n",
    "        dfallstorelist.reset_index(drop = True, inplace = True)\n",
    "        dfweeklist2 = dfweeklist.copy()\n",
    "        dfallstorelist['concat'] = 1\n",
    "        dfweeklist2['concat'] = 1\n",
    "        dfallstorelist = pd.merge(dfallstorelist,dfweeklist2,on='concat')\n",
    "        dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "        dfallstorelist = pd.merge(dfallstorelist,dfsales_total_recent,on=['location_id','week_end_dt'],\n",
    "                                 how = 'left')\n",
    "        \n",
    "        dfallstorelist.fillna(0,inplace = True)\n",
    "        dfallstorelist = dfallstorelist[(dfallstorelist['gross_sales_amt']==0)&                               (dfallstorelist['gross_transaction_cnt']==0)&                               (dfallstorelist['class_gross_sales_amt']==0)&                               (dfallstorelist['gross_transaction_cnt_ly']==0)&                               (dfallstorelist['class_gross_sales_amt_ly']==0)]\n",
    "        dfallstorelist = dfallstorelist.sort_values(['week_end_dt','location_id'],ascending = [0,1])\n",
    "        dfallstorelist = dfallstorelist[['location_id','week_end_dt']]\n",
    "        \n",
    "        x_temp=dfsales_total_recent[['week_end_dt','fiscal_week_nbr']]\n",
    "        y_temp=x_temp.drop_duplicates()\n",
    "        y_temp=y_temp[y_temp['fiscal_week_nbr']!=\"0\"]\n",
    "        y_temp=y_temp[y_temp['fiscal_week_nbr']!=0]\n",
    "        dfallstorelist = pd.merge(dfallstorelist,y_temp,on=\"week_end_dt\",how=\"left\")\n",
    "        dfallstorelist = pd.merge(dfallstorelist,dfstore_exc,on=\"location_id\",how='left')\n",
    "        dfallstorelist = pd.merge(dfallstorelist,df_tradearea_all,on=\"location_id\",how='left')\n",
    "        del dfallstorelist['open_dtwd']\n",
    "        del dfallstorelist['open_wk']\n",
    "        \n",
    "        # dfallstorelist.to_csv(outputpath + 'nobothyeardatastores.csv',index = False)\n",
    "        print(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "        logging.info(\"stores with no 2017&2016 sales and transaction data: \" + str(len(dfallstorelist.index)))\n",
    "        \n",
    "        test = dfallstorelist[dfallstorelist['week_end_dt']==recentweek_date]\n",
    "        print(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "        logging.info(\"Last Week: stores with no 2017&2016 sales and transaction data: \" + str(len(test.index)))\n",
    "        \n",
    "        # del test,dfweeklist2,dfallstorelist\n",
    "        \n",
    "        \n",
    "        # For later use to add index\n",
    "        \n",
    "        Recent_52_Week_nbr=dfweeklist[['week_end_dt','index']]\n",
    "        Recent_52_Week_nbr['52_Weeks_nbr']=52-Recent_52_Week_nbr['index']\n",
    "        del Recent_52_Week_nbr['index']\n",
    "        \n",
    "        # Recent_52_Week_nbr\n",
    "        \n",
    "        \n",
    "        # In[34]:\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic,\n",
    "                                        on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "        \n",
    "        dftraffic2 = dftraffic[['location_id', 'week_end_dt','traffic_day_1',\n",
    "               'traffic_day_2', 'traffic_day_3', 'traffic_day_4', 'traffic_day_5',\n",
    "               'traffic_day_6', 'traffic_day_7','traffic_week']]\n",
    "        dftraffic2.columns = ['location_id', 'weeklastyear','traffic_day_1_ly',\n",
    "               'traffic_day_2_ly', 'traffic_day_3_ly', 'traffic_day_4_ly', 'traffic_day_5_ly',\n",
    "               'traffic_day_6_ly', 'traffic_day_7_ly','traffic_week_ly']\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dftraffic2,\n",
    "                                        on=['location_id', 'weeklastyear'],how = 'left')\n",
    "        del dftraffic2\n",
    "        \n",
    "        \n",
    "        # In[35]:\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total,\n",
    "                                        on=['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "        \n",
    "        dfinventory_total2 = dfinventory_total[['location_id', 'week_end_dt','on_hand']]\n",
    "        dfinventory_total2.columns = ['location_id', 'weeklastyear','on_hand_ly']\n",
    "        \n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,dfinventory_total2,\n",
    "                                        on=['location_id', 'weeklastyear'],how = 'left')\n",
    "        del dfinventory_total2\n",
    "        \n",
    "        \n",
    "        # In[36]:\n",
    "        \n",
    "        recentweek_last=datetime.datetime.strptime(recentweek, '%Y-%m-%d').date()\n",
    "        recentweek_last=recentweek_last+datetime.timedelta(days=(-84))\n",
    "        \n",
    "        \n",
    "        # In[37]:\n",
    "        \n",
    "        dfsales_total_recent['yoysales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_ly'] - 1\n",
    "        dfsales_total_recent['yoytrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_ly'] - 1\n",
    "        dfsales_total_recent['wowsales'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['class_gross_sales_amt_lw'] - 1\n",
    "        dfsales_total_recent['wowtrans'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['gross_transaction_cnt_lw'] - 1\n",
    "        \n",
    "        \n",
    "        # In[38]:\n",
    "        \n",
    "        dfsales_total_recent_delete = dfsales_total_recent[(dfsales_total_recent['Store_Category']=='Complete')&                                                   ((abs(dfsales_total_recent['yoysales'])>0.2)&                                                   (abs(dfsales_total_recent['yoytrans'])>0.2))]#|\\\n",
    "                                                           #(abs(dfsales_total_recent['wowsales'])>0.2)|\\\n",
    "                                                           #(abs(dfsales_total_recent['wowtrans'])>0.2))]\n",
    "        dfsales_total_recent_delete = dfsales_total_recent_delete.sort_values(['week_end_dt','location_id'],\n",
    "                                                                             ascending = [0,1])\n",
    "        \n",
    "        dfsales_total_recent_delete=pd.merge(dfsales_total_recent_delete,dfstore_exc,on=\"location_id\",how='left')\n",
    "        dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,df_tradearea_all,on='location_id',how='left')\n",
    "        \n",
    "        del dfsales_total_recent_delete['open_dtwd']\n",
    "        del dfsales_total_recent_delete['open_wk_y']\n",
    "        dfsales_total_recent_delete.rename(index=str, columns={\"open_wk_x\": \"open_wk\"})\n",
    "        \n",
    "        dfsales_total_recent_delete = pd.merge(dfsales_total_recent_delete,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "        # dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores.csv',index = False)\n",
    "        print(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "        logging.info(\"stores with high yoy change: \" + str(len(dfsales_total_recent_delete.index)))\n",
    "        test = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "        print(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "        logging.info(\"Last Week: stores with high yoy change: \" + str(len(test.index)))\n",
    "        del test\n",
    "        \n",
    "        \n",
    "        # In[39]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # In[40]:\n",
    "        \n",
    "        #dfsales_total_recent =  dfsales_total_recent[(dfsales_total_recent['gross_transaction_cnt']!=0)|\\\n",
    "        #                                             (dfsales_total_recent['class_gross_sales_amt']!=0)]\n",
    "        dfsales_total_recent = dfsales_total_recent[(dfsales_total_recent['Store_Category']!='Complete')|                                            ((dfsales_total_recent['Store_Category']=='Complete')&                                            (abs(dfsales_total_recent['yoysales'])<=0.2)|                                            (abs(dfsales_total_recent['yoytrans'])<=0.2))]#&\\\n",
    "                                                    #(abs(dfsales_total_recent['wowsales'])<=0.2)&\\\n",
    "                                                    #(abs(dfsales_total_recent['wowtrans'])<=0.2))]\n",
    "        \n",
    "        \n",
    "        # In[41]:\n",
    "        \n",
    "        dfweeklist2 = dfweeklist[['week_end_dt']]\n",
    "        dfweeklist2['week_end_dt_8w'] = dfweeklist2['week_end_dt']+pd.DateOffset(-84)\n",
    "        # Name \"week_end_dt_8w\" reflects 12 weeks, not 8\n",
    "        \n",
    "        \n",
    "        # In[42]:\n",
    "        \n",
    "        dfweeklist_12plus = dfsales[['week_end_dt']].drop_duplicates()\n",
    "        dfweeklist_12plus = dfweeklist_12plus.sort_values('week_end_dt',ascending = False)\n",
    "        dfweeklist_12plus.reset_index(drop = True,inplace = True)\n",
    "        dfweeklist_12plus.reset_index(inplace = True)\n",
    "        dfweeklist_12plus = dfweeklist_12plus[dfweeklist_12plus['index']<64]\n",
    "        dfweeklist_12plus['weeklastyear'] = dfweeklist_12plus['week_end_dt'] + pd.DateOffset(-364)\n",
    "        \n",
    "        \n",
    "        # In[43]:\n",
    "        \n",
    "        dfsales_12plus = pd.merge(dfsales_total,dfweeklist_12plus,on= ['week_end_dt'])\n",
    "        \n",
    "        dfsales_12plus_lastyear = pd.merge(dfsales_total,\n",
    "                                         dfweeklist_12plus[['weeklastyear']],\n",
    "                                         left_on= 'week_end_dt',right_on = 'weeklastyear')\n",
    "        \n",
    "        dfsales_12plus_lastyear = dfsales_12plus_lastyear[['location_id','gross_transaction_cnt', 'class_gross_sales_amt','weeklastyear']]\n",
    "        dfsales_12plus_lastyear.columns = ['location_id','gross_transaction_cnt_ly', 'class_gross_sales_amt_ly','weeklastyear']\n",
    "        \n",
    "        dfsales_12plus = pd.merge(dfsales_12plus,dfsales_12plus_lastyear,\n",
    "                                        on = ['location_id','weeklastyear'],how = 'left')\n",
    "        dfsales_12plus.fillna(0,inplace = True)\n",
    "        \n",
    "        dfsales_12plus['Store_Category'] = np.where(dfsales_12plus['open_wk']>=dfsales_12plus['weeklastyear'],'New',\n",
    "                                                 np.where((dfsales_12plus['gross_transaction_cnt_ly']==0)&(dfsales_12plus['class_gross_sales_amt_ly']==0),\n",
    "                                                 'Converted',\n",
    "                                                 np.where((dfsales_12plus['gross_transaction_cnt']==0)&(dfsales_12plus['class_gross_sales_amt']==0),\n",
    "                                                'Converted','Complete')))\n",
    "        dfsales_12plus['yoysales'] = dfsales_12plus['class_gross_sales_amt']/dfsales_12plus['class_gross_sales_amt_ly'] - 1\n",
    "        dfsales_12plus['yoytrans'] = dfsales_12plus['gross_transaction_cnt']/dfsales_12plus['gross_transaction_cnt_ly'] - 1\n",
    "        dfsales_12plus = dfsales_12plus[(dfsales_12plus['Store_Category']!='Complete')|                                            ((dfsales_12plus['Store_Category']=='Complete')&                                            (abs(dfsales_12plus['yoysales'])<=0.2)|                                            (abs(dfsales_12plus['yoytrans'])<=0.2))]#&\\\n",
    "        dfsales_12plus = dfsales_12plus[['location_id','week_end_dt']]\n",
    "        \n",
    "        \n",
    "        # In[44]:\n",
    "        \n",
    "        # Finishing reading here\n",
    "        dfsales_rankingall = pd.DataFrame()\n",
    "        for i in range(52):\n",
    "            cweekdate = dfweeklist2['week_end_dt'][i]\n",
    "            recentweek_last = dfweeklist2['week_end_dt_8w'][i]\n",
    "            dfsales_ranking = dfsales_total[dfsales_total['week_end_dt']>recentweek_last]\n",
    "            dfsales_ranking = dfsales_ranking[dfsales_ranking['week_end_dt']<=cweekdate]\n",
    "            dfsales_ranking = pd.merge(dfsales_ranking,dfsales_12plus,\n",
    "                                   on = ['location_id', 'week_end_dt'])\n",
    "            dfsales_ranking = pd.merge(dfsales_ranking,\n",
    "                                       dftraffic[['location_id', 'week_end_dt', 'fiscal_week_nbr','traffic_week']],\n",
    "                                       on =['location_id', 'week_end_dt', 'fiscal_week_nbr'],how = 'left')\n",
    "            dfsales_ranking.fillna(0,inplace = True)\n",
    "            dfsales_ranking = dfsales_ranking[['location_id','class_gross_sales_amt','traffic_week']].groupby('location_id').sum()\n",
    "            dfsales_ranking['Rev/Traffic'] = dfsales_ranking['class_gross_sales_amt']/dfsales_ranking['traffic_week']\n",
    "            dfsales_ranking.reset_index(inplace = True)\n",
    "            dfsales_ranking = dfsales_ranking.sort_values('class_gross_sales_amt',ascending = False)\n",
    "            dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "            dfsales_ranking.reset_index(inplace = True)\n",
    "            dfsales_ranking = dfsales_ranking.rename(columns = {'index':'rev_index'})\n",
    "            dfsales_ranking = dfsales_ranking.replace(np.inf, 0)\n",
    "            \n",
    "            dfsales_ranking = dfsales_ranking.sort_values('Rev/Traffic',ascending = False)\n",
    "            dfsales_ranking.reset_index(drop = True,inplace = True)\n",
    "            dfsales_ranking.reset_index(inplace = True)\n",
    "            dfsales_ranking = dfsales_ranking.rename(columns = {'index':'traffi_index'})\n",
    "            \n",
    "            cols = len(dfsales_ranking.index)\n",
    "            dfsales_ranking['Store_Revenue_Rank'] = np.where(dfsales_ranking['rev_index']/cols <=0.2,'H',\n",
    "                                                            np.where(dfsales_ranking['rev_index']/cols <=0.8,'M','L'))\n",
    "            dfsales_ranking['Store_Revenue/Traffic_Rank'] = np.where(dfsales_ranking['traffi_index']/cols <=0.2,'H',\n",
    "                                                            np.where(dfsales_ranking['traffi_index']/cols <=0.8,'M','L'))\n",
    "            dfsales_ranking['week_end_dt'] = cweekdate\n",
    "            dfsales_rankingall = dfsales_rankingall.append(dfsales_ranking,ignore_index = False)\n",
    "        \n",
    "        \n",
    "        # In[45]:\n",
    "        \n",
    "        dfsales_total_recent.fillna(0,inplace = True)\n",
    "        dfsales_total_recent.reset_index(drop = True,inplace = True)\n",
    "        dfsales_total_recent = pd.merge(dfsales_total_recent,\n",
    "                                        dfsales_rankingall[['location_id','week_end_dt','Store_Revenue_Rank','Store_Revenue/Traffic_Rank']],\n",
    "                                        on = ['location_id','week_end_dt'],how = 'left')\n",
    "        dfsales_total_recent['Store_Revenue_Rank'].fillna('NA',inplace = True)\n",
    "        dfsales_total_recent['Store_Revenue/Traffic_Rank'].fillna('NA',inplace = True)\n",
    "        \n",
    "        \n",
    "        # In[46]:\n",
    "        \n",
    "        dfsales_total_recent['AOV'] = dfsales_total_recent['class_gross_sales_amt']/dfsales_total_recent['gross_transaction_cnt']\n",
    "        dfsales_total_recent['AOV_ly'] = dfsales_total_recent['class_gross_sales_amt_ly']/dfsales_total_recent['gross_transaction_cnt_ly']\n",
    "        dfsales_total_recent['Trans/Traffic'] = dfsales_total_recent['gross_transaction_cnt']/dfsales_total_recent['traffic_week']\n",
    "        dfsales_total_recent['Trans/Traffic_ly'] = dfsales_total_recent['gross_transaction_cnt_ly']/dfsales_total_recent['traffic_week_ly']\n",
    "        \n",
    "        \n",
    "        # In[47]:\n",
    "        \n",
    "        dfsales_total_recent['Store_Category'].unique()\n",
    "        \n",
    "        \n",
    "        # In[48]:\n",
    "        \n",
    "        df_complete = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Complete']\n",
    "        df_complete.reset_index(drop = True, inplace = True)\n",
    "        metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                      'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "                      'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "        columnheader = ['location_id', 'week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank']\n",
    "        for i in metricslist:\n",
    "            a = i+'_ly'\n",
    "            b = i+ 'YoYDiff'\n",
    "            columnheader = columnheader + [i,a,b]\n",
    "            df_complete[b] = df_complete[i]/df_complete[a] - 1\n",
    "        \n",
    "        \n",
    "        # In[49]:\n",
    "        \n",
    "        del dfstore['open_dtwd']\n",
    "        del dfstore['open_wk']\n",
    "        \n",
    "        \n",
    "        # In[50]:\n",
    "        \n",
    "        dma = pd.read_csv('/home/jian/BiglotsCode/OtherInput/zipdmamapping.csv',dtype = 'str')\n",
    "        dfstore['zip_cd'] = dfstore['zip_cd'].str[0:5]\n",
    "        dfstore = pd.merge(dfstore,dma,on = 'zip_cd',how = 'left')\n",
    "        \n",
    "        \n",
    "        # In[51]:\n",
    "        \n",
    "        df_complete = df_complete[columnheader]\n",
    "        df_complete = pd.merge(df_complete,dfstore,on='location_id',how = 'left')\n",
    "        df_complete = df_complete.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "        \n",
    "        \n",
    "        # In[52]:\n",
    "        \n",
    "        df_new = dfsales_total_recent[dfsales_total_recent['Store_Category']=='New']\n",
    "        df_new = df_new[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                         'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                         'Trans/Traffic','on_hand','Store_Category']]\n",
    "        df_new = pd.merge(df_new,dfstore_exc,on='location_id',how = 'left')\n",
    "        df_new = df_new.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "        df_new = pd.merge(df_new,df_tradearea_all,on='location_id',how='left')\n",
    "        del df_new['open_wk']\n",
    "        del df_new['open_dtwd']\n",
    "        del df_new['Store_Category']\n",
    "        df_new = pd.merge(df_new,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "        \n",
    "        # df_new.to_csv(outputpath + 'output2_new.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[53]:\n",
    "        \n",
    "        dfsales_total_recent['Store_Category'].unique()\n",
    "        \n",
    "        \n",
    "        # In[54]:\n",
    "        \n",
    "        dfnodata=pd.merge(dfnodata,dfstore_exc,on=\"location_id\",how='left')\n",
    "        dfnodata = pd.merge(dfnodata,df_tradearea_all,on='location_id',how='left')\n",
    "        dfnodata = pd.merge(dfnodata,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "        \n",
    "        # dfnodata.to_csv(outputpath + 'sales_nodata.csv',index = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_converted = dfsales_total_recent[dfsales_total_recent['Store_Category']=='Converted']\n",
    "        del df_converted['fiscal_week_nbr']\n",
    "        df_converted['week_end_dt'] = np.where(df_converted['week_end_dt']=='1970-01-01',\n",
    "                                               df_converted['weeklastyear'] + pd.DateOffset(364),\n",
    "                                               df_converted['week_end_dt'])\n",
    "        df_converted['weeklastyear'] = np.where(df_converted['weeklastyear']=='1970-01-01',\n",
    "                                               df_converted['week_end_dt'] + pd.DateOffset(-364),\n",
    "                                               df_converted['weeklastyear'])\n",
    "        \n",
    "        df_converted = pd.merge(df_converted,dfweeklist[['week_end_dt','fiscal_week_nbr']],\n",
    "                               on = 'week_end_dt',how='left')\n",
    "        df_converted = df_converted[['location_id', 'week_end_dt', 'fiscal_week_nbr',\n",
    "                         'Store_Revenue_Rank', 'Store_Revenue/Traffic_Rank',\n",
    "                         'class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                         'Trans/Traffic','on_hand','Store_Category',\n",
    "                         'weeklastyear','gross_transaction_cnt_ly','class_gross_sales_amt_ly']]\n",
    "        df_converted = pd.merge(df_converted,dfstore_exc,on='location_id',how = 'left')\n",
    "        df_converted = df_converted.sort_values(['location_id','week_end_dt'],ascending = [1,0])\n",
    "        \n",
    "        df_converted = pd.merge(df_converted,Recent_52_Week_nbr,on=\"week_end_dt\",how='left')\n",
    "        \n",
    "        \n",
    "        del df_converted['Store_Category']\n",
    "        df_converted = pd.merge(df_converted,df_tradearea_all,on='location_id',how='left')\n",
    "        \n",
    "        del df_converted['open_wk']\n",
    "        del df_converted['open_dtwd']\n",
    "        \n",
    "        \n",
    "        # df_converted.to_csv(outputpath + 'output3_converted.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[55]:\n",
    "        \n",
    "        TA_Information = pd.read_csv(\"/home/jian/BiglotsCode/OtherInput/New_TA_info.csv\")\n",
    "        TA_Information['trade_area_code']=TA_Information['Ta_Info'].apply(lambda x: x.split(\" | \")[0])\n",
    "        TA_Information=TA_Information[['trade_area_code','Ta_Info']]\n",
    "        TA_Information=TA_Information.drop_duplicates()\n",
    "        df_ta_info_na=pd.DataFrame(data = {'trade_area_code':['N0'], 'Ta_Info':['NA']})\n",
    "        TA_Information=TA_Information.append(df_ta_info_na,ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # In[56]:\n",
    "        \n",
    "        df_complete = pd.merge(df_complete,df_tradearea_all,on ='location_id',how = 'left')\n",
    "        df_complete['trade_area_code'].fillna('N0',inplace = True)\n",
    "        \n",
    "        df_complete = pd.merge(df_complete,Recent_52_Week_nbr,on='week_end_dt',how='left')\n",
    "        \n",
    "        \n",
    "        df_complete = pd.merge(df_complete,TA_Information,on='trade_area_code',how='left')\n",
    "        df_complete_new_list = df_complete.columns.tolist()\n",
    "        df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "        df_complete = df_complete[df_complete_new_list]\n",
    "        \n",
    "        df_complete['Store_Info'] = df_complete['location_id'].map(str)+\" | \"+df_complete['city_nm']+                        \" | \"+df_complete['state_nm']\n",
    "        df_complete_new_list = df_complete.columns.tolist()\n",
    "        df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "        df_complete = df_complete[df_complete_new_list]\n",
    "        \n",
    "        \n",
    "        # df_complete['revenue_trigger'] = np.where(abs(df_complete['class_gross_sales_amtYoYDiff'])>0.1,1,0)\n",
    "        # df_complete['transaction_trigger'] = np.where(abs(df_complete['gross_transaction_cntYoYDiff'])>0.1,1,0)\n",
    "        # df_complete['conversion_trigger'] = np.where(abs(df_complete['Trans/TrafficYoYDiff'])>0.1,1,0)\n",
    "        \n",
    "        # df_complete.to_csv(outputpath + 'output1.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[57]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # In[58]:\n",
    "        \n",
    "        # Rewrite for adding Store Key and Trade Area Key\n",
    "        \n",
    "        Write_df_list = [df_converted,dfnodata,df_new,dfsales_total_recent_delete,dfallstorelist]\n",
    "        for df in Write_df_list:\n",
    "        \n",
    "            df['trade_area_code'].fillna(\"N0\",inplace = True)\n",
    "            df['Store_Numeric']=df['location_id'].astype('str')\n",
    "            df['TA_Numeric']=df['trade_area_code'].apply(lambda x: x[1:len(x)])\n",
    "                                                         \n",
    "            df['Store_Numeric']= df['Store_Numeric'].apply(lambda x: x.zfill(4))\n",
    "            df['Store_Key_1']=df['Store_Numeric'].str[0:2]\n",
    "            df['Store_Key_2']=df['Store_Numeric'].str[2:4]\n",
    "            df['Store_Key_1']=df['Store_Key_1'].astype('int')\n",
    "            df['Store_Key_2']=df['Store_Key_2'].astype('int')\n",
    "            \n",
    "            \n",
    "            df['TA_Numeric']= df['TA_Numeric'].apply(lambda x: x.zfill(4))\n",
    "            df['TA_Key_1']=df['TA_Numeric'].str[0:2]\n",
    "            df['TA_Key_2']=df['TA_Numeric'].str[2:4]\n",
    "            df['TA_Key_1']=df['TA_Key_1'].astype('int')\n",
    "            df['TA_Key_2']=df['TA_Key_2'].astype('int')\n",
    "        \n",
    "            del df['TA_Numeric']\n",
    "            del df['Store_Numeric']\n",
    "        \n",
    "            \n",
    "        Write_df_list2 = [df_converted,dfnodata,df_new,dfsales_total_recent_delete,dfallstorelist]\n",
    "        # TA_Information['trade_area_code']=TA_Information['trade_area_code'].astype('int')\n",
    "        Write_df_list3 = []\n",
    "        for df in Write_df_list2:\n",
    "            df['trade_area_code'].fillna(\"N0\",inplace = True)\n",
    "            # df['trade_area_code']=df['trade_area_code'].astype('int')\n",
    "            \n",
    "            df = pd.merge(df,TA_Information,on='trade_area_code',how='left')\n",
    "            \n",
    "            df_complete_new_list = df.columns.tolist()\n",
    "            df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "            df = df[df_complete_new_list]\n",
    "        \n",
    "            df['Store_Info'] = df['location_id'].map(str)+\" | \"+df['city_nm'].map(str)+                        \" | \"+df['state_nm'].map(str)\n",
    "            df_complete_new_list = df.columns.tolist()\n",
    "            df_complete_new_list = df_complete_new_list[-1:]+df_complete_new_list[:-1]\n",
    "            df = df[df_complete_new_list]    \n",
    "            Write_df_list3.append(df)\n",
    "            \n",
    "        \n",
    "        df_converted=Write_df_list3[0]\n",
    "        dfnodata=Write_df_list3[1]\n",
    "        df_new=Write_df_list3[2]\n",
    "        dfsales_total_recent_delete=Write_df_list3[3]\n",
    "        dfallstorelist=Write_df_list3[4]\n",
    "        dfallstorelist=dfallstorelist[dfallstorelist['fiscal_week_nbr']!=0]\n",
    "        \n",
    "        df_complete.to_csv(outputpath + 'output1.csv',index = False)\n",
    "        df_complete.to_csv(Simeng_recent_weekly_data_folder + 'output1.csv',index = False)\n",
    "        df_converted.to_csv(outputpath + 'output3_converted'+' '+recentweek+'.csv',index = False)\n",
    "        df_converted.to_csv(Simeng_recent_weekly_data_folder + 'output3_converted'+' '+recentweek+'.csv',index = False)\n",
    "        dfnodata.to_csv(outputpath + 'sales_nodata'+' '+recentweek+'.csv',index = False)\n",
    "        \n",
    "        \n",
    "        df_new.to_csv(outputpath + 'output2_new'+' '+recentweek+'.csv',index = False)\n",
    "        df_new.to_csv(Simeng_recent_weekly_data_folder + 'output2_new'+' '+recentweek+'.csv',index = False)\n",
    "        \n",
    "        dfsales_total_recent_delete.to_csv(outputpath + 'highyoy_wowchangestores'+' '+recentweek+'.csv',index = False)\n",
    "        dfsales_total_recent_delete.to_csv(Simeng_recent_weekly_data_folder + 'highyoy_wowchangestores'+' '+recentweek+'.csv',index = False)\n",
    "        \n",
    "        dfallstorelist.to_csv(outputpath + 'nobothyeardatastores'+' '+recentweek+'.csv',index = False)\n",
    "        dfallstorelist.to_csv(Simeng_recent_weekly_data_folder + 'nobothyeardatastores'+' '+recentweek+'.csv',index = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # In[59]:\n",
    "        \n",
    "        '''\n",
    "        # Wide to long\n",
    "        \n",
    "        df_output1_17=df_complete[['location_id','week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank',\n",
    "                                   'Store_Revenue/Traffic_Rank', 'class_gross_sales_amt',\n",
    "                                   'gross_transaction_cnt','traffic_week','on_hand','location_desc', 'open_dt',\n",
    "                                   'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                                   'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']]\n",
    "        # df_output1_17=pd.merge(df_output1_17,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_output1_16=df_complete[['location_id','week_end_dt', 'fiscal_week_nbr', 'Store_Revenue_Rank',\n",
    "                                   'Store_Revenue/Traffic_Rank', 'class_gross_sales_amt_ly',\n",
    "                                   'gross_transaction_cnt_ly','traffic_week_ly','on_hand_ly','location_desc', 'open_dt',\n",
    "                                   'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                                   'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']]\n",
    "        # df_output1_16=pd.merge(df_output1_16,Recent_52_Week_nbr,on=\"week_end_dt\",how=\"left\")\n",
    "        \n",
    "        df_output1_16['week_end_dt']=df_output1_16['week_end_dt']+pd.DateOffset(-364)\n",
    "        \n",
    "        df_output1_16.columns=['location_id','week_end_dt', 'fiscal_week_nbr', 'class_gross_sales_amt',\n",
    "                                   'gross_transaction_cnt','traffic_week','on_hand','location_desc', 'open_dt',\n",
    "                                   'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                                   'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code']\n",
    "        \n",
    "        df_output1_datorama=pd.concat([df_output1_17,df_output1_16])\n",
    "        df_output1_datorama=df_output1_datorama.reindex_axis(df_output1_17.columns,axis=1)\n",
    "        df_output1_datorama.to_csv(outputpath + 'output1_datorama.csv',index = False)\n",
    "        '''\n",
    "        \n",
    "        # del df_output1_16\n",
    "        # del df_output1_17\n",
    "        \n",
    "        \n",
    "        # In[60]:\n",
    "        \n",
    "        df_tadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','trade_area_code']).sum()\n",
    "        df_tadata.reset_index(inplace = True)\n",
    "        \n",
    "        df_tadata['AOV'] = df_tadata['class_gross_sales_amt']/df_tadata['gross_transaction_cnt']\n",
    "        df_tadata['AOV_ly'] = df_tadata['class_gross_sales_amt_ly']/df_tadata['gross_transaction_cnt_ly']\n",
    "        df_tadata['Trans/Traffic'] = df_tadata['gross_transaction_cnt']/df_tadata['traffic_week']\n",
    "        df_tadata['Trans/Traffic_ly'] = df_tadata['gross_transaction_cnt_ly']/df_tadata['traffic_week_ly']\n",
    "        \n",
    "        metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                      'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "                      'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "        for i in metricslist:\n",
    "            a = i+'_ly'\n",
    "            b = i+ 'YoYDiff'\n",
    "            df_tadata[b] = df_tadata[i]/df_tadata[a] - 1\n",
    "        \n",
    "        \n",
    "        # In[61]:\n",
    "        \n",
    "        df_taclass1 = df_complete[['location_id','trade_area_code','week_end_dt','Store_Revenue_Rank',\n",
    "                                   'Store_Revenue/Traffic_Rank']]\n",
    "        df_taclass1.reset_index(drop = True, inplace = True)\n",
    "        df_taclass1['Number_of_HMStores'] = np.where(df_taclass1['Store_Revenue_Rank']!='L',1,0)\n",
    "        df_taclass1['Number_of_HMStores_RevTrafRank'] = np.where(df_taclass1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "        df_taclass1['Number of Stores'] = 1\n",
    "        df_taclass1 = df_taclass1.groupby(['trade_area_code','week_end_dt']).sum()\n",
    "        df_taclass1.reset_index(inplace = True)\n",
    "        \n",
    "        df_taclass2 = df_complete[['trade_area_code','week_end_dt','zip_cd']].drop_duplicates()\n",
    "        df_taclass2 = df_taclass2.groupby(['trade_area_code','week_end_dt']).count()\n",
    "        df_taclass2.columns = ['NumberofZipcodes']\n",
    "        df_taclass2.reset_index(inplace = True)\n",
    "        \n",
    "        df_taclass3 = df_complete[['trade_area_code','state_nm']].drop_duplicates(['trade_area_code'])\n",
    "        \n",
    "        df_tadata = pd.merge(df_tadata,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "        df_tadata = pd.merge(df_tadata,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "        df_tadata = pd.merge(df_tadata,df_taclass3,on =['trade_area_code'])\n",
    "        \n",
    "        df_tadata = df_tadata.sort_values(['trade_area_code','week_end_dt'],ascending = [1,0])\n",
    "        df_tadata.to_csv(outputpath + 'output4.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[62]:\n",
    "        \n",
    "        df_dmadata = df_complete.groupby(['week_end_dt', 'fiscal_week_nbr','DMA']).sum()\n",
    "        df_dmadata.reset_index(inplace = True)\n",
    "        \n",
    "        df_dmadata['AOV'] = df_dmadata['class_gross_sales_amt']/df_dmadata['gross_transaction_cnt']\n",
    "        df_dmadata['AOV_ly'] = df_dmadata['class_gross_sales_amt_ly']/df_dmadata['gross_transaction_cnt_ly']\n",
    "        df_dmadata['Trans/Traffic'] = df_dmadata['gross_transaction_cnt']/df_dmadata['traffic_week']\n",
    "        df_dmadata['Trans/Traffic_ly'] = df_dmadata['gross_transaction_cnt_ly']/df_dmadata['traffic_week_ly']\n",
    "        \n",
    "        metricslist = ['class_gross_sales_amt','gross_transaction_cnt','AOV','traffic_week',\n",
    "                      'Trans/Traffic', 'traffic_day_1', 'traffic_day_2', 'traffic_day_3',\n",
    "                      'traffic_day_4', 'traffic_day_5', 'traffic_day_6', 'traffic_day_7','on_hand']\n",
    "        for i in metricslist:\n",
    "            a = i+'_ly'\n",
    "            b = i+ 'YoYDiff'\n",
    "            df_dmadata[b] = df_dmadata[i]/df_dmadata[a] - 1\n",
    "        \n",
    "        \n",
    "        # In[63]:\n",
    "        \n",
    "        df_dmadata1 = df_complete[['location_id','DMA','week_end_dt','Store_Revenue_Rank',\n",
    "                                   'Store_Revenue/Traffic_Rank']]\n",
    "        df_dmadata1.reset_index(drop = True, inplace = True)\n",
    "        df_dmadata1['Number_of_HMStores'] = np.where(df_dmadata1['Store_Revenue_Rank']!='L',1,0)\n",
    "        df_dmadata1['Number_of_HMStores_RevTrafRank'] = np.where(df_dmadata1['Store_Revenue/Traffic_Rank']!='L',1,0)\n",
    "        df_dmadata1['Number of Stores'] = 1\n",
    "        df_dmadata1 = df_dmadata1.groupby(['DMA','week_end_dt']).sum()\n",
    "        df_dmadata1.reset_index(inplace = True)\n",
    "        \n",
    "        df_dmadata2 = df_complete[['DMA','week_end_dt','zip_cd']].drop_duplicates()\n",
    "        df_dmadata2 = df_dmadata2.groupby(['DMA','week_end_dt']).count()\n",
    "        df_dmadata2.columns = ['NumberofZipcodes']\n",
    "        df_dmadata2.reset_index(inplace = True)\n",
    "        \n",
    "        df_dmadata4 = df_complete[['DMA','week_end_dt','trade_area_code']].drop_duplicates()\n",
    "        df_dmadata4 = df_dmadata4.groupby(['DMA','week_end_dt']).count()\n",
    "        df_dmadata4.columns = ['NumberofTAs']\n",
    "        df_dmadata4.reset_index(inplace = True)\n",
    "        \n",
    "        df_dmadata3 = df_complete[['DMA','state_nm']].drop_duplicates(['DMA'])\n",
    "        \n",
    "        df_dmadata = pd.merge(df_dmadata,df_dmadata1,on =['DMA','week_end_dt'])\n",
    "        df_dmadata = pd.merge(df_dmadata,df_dmadata2,on =['DMA','week_end_dt'])\n",
    "        df_dmadata = pd.merge(df_dmadata,df_dmadata4,on =['DMA','week_end_dt'])\n",
    "        df_dmadata = pd.merge(df_dmadata,df_dmadata3,on =['DMA'])\n",
    "        \n",
    "        df_dmadata = df_dmadata.sort_values(['DMA','week_end_dt'],ascending = [1,0])\n",
    "        df_dmadata.to_csv(outputpath + 'output5.csv',index = False)\n",
    "        del df_dmadata1,df_dmadata2,df_dmadata3,df_dmadata4\n",
    "        \n",
    "        \n",
    "        # In[64]:\n",
    "        \n",
    "        df_storeweeklist = pd.merge(df_complete[['location_id', 'week_end_dt']],\n",
    "                                    dfweeklist[['week_end_dt','weeklastyear']],on ='week_end_dt')\n",
    "        \n",
    "        \n",
    "        # In[65]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # In[ ]:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # In[66]:\n",
    "        \n",
    "        dfsales_class1 = pd.merge(dfsales[['location_id', 'week_end_dt','class_code_id','class_gross_sales_amt']],\n",
    "                                  df_storeweeklist,\n",
    "                                  on = ['location_id', 'week_end_dt'])\n",
    "        dfsales_class2 = pd.merge(dfsales[['location_id', 'week_end_dt', 'class_code_id','class_gross_sales_amt']],\n",
    "                                  df_storeweeklist,\n",
    "                                  left_on = ['location_id', 'week_end_dt'],\n",
    "                                  right_on = ['location_id', 'weeklastyear'])\n",
    "        \n",
    "        del dfsales_class2['week_end_dt_x']\n",
    "        dfsales_class2 = dfsales_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "        \n",
    "        dfsales_class1 = pd.merge(dfsales_class1,dfsales_class2,\n",
    "                                  on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "        dfsales_class1.fillna(0,inplace = True)\n",
    "        del dfsales_class2\n",
    "        del dfsales_class1['weeklastyear_x']\n",
    "        del dfsales_class1['weeklastyear_y']\n",
    "        dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_x':'class_gross_sales_amt'})\n",
    "        dfsales_class1 = dfsales_class1.rename(columns = {'class_gross_sales_amt_y':'class_gross_sales_amt_ly'})\n",
    "        dfsales_class1['class_gross_sales_amt_yoy'] = dfsales_class1['class_gross_sales_amt']/dfsales_class1['class_gross_sales_amt_ly'] -1\n",
    "        \n",
    "        \n",
    "        # In[67]:\n",
    "        \n",
    "        dfinventory_class1 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']], \n",
    "                                      df_storeweeklist,\n",
    "                                  on = ['location_id', 'week_end_dt'])\n",
    "        dfinventory_class2 = pd.merge(dfinventory[['location_id', 'week_end_dt', 'class_code_id','on_hand']],\n",
    "                                  df_storeweeklist,\n",
    "                                  left_on = ['location_id', 'week_end_dt'],\n",
    "                                  right_on = ['location_id', 'weeklastyear'])\n",
    "        \n",
    "        del dfinventory_class2['week_end_dt_x']\n",
    "        dfinventory_class2 = dfinventory_class2.rename(columns = {'week_end_dt_y':'week_end_dt'})\n",
    "        \n",
    "        dfinventory_class1 = pd.merge(dfinventory_class1,dfinventory_class2,\n",
    "                                  on =['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "        dfinventory_class1.fillna(0,inplace = True)\n",
    "        del dfinventory_class2\n",
    "        del dfinventory_class1['weeklastyear_x']\n",
    "        del dfinventory_class1['weeklastyear_y']\n",
    "        dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_x':'on_hand'})\n",
    "        dfinventory_class1 = dfinventory_class1.rename(columns = {'on_hand_y':'on_hand_ly'})\n",
    "        dfinventory_class1['on_hand_yoy'] = dfinventory_class1['on_hand']/dfinventory_class1['on_hand_ly'] -1\n",
    "        \n",
    "        \n",
    "        # In[68]:\n",
    "        \n",
    "        df_tadetail = pd.merge(dfsales_class1,dfinventory_class1,\n",
    "                              on=['location_id','week_end_dt','class_code_id'],how='outer')\n",
    "        df_tadetail = pd.merge(df_tadetail,df_tradearea_all,on ='location_id',how = 'left')\n",
    "        df_tadetail['trade_area_code'].fillna('NA',inplace = True)\n",
    "        df_tadetail.fillna(0,inplace = True)\n",
    "        \n",
    "        df_tadetail = df_tadetail.groupby(['trade_area_code','week_end_dt','class_code_id']).sum()\n",
    "        df_tadetail['class_gross_sales_amt_yoy'] = df_tadetail['class_gross_sales_amt']/df_tadetail['class_gross_sales_amt_ly'] -1\n",
    "        df_tadetail['on_hand_yoy'] = df_tadetail['on_hand']/df_tadetail['on_hand_ly'] -1\n",
    "        df_tadetail.reset_index(inplace = True)\n",
    "        \n",
    "        df_tadetail = pd.merge(df_tadetail,df_taclass1,on =['trade_area_code','week_end_dt'])\n",
    "        df_tadetail = pd.merge(df_tadetail,df_taclass2,on =['trade_area_code','week_end_dt'])\n",
    "        df_tadetail = pd.merge(df_tadetail,df_taclass3,on =['trade_area_code'])\n",
    "        df_tadetail = pd.merge(dfweeklist[['week_end_dt','fiscal_week_nbr']],df_tadetail,\n",
    "                              on ='week_end_dt')\n",
    "        df_tadetail.to_csv(outputpath + 'output4_classdetail.csv',index = False)\n",
    "        \n",
    "        \n",
    "        # In[69]:\n",
    "        \n",
    "        writer = pd.ExcelWriter(outputpath+'BigLots_Weekly_Data_'+recentweek+'.xlsx',\n",
    "                                    #engine='xlsxwriter',\n",
    "                                    datetime_format='yyyy-mm-dd',\n",
    "                                    date_format='yyyy-mm-dd')\n",
    "        test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'gross_transaction_cnt']]\n",
    "        test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "        test.to_excel(writer,'Transactions', index=False)\n",
    "        test = dfsales_total[['location_id', 'week_end_dt', 'fiscal_week_nbr', 'class_gross_sales_amt']]\n",
    "        test = test.sort_values(['location_id', 'week_end_dt'])\n",
    "        test.to_excel(writer,'Revenue', index=False)\n",
    "        dftraffic = dftraffic[~dftraffic['location_id'].isin(closed_onlinestorelist)]\n",
    "        dftraffic = dftraffic.sort_values(['location_id', 'week_end_dt'])\n",
    "        dfinventory_total = dfinventory_total[~dfinventory_total['location_id'].isin(closed_onlinestorelist)]\n",
    "        dfinventory_total = dfinventory_total.sort_values(['location_id', 'week_end_dt'])\n",
    "        dftraffic.to_excel(writer,'Traffic', index=False)\n",
    "        dfinventory_total.to_excel(writer,'Inventory', index=False)\n",
    "        \n",
    "        \n",
    "        # In[70]:\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "        # In[71]:\n",
    "        \n",
    "        df_complete_week = df_complete[df_complete['week_end_dt']==recentweek_date]\n",
    "        df_complete_week.to_csv(outputpath + 'output1_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        \n",
    "        df_highvariance_week = dfsales_total_recent_delete[dfsales_total_recent_delete['week_end_dt']==recentweek_date]\n",
    "        df_highvariance_week.to_csv(outputpath + 'highyoy_wowchangestores_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        \n",
    "        df_tadetail_week = df_tadetail[df_tadetail['week_end_dt']==recentweek_date]\n",
    "        df_tadetail_week.to_csv(outputpath + 'output4_classdetail_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        df_dmadata_week = df_dmadata[df_dmadata['week_end_dt']==recentweek_date]\n",
    "        df_dmadata_week.to_csv(outputpath + 'output5_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        df_tadata_week = df_tadata[df_tadata['week_end_dt']==recentweek_date]\n",
    "        df_tadata_week.to_csv(outputpath + 'output4_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        \n",
    "        df_converted_week = df_converted[df_converted['week_end_dt']==recentweek_date]\n",
    "        df_converted_week.to_csv(outputpath + 'output3_converted_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        \n",
    "        df_new_week = df_new[df_new['week_end_dt']==recentweek_date]\n",
    "        df_new_week.to_csv(outputpath + 'output2_new_' + recentweek + '.csv',index = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # In[72]:\n",
    "        \n",
    "        df_complete_week = df_complete_week[['location_id', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                             'location_desc', 'open_dt',\n",
    "                           'address_line_1', 'address_line_2', 'city_nm', 'state_nm', 'zip_cd',\n",
    "                           'longitude_meas', 'latitude_meas', 'DMA', 'trade_area_code',\n",
    "                           'class_gross_sales_amt',\n",
    "                           'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                           'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                           'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                           'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "        \n",
    "        \n",
    "        # In[73]:\n",
    "        \n",
    "        df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "        df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "        df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "        df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]\n",
    "        \n",
    "        \n",
    "        # In[74]:\n",
    "        \n",
    "        import xlsxwriter\n",
    "        \n",
    "        writer = pd.ExcelWriter(outputpath+'Output1Tracker_'+recentweek+'.xlsx',\n",
    "                                    engine='xlsxwriter',\n",
    "                                    datetime_format='yyyy-mm-dd',\n",
    "                                    date_format='yyyy-mm-dd')\n",
    "        \n",
    "        workbook  = writer.book\n",
    "        \n",
    "        format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "        format2 = workbook.add_format({'text_wrap' : True})\n",
    "        format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "        format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                                       'bg_color': 'FF9999'})\n",
    "        \n",
    "        df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "        worksheet = writer.sheets['TransactionTracker']\n",
    "        worksheet.set_row(0,None, format2)\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('E:E', 12, None)\n",
    "        worksheet.set_column('F:F', 16, None)\n",
    "        worksheet.set_column('O:Y', None, format1)\n",
    "        worksheet.set_column('Q:Q', None, format3)\n",
    "        worksheet.set_column('T:T', None, format4)\n",
    "        worksheet.set_column('W:W', None, format3)\n",
    "        worksheet.set_column('Z:Z', None, format3)\n",
    "        worksheet.set_column('U:V', None, format3)\n",
    "        \n",
    "        df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "        worksheet = writer.sheets['ConversionTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('E:E', 12, None)\n",
    "        worksheet.set_column('F:F', 16, None)\n",
    "        worksheet.set_column('O:Y', None, format1)\n",
    "        worksheet.set_column('Q:Q', None, format3)\n",
    "        worksheet.set_column('T:T', None, format3)\n",
    "        worksheet.set_column('W:W', None, format4)\n",
    "        worksheet.set_column('Z:Z', None, format3)\n",
    "        worksheet.set_column('U:V', None, format3)\n",
    "        \n",
    "        df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "        worksheet = writer.sheets['TrafficTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('E:E', 12, None)\n",
    "        worksheet.set_column('F:F', 16, None)\n",
    "        worksheet.set_column('O:Y', None, format1)\n",
    "        worksheet.set_column('Q:Q', None, format3)\n",
    "        worksheet.set_column('T:T', None, format3)\n",
    "        worksheet.set_column('W:W', None, format3)\n",
    "        worksheet.set_column('Z:Z', None, format4)\n",
    "        worksheet.set_column('U:V', None, format3)\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "        # In[75]:\n",
    "        \n",
    "        df_complete_week = df_tadata_week[['trade_area_code', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                             'Number_of_HMStores', 'Number_of_HMStores_RevTrafRank',\n",
    "                           'Number of Stores', 'NumberofZipcodes', 'state_nm',\n",
    "                           'class_gross_sales_amt',\n",
    "                           'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                           'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                           'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                           'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "        \n",
    "        df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "        df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "        df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "        df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]\n",
    "        \n",
    "        \n",
    "        # In[76]:\n",
    "        \n",
    "        import xlsxwriter\n",
    "        \n",
    "        writer = pd.ExcelWriter(outputpath+'Output4Tracker_'+recentweek+'.xlsx',\n",
    "                                    engine='xlsxwriter',\n",
    "                                    datetime_format='yyyy-mm-dd',\n",
    "                                    date_format='yyyy-mm-dd')\n",
    "        \n",
    "        workbook  = writer.book\n",
    "        \n",
    "        format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "        format2 = workbook.add_format({'text_wrap' : True})\n",
    "        format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "        format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                                       'bg_color': 'FF9999'})\n",
    "        \n",
    "        df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "        worksheet = writer.sheets['TransactionTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('I:S', None, format1)\n",
    "        worksheet.set_column('K:K', None, format3)\n",
    "        worksheet.set_column('N:N', None, format4)\n",
    "        worksheet.set_column('Q:Q', None, format3)\n",
    "        worksheet.set_column('T:T', None, format3)\n",
    "        worksheet.set_column('O:P', None, format3)\n",
    "        \n",
    "        df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "        worksheet = writer.sheets['ConversionTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('I:S', None, format1)\n",
    "        worksheet.set_column('K:K', None, format3)\n",
    "        worksheet.set_column('N:N', None, format3)\n",
    "        worksheet.set_column('Q:Q', None, format4)\n",
    "        worksheet.set_column('T:T', None, format3)\n",
    "        worksheet.set_column('O:P', None, format3)\n",
    "        \n",
    "        df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "        worksheet = writer.sheets['TrafficTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('I:S', None, format1)\n",
    "        worksheet.set_column('K:K', None, format3)\n",
    "        worksheet.set_column('N:N', None, format3)\n",
    "        worksheet.set_column('Q:Q', None, format3)\n",
    "        worksheet.set_column('T:T', None, format4)\n",
    "        worksheet.set_column('O:P', None, format3)\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "        # In[77]:\n",
    "        \n",
    "        df_complete_week = df_dmadata_week[['DMA', 'week_end_dt', 'fiscal_week_nbr', \n",
    "                                             'Number_of_HMStores',\n",
    "                             'Number_of_HMStores_RevTrafRank', 'Number of Stores',\n",
    "                             'NumberofZipcodes', 'NumberofTAs', 'state_nm',\n",
    "                           'class_gross_sales_amt',\n",
    "                           'class_gross_sales_amt_ly', 'class_gross_sales_amtYoYDiff',\n",
    "                           'gross_transaction_cnt', 'gross_transaction_cnt_ly','gross_transaction_cntYoYDiff',\n",
    "                           'Trans/Traffic', 'Trans/Traffic_ly', 'Trans/TrafficYoYDiff',\n",
    "                           'traffic_week', 'traffic_week_ly', 'traffic_weekYoYDiff']]\n",
    "        \n",
    "        df_complete_week1 = df_complete_week.sort_values(['gross_transaction_cntYoYDiff'],ascending = True)\n",
    "        df_complete_week1 = df_complete_week1[df_complete_week1['gross_transaction_cntYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week2 = df_complete_week.sort_values(['Trans/TrafficYoYDiff'],ascending = True)\n",
    "        df_complete_week2 = df_complete_week2[df_complete_week2['Trans/TrafficYoYDiff']<=-0.1]\n",
    "        \n",
    "        df_complete_week3 = df_complete_week.sort_values(['traffic_weekYoYDiff'],ascending = True)\n",
    "        df_complete_week3 = df_complete_week3[df_complete_week3['traffic_weekYoYDiff']<=-0.1]\n",
    "        \n",
    "        \n",
    "        # In[78]:\n",
    "        \n",
    "        writer = pd.ExcelWriter(outputpath+'Output5Tracker_'+recentweek+'.xlsx',\n",
    "                                    engine='xlsxwriter',\n",
    "                                    datetime_format='yyyy-mm-dd',\n",
    "                                    date_format='yyyy-mm-dd')\n",
    "        \n",
    "        workbook  = writer.book\n",
    "        \n",
    "        format1 = workbook.add_format({'num_format': '#,##0'})\n",
    "        format2 = workbook.add_format({'text_wrap' : True})\n",
    "        format3 = workbook.add_format({'num_format': '0.0%'})\n",
    "        format4 = workbook.add_format({'num_format': '0.0%',\n",
    "                                       'bg_color': 'FF9999'})\n",
    "        \n",
    "        df_complete_week1.to_excel(writer,'TransactionTracker', index=False)\n",
    "        worksheet = writer.sheets['TransactionTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('J:T', None, format1)\n",
    "        worksheet.set_column('L:L', None, format3)\n",
    "        worksheet.set_column('O:O', None, format4)\n",
    "        worksheet.set_column('R:R', None, format3)\n",
    "        worksheet.set_column('U:U', None, format3)\n",
    "        worksheet.set_column('P:Q', None, format3)\n",
    "        \n",
    "        df_complete_week2.to_excel(writer,'ConversionTracker', index=False)\n",
    "        worksheet = writer.sheets['ConversionTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('J:T', None, format1)\n",
    "        worksheet.set_column('L:L', None, format3)\n",
    "        worksheet.set_column('O:O', None, format3)\n",
    "        worksheet.set_column('R:R', None, format4)\n",
    "        worksheet.set_column('U:U', None, format3)\n",
    "        worksheet.set_column('P:Q', None, format3)\n",
    "        \n",
    "        df_complete_week3.to_excel(writer,'TrafficTracker', index=False)\n",
    "        worksheet = writer.sheets['TrafficTracker']\n",
    "        worksheet.set_column('B:B', 12, None)\n",
    "        worksheet.set_column('J:T', None, format1)\n",
    "        worksheet.set_column('L:L', None, format3)\n",
    "        worksheet.set_column('O:O', None, format3)\n",
    "        worksheet.set_column('R:R', None, format3)\n",
    "        worksheet.set_column('U:U', None, format4)\n",
    "        worksheet.set_column('P:Q', None, format3)\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "        # In[79]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # # Write all historical data into long and wide format by store\n",
    "        \n",
    "        # In[80]:\n",
    "        \n",
    "        from datetime import datetime\n",
    "        \n",
    "        \n",
    "        # In[81]:\n",
    "        \n",
    "        sales_long_df=pd.read_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_\" +lastweeksdate+\".csv\")\n",
    "        del sales_long_df['week_indicator']\n",
    "        sales_long_df['week_end_date']=sales_long_df['week_end_date'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "        start_record_date=min(sales_long_df['week_end_date'])\n",
    "        \n",
    "        \n",
    "        # In[82]:\n",
    "        \n",
    "        new_sales_df = pd.read_csv(newsalespath,sep = '|',dtype = 'str')\n",
    "        new_sales_df['subclass_gross_sales_amt']=new_sales_df['subclass_gross_sales_amt'].astype(float)\n",
    "        new_sales_df=new_sales_df.groupby(['location_id','week_end_dt','fiscal_week_nbr','gross_sales_amt','gross_transaction_cnt','class_code_id'])['subclass_gross_sales_amt'].sum().to_frame().reset_index()\n",
    "        new_sales_df=new_sales_df.rename(columns={\"subclass_gross_sales_amt\":\"class_gross_sales_amt\"})\n",
    "        \n",
    "        new_traffic_df = pd.read_csv(newtrafficpath,sep = '|',dtype = 'str')\n",
    "        new_sales_df=new_sales_df[~new_sales_df['location_id'].isin([\"145\",\"6990\"])]\n",
    "        new_traffic_df=new_traffic_df[~new_traffic_df['location_id'].isin([\"145\",\"6990\"])]\n",
    "        \n",
    "        new_sales_df['location_id']=new_sales_df['location_id'].astype(int)\n",
    "        new_sales_df['week_end_dt']=new_sales_df['week_end_dt'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "        new_sales_df['class_gross_sales_amt']=new_sales_df['class_gross_sales_amt'].astype(float)\n",
    "        new_sales_df['gross_transaction_cnt']=new_sales_df['gross_transaction_cnt'].astype(int)\n",
    "        \n",
    "        new_traffic_df['location_id']=new_traffic_df['location_id'].astype(int)\n",
    "        new_traffic_df['week_end_dt']=new_traffic_df['week_end_dt'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "        new_traffic_df['traffic_day_1']=new_traffic_df['traffic_day_1'].astype(int)\n",
    "        new_traffic_df['traffic_day_2']=new_traffic_df['traffic_day_2'].astype(int)\n",
    "        new_traffic_df['traffic_day_3']=new_traffic_df['traffic_day_3'].astype(int)\n",
    "        new_traffic_df['traffic_day_4']=new_traffic_df['traffic_day_4'].astype(int)\n",
    "        new_traffic_df['traffic_day_5']=new_traffic_df['traffic_day_5'].astype(int)\n",
    "        new_traffic_df['traffic_day_6']=new_traffic_df['traffic_day_6'].astype(int)\n",
    "        new_traffic_df['traffic_day_7']=new_traffic_df['traffic_day_7'].astype(int)\n",
    "        new_traffic_df['traffic']=new_traffic_df[['traffic_day_1','traffic_day_2','traffic_day_3','traffic_day_4',\n",
    "                                                 'traffic_day_5','traffic_day_6','traffic_day_7']].sum(axis=1)\n",
    "        new_traffic_df=new_traffic_df[['location_id','week_end_dt','traffic']]\n",
    "        new_traffic_df=new_traffic_df.drop_duplicates()\n",
    "        new_traffic_df.reset_index(inplace=True)\n",
    "        del new_traffic_df['index']\n",
    "        \n",
    "        \n",
    "        # In[83]:\n",
    "        \n",
    "        new_sales_df_app=new_sales_df[['location_id','week_end_dt','class_gross_sales_amt']].drop_duplicates()\n",
    "        new_sales_df_app=new_sales_df.groupby(['location_id','week_end_dt'])['class_gross_sales_amt'].sum().to_frame()\n",
    "        new_sales_df_app.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        # In[84]:\n",
    "        \n",
    "        new_transaction_df_app=new_sales_df[['location_id','week_end_dt','gross_transaction_cnt']]\n",
    "        new_transaction_df_app=new_transaction_df_app.drop_duplicates().reset_index()\n",
    "        del new_transaction_df_app['index']\n",
    "        \n",
    "        \n",
    "        # In[85]:\n",
    "        \n",
    "        recentweek = str(max(new_sales_df_app['week_end_dt']).date())\n",
    "        \n",
    "        \n",
    "        # In[86]:\n",
    "        \n",
    "        append_to_long=pd.merge(new_sales_df_app,new_transaction_df_app,on=['location_id','week_end_dt'],how='outer')\n",
    "        append_to_long=pd.merge(append_to_long,new_traffic_df,on=['location_id','week_end_dt'],how='outer')\n",
    "        append_to_long.columns=['location_id','week_end_date','sales','transactions','traffics']\n",
    "        append_to_long=append_to_long.fillna(0)\n",
    "        append_to_long['traffics']=append_to_long['traffics'].astype(int)\n",
    "        sales_long_df=sales_long_df.append(append_to_long)\n",
    "        sales_long_df['week_indicator']=sales_long_df['week_end_date'].apply(lambda x: int((x-start_record_date).days/7+1))\n",
    "        sales_long_df=sales_long_df.sort_values(['location_id','week_end_date'])\n",
    "        sales_long_df['week_indicator']=sales_long_df['week_indicator'].apply(lambda x: x% 52)\n",
    "        sales_long_df.to_csv(\"/home/jian/BiglotsCode/outputs/combined_sales_long_\" +recentweek+\".csv\",index=False)\n",
    "        \n",
    "        sales_long_df_Simeng=sales_long_df.copy()\n",
    "        sales_long_df_Simeng['location_id']=sales_long_df_Simeng['location_id'].astype(str)\n",
    "        del sales_long_df_Simeng['DMA']\n",
    "        del sales_long_df['DMA']\n",
    "        \n",
    "        store_DMA=pd.read_excel(\"/home/jian/BiglotsCode/OtherInput/all_store_DMA_20180726.xlsx\",dtype=str)\n",
    "        store_DMA=store_DMA[['location_id','cleaned_dma']]\n",
    "        store_DMA=store_DMA.rename(columns={\"cleaned_dma\":\"DMA\"})\n",
    "        store_DMA['DMA']=store_DMA['DMA'].replace(\"xx\",\"NA\")\n",
    "        sales_long_df_Simeng=pd.merge(sales_long_df_Simeng,store_DMA,on=\"location_id\",how=\"left\")\n",
    "        \n",
    "        sales_long_df_Simeng.to_csv(Simeng_recent_weekly_data_folder+\"combined_sales_long_\" +recentweek+\".csv\",index=False)\n",
    "        \n",
    "        \n",
    "        # In[87]:\n",
    "        \n",
    "        writer = pd.ExcelWriter(\"/home/jian/BiglotsCode/outputs/Output_\"+recentweek+\"/wide_sales_date\"+recentweek+\".xlsx\", engine='xlsxwriter')\n",
    "        sales_long_df['week_end_date']=sales_long_df['week_end_date'].apply(lambda x: str(x.date()))\n",
    "        sales_wide_df_sales=sales_long_df[['location_id','week_end_date','sales']].pivot(index='location_id',columns='week_end_date',values='sales')\n",
    "        sales_wide_df_sales=sales_wide_df_sales.fillna(0)\n",
    "        sales_wide_df_sales.to_excel(writer, sheet_name='sales')\n",
    "        trans_wide_df_sales=sales_long_df[['location_id','week_end_date','transactions']].pivot(index='location_id',columns='week_end_date',values='transactions')\n",
    "        trans_wide_df_sales=trans_wide_df_sales.fillna(0)\n",
    "        trans_wide_df_sales.to_excel(writer, sheet_name='transactions')\n",
    "        traffics_wide_df_sales=sales_long_df[['location_id','week_end_date','traffics']].pivot(index='location_id',columns='week_end_date',values='traffics')\n",
    "        traffics_wide_df_sales=traffics_wide_df_sales.fillna(0)\n",
    "        traffics_wide_df_sales.to_excel(writer, sheet_name='traffics')\n",
    "        \n",
    "        # summary=pd.DataFrame(columns=['location_id'])\n",
    "        \n",
    "        \n",
    "        # In[88]:\n",
    "        \n",
    "        T_sales_wide_df_sales=sales_wide_df_sales.T\n",
    "        T_sales_wide_df_sales['sales']=T_sales_wide_df_sales[T_sales_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "        T_sales_wide_df_sales=T_sales_wide_df_sales['sales'].to_frame().T\n",
    "        \n",
    "        T_trans_wide_df_sales=trans_wide_df_sales.T\n",
    "        T_trans_wide_df_sales['trans']=T_trans_wide_df_sales[T_trans_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "        T_trans_wide_df_sales=T_trans_wide_df_sales['trans'].to_frame().T\n",
    "        \n",
    "        T_traffics_wide_df_sales=traffics_wide_df_sales.T\n",
    "        T_traffics_wide_df_sales['traffics']=T_traffics_wide_df_sales[T_traffics_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "        T_traffics_wide_df_sales=T_traffics_wide_df_sales['traffics'].to_frame().T\n",
    "        \n",
    "        count_wide_df_sales=sales_wide_df_sales.copy()\n",
    "        \n",
    "        for col in count_wide_df_sales.columns.tolist():\n",
    "            count_wide_df_sales[col]=np.where(count_wide_df_sales[col]>0,1,0)\n",
    "        \n",
    "        T_count_wide_df_sales=count_wide_df_sales.T\n",
    "        T_count_wide_df_sales['counts']=T_count_wide_df_sales[T_count_wide_df_sales.columns.tolist()].sum(axis=1)\n",
    "        T_count_wide_df_sales=T_count_wide_df_sales['counts'].to_frame().T\n",
    "        \n",
    "        T_avg_sales_wide_df_sales=T_sales_wide_df_sales.copy()\n",
    "        T_avg_sales_wide_df_sales.index=['avg_sales']\n",
    "        for col in T_avg_sales_wide_df_sales.columns.tolist():\n",
    "            T_avg_sales_wide_df_sales[col]['avg_sales']=T_sales_wide_df_sales[col]['sales']/T_count_wide_df_sales[col]['counts']\n",
    "        \n",
    "        \n",
    "        summary=T_sales_wide_df_sales.append(T_trans_wide_df_sales).append(T_traffics_wide_df_sales).append(T_count_wide_df_sales).append(T_avg_sales_wide_df_sales)\n",
    "        \n",
    "        summary.to_excel(writer,sheet_name=\"summary\")\n",
    "        \n",
    "        \n",
    "        # In[89]:\n",
    "        \n",
    "        workbook  = writer.book\n",
    "        worksheet = writer.sheets['summary']\n",
    "        \n",
    "        \n",
    "        # In[90]:\n",
    "        \n",
    "        # Create a sales line chart\n",
    "        chartloc = len(summary.index) + 4\n",
    "        \n",
    "        chart_sales = workbook.add_chart({'type': 'line'})\n",
    "        chart_sales.add_series({\n",
    "                'name':       '2019',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "                'values':     ['summary', 1, summary.shape[1]-11, 1, summary.shape[1]]})\n",
    "        \n",
    "        chart_sales.add_series({\n",
    "                'name':       '2018',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "                'values':     ['summary', 1, summary.shape[1]-11-52, 1, summary.shape[1]-52]})\n",
    "        \n",
    "        chart_sales.add_series({\n",
    "                'name':       '2017',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "                'values':     ['summary', 1, summary.shape[1]-11-52-52, 1, summary.shape[1]-52-52]})\n",
    "        \n",
    "        chart_sales.set_x_axis({'name': 'Date'})\n",
    "        chart_sales.set_y_axis({'name': 'Revenue', 'major_gridlines': {'visible': True}})\n",
    "        \n",
    "        chart_sales.set_size({'width': 960, 'height': 432})\n",
    "        chart_sales.set_title({'name': 'Recent 12 Weeks Sales'})\n",
    "        chart_sales.set_legend({'position': 'bottom'})\n",
    "        worksheet.insert_chart('B'+str(chartloc), chart_sales) \n",
    "        \n",
    "        \n",
    "        # In[91]:\n",
    "        \n",
    "        # Create a transaction line chart\n",
    "        chartloc = len(summary.index) + 4\n",
    "        \n",
    "        chart_trans = workbook.add_chart({'type': 'line'})\n",
    "        chart_trans.add_series({\n",
    "                'name':       '2019',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "                'values':     ['summary', 2, summary.shape[1]-11, 2, summary.shape[1]]})\n",
    "        \n",
    "        chart_trans.add_series({\n",
    "                'name':       '2018',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "                'values':     ['summary', 2, summary.shape[1]-11-52, 2, summary.shape[1]-52]})\n",
    "        \n",
    "        chart_trans.add_series({\n",
    "                'name':       '2017',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "                'values':     ['summary', 2, summary.shape[1]-11-52-52, 2, summary.shape[1]-52-52]})\n",
    "        \n",
    "        chart_trans.set_x_axis({'name': 'Date'})\n",
    "        chart_trans.set_y_axis({'name': 'Transaction', 'major_gridlines': {'visible': True}})\n",
    "        \n",
    "        chart_trans.set_size({'width': 960, 'height': 432})\n",
    "        chart_trans.set_title({'name': 'Recent 12 Weeks Transactions'})\n",
    "        chart_trans.set_legend({'position': 'bottom'})\n",
    "        worksheet.insert_chart('Q'+str(chartloc), chart_trans) \n",
    "        \n",
    "        \n",
    "        # In[92]:\n",
    "        \n",
    "        # Create a traffics line chart\n",
    "        chartloc = len(summary.index) + 4\n",
    "        \n",
    "        chart_traffics = workbook.add_chart({'type': 'line'})\n",
    "        chart_traffics.add_series({\n",
    "                'name':       '2019',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "                'values':     ['summary', 3, summary.shape[1]-11, 3, summary.shape[1]]})\n",
    "        \n",
    "        chart_traffics.add_series({\n",
    "                'name':       '2018',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "                'values':     ['summary', 3, summary.shape[1]-11-52, 3, summary.shape[1]-52]})\n",
    "        \n",
    "        chart_traffics.add_series({\n",
    "                'name':       '2017',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "                'values':     ['summary', 3, summary.shape[1]-11-52-52, 3, summary.shape[1]-52-52]})\n",
    "        \n",
    "        chart_traffics.set_x_axis({'name': 'Date'})\n",
    "        chart_traffics.set_y_axis({'name': 'Revenue', \n",
    "                               'major_gridlines': {'visible': True}})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        chart_traffics.set_size({'width': 960, 'height': 432})\n",
    "        chart_traffics.set_title({'name': 'Recent 12 weeks traffics'})\n",
    "        chart_traffics.set_legend({'position': 'bottom'})\n",
    "        worksheet.insert_chart('AF'+str(chartloc), chart_traffics) \n",
    "        \n",
    "        \n",
    "        # In[93]:\n",
    "        \n",
    "        # Create a average sales line chart\n",
    "        chartloc = len(summary.index) + 4+25\n",
    "        \n",
    "        chart_avg_sales = workbook.add_chart({'type': 'line'})\n",
    "        chart_avg_sales.add_series({\n",
    "                'name':       '2019',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11, 0, summary.shape[1]],\n",
    "                'values':     ['summary', 5, summary.shape[1]-11, 5, summary.shape[1]]})\n",
    "        \n",
    "        chart_avg_sales.add_series({\n",
    "                'name':       '2018',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52, 0, summary.shape[1]-52],\n",
    "                'values':     ['summary', 5, summary.shape[1]-11-52, 5, summary.shape[1]-52]})\n",
    "        \n",
    "        chart_avg_sales.add_series({\n",
    "                'name':       '2017',\n",
    "                'categories': ['summary', 0, summary.shape[1]-11-52-52, 0, summary.shape[1]-52-52],\n",
    "                'values':     ['summary', 5, summary.shape[1]-11-52-52, 5, summary.shape[1]-52-52]})\n",
    "        \n",
    "        chart_avg_sales.set_x_axis({'name': 'Date'})\n",
    "        chart_avg_sales.set_y_axis({'name': 'Avg_Revenue', \n",
    "                               'major_gridlines': {'visible': True}})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        chart_avg_sales.set_size({'width': 960, 'height': 432})\n",
    "        chart_avg_sales.set_title({'name': 'Recent 12 weeks sales per open store'})\n",
    "        chart_avg_sales.set_legend({'position': 'bottom'})\n",
    "        worksheet.insert_chart('B'+str(chartloc), chart_avg_sales) \n",
    "        \n",
    "        \n",
    "        # In[94]:\n",
    "        \n",
    "        sales_long_df_positive_sales=sales_long_df[sales_long_df['sales']>0]\n",
    "        store_counts=sales_long_df_positive_sales.groupby(['week_end_date'])['location_id'].count().to_frame()\n",
    "        store_counts.to_excel(writer,\"store_counts\")\n",
    "        \n",
    "        \n",
    "        # In[95]:\n",
    "        \n",
    "        all_store_dma=pd.read_excel(\"/home/jian/BigLots/static_files/all_store_DMA_20180602.xlsx\")\n",
    "        all_store_dma=all_store_dma[['location_id','DMA']]\n",
    "        store_counts_DMA=pd.merge(sales_long_df_positive_sales,all_store_dma,on='location_id',how='left')\n",
    "        store_counts_DMA=store_counts_DMA.groupby(['week_end_date','DMA'])['location_id'].count().to_frame()\n",
    "        store_counts_DMA.reset_index(inplace=True)\n",
    "        store_counts_DMA.to_excel(writer,\"store_counts_DMA\")\n",
    "        \n",
    "        \n",
    "        # In[96]:\n",
    "        \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "        # In[97]:\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        # # Weather Forecast\n",
    "        \n",
    "        # In[98]:\n",
    "        \n",
    "        import json\n",
    "        import datetime\n",
    "        Tuesday_StampDate_Str=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1))\n",
    "        Tuesday_today_str =Tuesday_StampDate_Str[0:4]+Tuesday_StampDate_Str[5:7]+Tuesday_StampDate_Str[8:10]\n",
    "        \n",
    "        '''     \n",
    "        # In[99]:\n",
    "        \n",
    "        host = \"64.237.51.251\" #hard-coded\n",
    "        port = 22\n",
    "        transport = paramiko.Transport((host, port))\n",
    "        \n",
    "        password = \"jian@juba2017\" #hard-coded\n",
    "        username = \"jian\" #hard-coded\n",
    "        transport.connect(username = username, password = password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        Jay_Forecast=sftp.listdir(\"/home/jzou/biglots/weather/forcast_api_response/\")\n",
    "        Jay_Actual=sftp.listdir(\"/home/jzou/biglots/weather/api_response/\")\n",
    "        \n",
    "        json_Forecast_list=[x.split(\"/\")[len(x.split(\"/\"))-1] for x in Jay_Forecast]\n",
    "        json_Actual_list=[x.split(\"/\")[len(x.split(\"/\"))-1] for x in Jay_Actual]\n",
    "        \n",
    "        existing_forecast=glob.glob(\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"+\"*.json\")\n",
    "        existing_actual=glob.glob(\"/home/jian/Projects/Big_Lots/Weather/Json_data/daily/\"+\"*.json\")\n",
    "        existing_forecast_list=[x.split(\"/\")[len(x.split(\"/\"))-1] for x in existing_forecast]\n",
    "        existing_actual_list=[x.split(\"/\")[len(x.split(\"/\"))-1] for x in existing_actual]\n",
    "        \n",
    "        new_Forecast_files=[x for x in json_Forecast_list if x not in existing_forecast_list]\n",
    "        new_Actual_files=[x for x in json_Actual_list if x not in existing_actual_list]\n",
    "        \n",
    "        \n",
    "        for new_Forecast in new_Forecast_files:\n",
    "            localpath=\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"+new_Forecast\n",
    "            remotepath=\"/home/jzou/biglots/weather/forcast_api_response/\"+new_Forecast\n",
    "            try:\n",
    "                os.stat(localpath)\n",
    "            except:\n",
    "                sftp.get(remotepath,localpath)\n",
    "                \n",
    "        for new_Actual in new_Actual_files:\n",
    "            localpath=\"/home/jian/Projects/Big_Lots/Weather/Json_data/daily/\"+new_Actual\n",
    "            remotepath=\"/home/jzou/biglots/weather/api_response/\"+new_Actual\n",
    "            try:\n",
    "                os.stat(localpath)\n",
    "            except:\n",
    "                sftp.get(remotepath,localpath)\n",
    "        sftp.close()\n",
    "        '''\n",
    " \n",
    "        # In[100]:\n",
    "        \n",
    "        weather_forecast_file_14=\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"+Tuesday_StampDate_Str+\": 14.json\" #2:00 p.m.        \n",
    "        weather_forecast_file_15=\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"+Tuesday_StampDate_Str+\": 15.json\" #3:00 p.m.\n",
    "        weather_forecast_file_16=\"/home/jian/Projects/Big_Lots/Weather/Json_data/forecast/\"+Tuesday_StampDate_Str+\": 16.json\" #4:00 p.m.\n",
    "        \n",
    "        \n",
    "        # In[101]:\n",
    "        \n",
    "        \n",
    "        # In[102]:\n",
    "        try:\n",
    "            json_forecast=json.load(open(weather_forecast_file_14,\"r\"))\n",
    "        except:\n",
    "            try:\n",
    "                json_forecast=json.load(open(weather_forecast_file_15,\"r\"))\n",
    "            except:\n",
    "                json_forecast=json.load(open(weather_forecast_file_16,\"r\"))\n",
    "        list_k=(5-datetime.datetime.strptime(Tuesday_StampDate_Str,\"%Y-%m-%d\").date().weekday())*8-1\n",
    "        inclusion_store_df=pd.read_table(newsalespath,dtype=str,sep=\"|\")\n",
    "        inclusion_store_df=inclusion_store_df[~inclusion_store_df['location_id'].isin(['145','6990'])]\n",
    "        \n",
    "        \n",
    "        # In[103]:\n",
    "        \n",
    "        inclusion_store_df['subclass_gross_sales_amt']=inclusion_store_df['subclass_gross_sales_amt'].astype(float)\n",
    "        inclusion_store_df['gross_transaction_cnt']=inclusion_store_df['gross_transaction_cnt'].astype(float)\n",
    "        \n",
    "        inclusion_store_df_trans=inclusion_store_df[['location_id','week_end_dt','gross_transaction_cnt']].drop_duplicates()\n",
    "        inclusion_store_df_sales=inclusion_store_df.groupby(['location_id','week_end_dt'])['subclass_gross_sales_amt'].sum().reset_index()\n",
    "        \n",
    "        inclusion_store_df_trans=inclusion_store_df_trans.rename(columns={\"gross_transaction_cnt\":\"transaction\"})\n",
    "        inclusion_store_df_sales=inclusion_store_df_sales.rename(columns={\"subclass_gross_sales_amt\":\"sales\"})\n",
    "        df_stores=pd.merge(inclusion_store_df_sales,inclusion_store_df_trans,on=[\"location_id\",\"week_end_dt\"],how=\"outer\")\n",
    "        \n",
    "        \n",
    "        # In[104]:\n",
    "        \n",
    "        store_list_txt=pd.read_table(\"/home/jian/BiglotsCode/OtherInput/MediaStormStores_20180703.txt\",sep=\"|\",dtype=str)\n",
    "        store_list_txt['zip_cd']=store_list_txt['zip_cd'].apply(lambda x: x.split(\"-\")[0].zfill(5))\n",
    "        store_list_txt=store_list_txt[['location_id','location_desc','address_line_1','address_line_2','city_nm','state_nm','zip_cd']]\n",
    "        \n",
    "        df_stores=pd.merge(df_stores,store_list_txt,on=\"location_id\",how=\"left\")\n",
    "        df_stores=df_stores.reset_index()\n",
    "        del df_stores['index']\n",
    "        \n",
    "        zip_DMA=pd.read_excel(\"/home/jian/Docs/Geo_mapping/Zips by DMA by County16-17 nielsen.xlsx\",skiprows=1,dtype=str)\n",
    "        zip_DMA=zip_DMA.iloc[:,[0,2]].drop_duplicates()\n",
    "        zip_DMA=zip_DMA.rename(columns={\"CODE\":\"zip_cd\",\"NAME\":\"DMA\"})\n",
    "        zip_DMA=zip_DMA.drop_duplicates(['zip_cd'])\n",
    "        df_stores=pd.merge(df_stores,zip_DMA,on=\"zip_cd\",how=\"left\")\n",
    "        \n",
    "        \n",
    "        # In[105]:\n",
    "        \n",
    "        group_weight=pd.read_excel(\"/home/jian/Projects/Big_Lots/Weather/Q1_Weather_Counts/Q1_inclusion_store_all_weather_type_ranked.xlsx\",sheetname=\"all_weather_group_list\")\n",
    "        group_weight['Severity']=group_weight['Severity'].astype(int)\n",
    "        group_weight=group_weight[['all_type_group','Severity']]\n",
    "        group_weight_dict=group_weight[['all_type_group', 'Severity']].set_index('all_type_group').T.to_dict()\n",
    "        \n",
    "        \n",
    "        # In[106]:\n",
    "        \n",
    "        output_forecast_weather=pd.DataFrame()\n",
    "        index_num=0\n",
    "        for zip_cd in df_stores['zip_cd'].unique().tolist():\n",
    "            if zip_cd in list(json_forecast.keys()):\n",
    "                weather_list=json_forecast[zip_cd]['list'][list_k]['weather']\n",
    "                forecast_time=datetime.datetime.fromtimestamp(json_forecast[zip_cd]['list'][list_k]['dt'])\n",
    "                all_forecast_group_value_zip=[]\n",
    "                all_forecast_desc_value_zip=[]\n",
    "                for j in range(len(weather_list)):\n",
    "                    weather_forecast_group=weather_list[j]['main']\n",
    "                    # weather_forecast_desc=weather_list[j]['description']\n",
    "                    all_forecast_group_value_zip=list(set(all_forecast_group_value_zip+[weather_forecast_group]))\n",
    "                    all_forecast_desc_value_zip=list(set(all_forecast_desc_value_zip+[weather_forecast_group]))\n",
    "        \n",
    "                all_forecast_group_severity_zip=[]\n",
    "                all_forecast_desc_severity_zip=[]\n",
    "                for k in range(len(all_forecast_group_value_zip)):\n",
    "                    forecast_severity_zip=group_weight_dict[all_forecast_group_value_zip[k]]['Severity']\n",
    "                    all_forecast_group_severity_zip=list(set(all_forecast_group_severity_zip+[forecast_severity_zip]))\n",
    "                    all_forecast_desc_severity_zip\n",
    "                    if k==0:\n",
    "                        selected_havest_forecast_group_value_zip=all_forecast_group_value_zip[0]\n",
    "                        # selected_havest_forecast_desc_value_zip=\n",
    "                    else:\n",
    "                        if group_weight_dict[all_forecast_group_value_zip[k]]['Severity']>group_weight_dict[selected_havest_forecast_group_value_zip]['Severity']:\n",
    "                            selected_havest_forecast_group_value_zip=all_forecast_group_value_zip[k]\n",
    "                            # selected_havest_forecast_desc_value_zip\n",
    "                forecast_max_severity_zip=max(all_forecast_group_severity_zip)\n",
    "        \n",
    "                df_app=pd.DataFrame({\"zip_cd\":zip_cd,\"Forecast_Time\":forecast_time,\"Forecast_Tuesday\":Tuesday_StampDate_Str,\"Forecast_Severity\":forecast_max_severity_zip,\n",
    "                                    \"Forecast_Weather_Type\":selected_havest_forecast_group_value_zip},index=[index_num])\n",
    "                index_num=index_num+1\n",
    "                output_forecast_weather=output_forecast_weather.append(df_app)\n",
    "            else:\n",
    "                print(zip_cd,\"Not Collected\")\n",
    "                df_app=pd.DataFrame({\"zip_cd\":zip_cd,\"Forecast_Time\":\"Not_Collected\",\"Forecast_Tuesday\":\"Not_Collected\",\"Forecast_Severity\":'Not_Collected',\n",
    "                                    \"Forecast_Weather_Type\":'Not_Collected'},index=[index_num])\n",
    "                index_num=index_num+1\n",
    "                output_forecast_weather=output_forecast_weather.append(df_app)\n",
    "        \n",
    "        \n",
    "        # In[107]:\n",
    "        \n",
    "        output=pd.merge(df_stores,output_forecast_weather,on=\"zip_cd\",how=\"left\")\n",
    "        week_end_date=output['week_end_dt'].unique()[0]\n",
    "        Saturday_str=str(output[output['Forecast_Time']!='Not_Collected']['Forecast_Time'].apply(lambda x: x.date()).unique()[0])\n",
    "        output['location_id']=output['location_id'].astype(int)\n",
    "        output=output.sort_values('location_id')\n",
    "        \n",
    "        \n",
    "        # In[108]:\n",
    "        \n",
    "        output.to_csv(\"/home/jian/BiglotsCode/outputs/Output_\"+week_end_date+\"/weather_forecast_for_Saturday_\"+Saturday_str+\".csv\",index=False)\n",
    "        output.to_csv(Simeng_recent_weekly_data_folder+\"weather_forecast_for_Saturday_\"+Saturday_str+\".csv\",index=False)\n",
    "        \n",
    "        \n",
    "        # In[ ]:\n",
    "        for col in ['zip_cd','location_desc','DMA','address_line_1','address_line_2','city_nm','state_nm']:\n",
    "            output=output.rename(columns={col:\"Store_\"+col})\n",
    "        \n",
    "        \n",
    "        \n",
    "        zip_store_1_to_1=pd.read_csv(\"/home/jian/BiglotsCode/OtherInput/Zip_in_TA_associated_Store_1_to_1.csv\",dtype=str)\n",
    "        zip_store_1_to_1['location_id']=zip_store_1_to_1['location_id'].astype(int)\n",
    "        zip_store_1_to_1['distance']=zip_store_1_to_1['distance'].replace(\"Zip_Center_Missing\",\"-999\")\n",
    "        zip_store_1_to_1['distance']=zip_store_1_to_1['distance'].astype(float).apply(lambda x: round(x,2))\n",
    "        zip_store_1_to_1['distance']=zip_store_1_to_1['distance'].replace(-999.0,\"NA\")\n",
    "        \n",
    "        \n",
    "        new_store_list=[x for x in output['location_id'].unique().tolist() if x not in zip_store_1_to_1['location_id'].unique().tolist()]\n",
    "        \n",
    "        gc.collect()\n",
    "        store_list_1=pd.read_table(\"/home/jian/BiglotsCode/OtherInput/MediaStormStoreList_Nov15.txt\",dtype=str,sep=\"|\")\n",
    "        store_list_2=pd.read_table(\"/home/jian/BiglotsCode/OtherInput/MediaStormStores_20180703.txt\",dtype=str,sep=\"|\")\n",
    "        \n",
    "        store_list_1=store_list_1[['location_id','longitude_meas','latitude_meas']]\n",
    "        store_list_2=store_list_2[['location_id','longitude_meas','latitude_meas']]\n",
    "        store_list_1=store_list_1[~store_list_1['location_id'].isin(store_list_2['location_id'])]\n",
    "        store_list=store_list_2.append(store_list_1)\n",
    "        store_list=store_list[store_list['location_id']!=\"6990\"]\n",
    "        store_list=store_list[store_list['location_id']!=\"145\"]\n",
    "        \n",
    "        store_list['location_id']=store_list['location_id'].astype(int)\n",
    "        store_list['latitude_meas']=store_list['latitude_meas'].astype(float)\n",
    "        store_list['longitude_meas']=store_list['longitude_meas'].astype(float)\n",
    "        \n",
    "        i=0\n",
    "        import haversine\n",
    "        zip_centers=json.load(open(\"/home/jian/Docs/Geo_mapping/center_of_rentrak_zip.json\",\"r\"))\n",
    "        \n",
    "        df_new_store_zips=pd.DataFrame()\n",
    "        for x in new_store_list:\n",
    "            df_store_x=store_list[store_list['location_id']==x]\n",
    "            if len(df_store_x)>=1:\n",
    "                store_loc_lat=df_store_x['latitude_meas'].unique().tolist()[0]\n",
    "                store_loc_long=df_store_x['longitude_meas'].unique().tolist()[0]\n",
    "                store_loc=(store_loc_lat,store_loc_long)\n",
    "                \n",
    "                for zip_cd in list(zip_centers.keys()):\n",
    "                    dist=haversine.haversine(store_loc,zip_centers[zip_cd],miles=True)\n",
    "                    if dist<=10:\n",
    "                        df=pd.DataFrame({\"zip_cd\":zip_cd,\"revenue_flag\":\"10miles_new_store\",\"location_id\":x,\"distance\":round(dist,2)},index=[i])\n",
    "                        i=i+1\n",
    "                        df_new_store_zips=df_new_store_zips.append(df)\n",
    "            else:\n",
    "                print(\"store \"+str(x)+\" not in store_list (no lat/long)\")\n",
    "                logging.info(\"store \"+str(x)+\" not in store_list (no lat/long)\")\n",
    "                df=pd.DataFrame({\"zip_cd\":'new store',\"revenue_flag\":\"new store\",\"location_id\":x,\"distance\":round(0,2)},index=[i])\n",
    "                df_new_store_zips=df_new_store_zips.append(df)\n",
    "                \n",
    "        \n",
    "        df_new_store_zips=df_new_store_zips[~df_new_store_zips['zip_cd'].isin(zip_store_1_to_1['zip_cd'])]\n",
    "        zip_store_1_to_1=zip_store_1_to_1.append(df_new_store_zips)\n",
    "        output_2_by_zip=pd.merge(output,zip_store_1_to_1,on=\"location_id\",how=\"left\")\n",
    "        output_2_by_zip['zip_cd']=output_2_by_zip['zip_cd'].fillna(\"NewStore_in_TA\")\n",
    "        output_2_by_zip['distance']=output_2_by_zip['distance'].fillna(\"NewStore_in_TA\")\n",
    "        output_2_by_zip['revenue_flag']=output_2_by_zip['revenue_flag'].fillna(\"NewStore_in_TA\")\n",
    "        output_2_by_zip=output_2_by_zip.rename(columns={\"zip_cd\":\"TA_Zips\"})\n",
    "        \n",
    "        output_2_by_zip=output_2_by_zip[['TA_Zips','location_id','revenue_flag','distance','week_end_dt','sales','transaction','Store_location_desc',\n",
    "                                        'Store_address_line_1','Store_address_line_2','Store_city_nm','Store_state_nm','Store_zip_cd','Store_DMA','Forecast_Severity','Forecast_Tuesday',\n",
    "                                        'Forecast_Weather_Type','Forecast_Time']]\n",
    "        output_2_by_zip.to_csv(\"/home/jian/BiglotsCode/outputs/Output_\"+week_end_date+\"/By_Zip_weather_forecast_for_Saturday_\"+Saturday_str+\".csv\",index=False)\n",
    "        output_2_by_zip.to_csv(Simeng_recent_weekly_data_folder+\"By_Zip_weather_forecast_for_Saturday_\"+Saturday_str+\".csv\",index=False)\n",
    "        \n",
    "        \n",
    "        # Transfer bi-weekly data for Spencer\n",
    "        host = \"64.237.51.251\" #hard-coded\n",
    "        port = 22\n",
    "        transport = paramiko.Transport((host, port))\n",
    "        \n",
    "        password = \"jian@juba2017\" #hard-coded\n",
    "        username = \"jian\" #hard-coded\n",
    "        transport.connect(username = username, password = password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        \n",
    "        Saturday_str_for_SP=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3)\n",
    "        Saturday_str_for_SP=str(Saturday_str_for_SP.year)+\"_\"+str(Saturday_str_for_SP.month)+\"_\"+str(Saturday_str_for_SP.day)\n",
    "        remote_new_folder=\"/home/nielsen/Spencer/BigLotsCRM/inputdata/MediaStorm_\"+Saturday_str_for_SP+\"/\"\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        thisweeksdate=str(datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday()-1+3))\n",
    "        localpath=\"/home/jian/BigLots/MediaStorm_\"+thisweeksdate+\"/\"\n",
    "        bi_weekly_data=glob.glob(localpath+\"*.txt\")\n",
    "        bi_weekly_data=[x for x in bi_weekly_data if \"BiWeekly\" in x]\n",
    "        \n",
    "        if len(bi_weekly_data)>0:\n",
    "            try:\n",
    "                sftp.chdir(remote_new_folder)  # Test if remote_path exists\n",
    "            except IOError:\n",
    "                sftp.mkdir(remote_new_folder)  # Create remote_path\n",
    "                sftp.chdir(remote_new_folder)\n",
    "                \n",
    "        \n",
    "        if len(bi_weekly_data)==0:\n",
    "            message = \"\"\"From: Jian Liang <jliangmkt@gmail.com>\n",
    "To: Jian <jian@jubaplus.com>, Spencer Zhao <spencer@jubaplus.com>, Simeng Sun <ssun@mediastorm.biz>\n",
    "MIME-Version: 1.0\n",
    "Content-type: text\n",
    "Subject: No Bi-weekly Big Lots data this week\n",
    "\n",
    "Hi Spencer & Simeng,\n",
    "\n",
    "There is no bi-weekly Big Lots CRM data this week. The weekly data is ready to be uploaded into Datorama.\n",
    "\n",
    "Thanks,\n",
    "Jian\n",
    "            \"\"\"\n",
    "            \n",
    "        else:\n",
    "            for bi_weekly_file in bi_weekly_data:\n",
    "                sftp.put(bi_weekly_file,remote_new_folder+bi_weekly_file.split(\"/\")[len(bi_weekly_file.split(\"/\"))-1])\n",
    "                \n",
    "            message = \"\"\"From: Jian Liang <jliangmkt@gmail.com>\n",
    "To: Jian <jian@jubaplus.com>, Spencer Zhao <spencer@jubaplus.com>, Simeng Sun <ssun@mediastorm.biz>\n",
    "MIME-Version: 1.0\n",
    "Content-type: text\n",
    "Subject: Bi-weekly Big Lots data transfered\n",
    "\n",
    "Hi Spencer & Simeng,\n",
    "\n",
    "The bi-weekly Big Lots CRM data is uploaded to your folder. The weekly data is ready to be uploaded into Datorama.\n",
    "\n",
    "Thanks,\n",
    "Jian\n",
    "            \"\"\"\n",
    "            \n",
    "        \n",
    "        sftp.close()\n",
    "        \n",
    "        smtpObj = smtplib.SMTP('smtp.gmail.com',587)\n",
    "        smtpObj.ehlo()\n",
    "        smtpObj.starttls()\n",
    "        smtpObj.login('jliangmkt@gmail.com','jlj198256')\n",
    "        \n",
    "        \n",
    "        sender=\"jliangmkt@gmail.com\"\n",
    "        receivers=['jian@jubaplus.com','spencer@jubaplus.com','ssun@mediastorm.biz']\n",
    "        try:\n",
    "           smtpObj.sendmail(sender, receivers, message)         \n",
    "           print(\"Successfully sent email\")\n",
    "        except:\n",
    "           print(\"Error: unable to send email\")\n",
    "\n",
    "\n",
    "\n",
    "######### Upload to LiveRamp-Bing\n",
    "        import pandas as pd\n",
    "        import datetime\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import hashlib\n",
    "        import gc\n",
    "\n",
    "        def recursive_file_gen(my_root_dir):\n",
    "            for root, dirs, files in os.walk(my_root_dir):\n",
    "                for file in files:\n",
    "                    yield os.path.join(root, file)\n",
    "    \t            \n",
    "\n",
    "\n",
    "        thismonday=datetime.datetime.now().date()-datetime.timedelta(days=datetime.datetime.now().date().weekday())\n",
    "    \t# thismonday=datetime.date(2019,3,25)\n",
    "        print(\"thismonday\", thismonday)\n",
    "\n",
    "        last_week_end_saturday=thismonday-datetime.timedelta(days=2)\n",
    "\n",
    "        writer_pather=\"/home/jian/celery/Bing_LiveRamp/output/\"\n",
    "\n",
    "        posibble_recent_folder=\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/\"\n",
    "        daily_files_recent=[x for x in list(recursive_file_gen(posibble_recent_folder)) if \"Daily\" in x]\n",
    "\n",
    "        list_1_after_201806_2019=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"aily\" in x) & (\".txt\" in x) ]\n",
    "        list_1_after_201806_2019=[x for x in list_1_after_201806_2019 if str(last_week_end_saturday) in x]\n",
    "\n",
    "\n",
    "        daily_files_last_week=daily_files_recent+list_1_after_201806_2019\n",
    "        if len(daily_files_last_week)==1:\n",
    "            daily_file_last_week=daily_files_last_week[0]\n",
    "            print(\"Good to load\")\n",
    "        else:\n",
    "            daily_file_last_week=np.nan\n",
    "            print(\"Last week daily data not avaiable\")\n",
    "\n",
    "\n",
    "\n",
    "        qc_weekly=pd.read_table(daily_file_last_week,sep=\"|\",dtype=str)\n",
    "        qc_weekly=qc_weekly[qc_weekly['location_id']!=\"6990\"]\n",
    "        qc_weekly['item_transaction_amt']=qc_weekly['item_transaction_amt'].astype(float)\n",
    "        qc_weekly_sales=qc_weekly.groupby(['location_id'])['item_transaction_amt'].sum().to_frame().reset_index().rename(columns={\"item_transaction_amt\":\"sales_from_Daily\"})\n",
    "        qc_weekly_trans=qc_weekly[['location_id','transaction_dt','transaction_id','customer_id_hashed']].drop_duplicates()\n",
    "        qc_weekly_trans=qc_weekly_trans.groupby(['location_id'])['transaction_id'].count().to_frame().reset_index().rename(columns={\"transaction_id\":\"trans_from_Daily\"})\n",
    "\n",
    "        qc_weekly_from_daily=pd.merge(qc_weekly_sales,qc_weekly_trans,on=\"location_id\",how=\"outer\")\n",
    "        qc_weekly_from_daily.shape\n",
    "\n",
    "\n",
    "        import glob\n",
    "        weekly_data_path=glob.glob(\"/home/jian/BigLots/2019_by_weeks/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "        weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "        if len(weekly_data_path)==0:    \n",
    "            weekly_data_path=glob.glob(\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/*\")\n",
    "            weekly_data_path=[x for x in weekly_data_path if \"SalesWeekly\" in x]\n",
    "            \n",
    "        if len(weekly_data_path)==1:\n",
    "            weekly_data_path=weekly_data_path[0]\n",
    "            \n",
    "        else:\n",
    "            print(\"Checking the new weekly data\")\n",
    "\n",
    "            \n",
    "        Weekly_Data=pd.read_table(weekly_data_path,dtype=str,sep=\"|\",usecols=[\"location_id\",'week_end_dt','gross_sales_amt','gross_transaction_cnt'])\n",
    "        Weekly_Data=Weekly_Data[Weekly_Data['location_id']!=\"6990\"]\n",
    "        Weekly_Data=Weekly_Data.drop_duplicates()\n",
    "        Weekly_Data['gross_sales_amt']=Weekly_Data['gross_sales_amt'].astype(float)\n",
    "        Weekly_Data['gross_transaction_cnt']=Weekly_Data['gross_transaction_cnt'].astype(int)\n",
    "\n",
    "\n",
    "        QC_df=pd.merge(Weekly_Data,qc_weekly_from_daily,on=\"location_id\",how=\"outer\")\n",
    "    \t\n",
    "\n",
    "        QC_df['Sales_Diff']=(QC_df['gross_sales_amt']-QC_df['sales_from_Daily'])/QC_df['sales_from_Daily']\n",
    "        QC_df['Trans_Diff']=(QC_df['gross_transaction_cnt']-QC_df['trans_from_Daily'])/QC_df['gross_transaction_cnt']\n",
    "\n",
    "        print(\"1% store sales variances: \"+str(QC_df[(QC_df['Sales_Diff'].apply(lambda x: np.abs(x)>0.01))].shape[0]))\n",
    "        print(\"4% store trans variances: \"+str(QC_df[(QC_df['Trans_Diff'].apply(lambda x: np.abs(x)>0.04))].shape[0]))\n",
    "\n",
    "\n",
    "        sales_daily_lastweek=pd.read_table(daily_file_last_week,sep=\"|\",dtype=str,usecols=['location_id','customer_id_hashed','transaction_dt','item_transaction_amt'])\n",
    "        sales_daily_lastweek=sales_daily_lastweek[~pd.isnull(sales_daily_lastweek['customer_id_hashed'])]\n",
    "        sales_daily_lastweek=sales_daily_lastweek[sales_daily_lastweek['location_id']!=\"6990\"]\n",
    "        sales_daily_lastweek['item_transaction_amt']=sales_daily_lastweek['item_transaction_amt'].astype(float)\n",
    "        sales_daily_lastweek_agg=sales_daily_lastweek.groupby(['customer_id_hashed','transaction_dt'])['item_transaction_amt'].sum().to_frame().reset_index()\n",
    "        sales_daily_lastweek_agg=sales_daily_lastweek_agg.rename(columns={\"transaction_dt\":\"Timestamp\",\"item_transaction_amt\":\"Conversion_Amount\"})\n",
    "        sales_daily_lastweek_agg['Timestamp']=sales_daily_lastweek_agg['Timestamp'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "\n",
    "\n",
    "        posibble_recent_Master=\"/home/jian/BigLots/MediaStorm_\"+str(last_week_end_saturday)+\"/\"\n",
    "        master_files_recent=[x for x in list(recursive_file_gen(posibble_recent_Master)) if \"ster\" in x]\n",
    "\n",
    "        if len(master_files_recent)==1:\n",
    "            master_files_recent=master_files_recent[0]\n",
    "            print(\"Good to load\")\n",
    "        else:\n",
    "            master_files_recent=np.nan\n",
    "            print(\"Last week Master file not avaiable, Check the cell below if already in\")\n",
    "    \t    \n",
    "        recent_date=last_week_end_saturday\n",
    "\n",
    "        Master_2019_weekly=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2019_by_weeks/\")) if (\"aster\" in x) & (\".txt\" in x) ]\n",
    "        Master_2018_weekly=[x for x in list(recursive_file_gen(\"/home/jian/BigLots/2018_by_weeks/\")) if (\"aster\" in x) & (\".txt\" in x) ]\n",
    "\n",
    "\n",
    "        weekly_df=pd.DataFrame({\"file_path\":Master_2019_weekly+Master_2018_weekly})\n",
    "        weekly_df['date']=weekly_df['file_path'].apply(lambda x: x.split(\"_by_weeks/MediaStorm_\")[1][:10])\n",
    "        weekly_df['date']=weekly_df['date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\").date())\n",
    "        weekly_df=weekly_df.sort_values(\"date\",ascending=False).reset_index()\n",
    "        del weekly_df['index']\n",
    "\n",
    "        if pd.notnull(master_files_recent):\n",
    "            weekly_df=pd.DataFrame({\"file_path\":master_files_recent,\"date\":recent_date},index=[0]).append(weekly_df)\n",
    "        weekly_df=weekly_df.drop_duplicates().reset_index()\n",
    "        del weekly_df['index']     \n",
    "        \n",
    "\n",
    "        data_0=pd.read_table(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStorm_Lapsed_Reward_Member_Master_from2014-08-26to2017-02-26.zip\",\n",
    "                         dtype=str,sep=\"|\",usecols=['customer_id_hashed','email_address_hash','customer_zip_code']).drop_duplicates()\n",
    "        data_1=pd.read_csv(\"/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/MediaStormCustTot-hashed-email.txt\",\n",
    "    \t                     dtype=str,header=None,usecols=[0,1,5])\n",
    "        data_1.columns=['customer_id_hashed','email_address_hash','customer_zip_code']\n",
    "        data_1['customer_id_hashed']=data_1['customer_id_hashed'].apply(lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest())\n",
    "        data_2 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'MediaStormCustomerTransactionTotals_2018-01-09_2018-03-31.txt',\n",
    "    \t                     sep = ',',dtype = str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "        print(data_2.shape)\n",
    "        data_3 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'Existing Reward Member Master as of 2018-06-05.txt',\n",
    "    \t                     dtype = str,sep = '|',usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "        print(data_3.shape)\n",
    "        data_4 = pd.read_csv('/home/jian/Projects/Big_Lots/Loyal_members/loyalty_register_data/'+'New Reward Member Master as of 2018-06-05.txt',\n",
    "    \t                     dtype = str,sep = '|',usecols=['customer_id_hashed','email_address_hash','customer_zip_code'])\n",
    "        print(data_4.shape)\n",
    "\n",
    "        master_old=data_4.append(data_3).append(data_2).append(data_1).append(data_0).drop_duplicates()\n",
    "        del data_4\n",
    "        del data_3\n",
    "        del data_2\n",
    "        del data_1\n",
    "        del data_0\n",
    "        gc.collect()\n",
    "\n",
    "        all_weekly_biweekly_master_file=pd.DataFrame()\n",
    "        for file_path in weekly_df['file_path'].tolist():\n",
    "            df=pd.read_table(file_path,dtype=str,usecols=['customer_id_hashed','email_address_hash','customer_zip_code'],sep=\"|\")\n",
    "            all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.append(df)\n",
    "\n",
    "        all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.append(master_old)\n",
    "        print(all_weekly_biweekly_master_file.shape)\n",
    "        print(len(all_weekly_biweekly_master_file['customer_id_hashed'].unique()))\n",
    "\n",
    "        all_weekly_biweekly_master_file=all_weekly_biweekly_master_file.drop_duplicates('customer_id_hashed')\n",
    "\n",
    "        sales_daily_lastweek_agg=pd.merge(sales_daily_lastweek_agg,all_weekly_biweekly_master_file,on=\"customer_id_hashed\",how=\"left\").rename(columns={\"email_address_hash\":\"Email_1\",\"customer_zip_code\":\"Zip\"})\n",
    "        sales_daily_lastweek_agg.head(2)\n",
    "\n",
    "\n",
    "        print(\"Null Email rows excluded: \"+str(sales_daily_lastweek_agg[pd.isnull(sales_daily_lastweek_agg['Email_1'])].shape[0]))\n",
    "        print(sales_daily_lastweek_agg.shape[0]/sales_daily_lastweek_agg[pd.isnull(sales_daily_lastweek_agg['Email_1'])].shape[0])\n",
    "\n",
    "        sales_daily_lastweek_agg=sales_daily_lastweek_agg[~pd.isnull(sales_daily_lastweek_agg['Email_1'])]\n",
    "        del sales_daily_lastweek_agg['customer_id_hashed']\n",
    "\n",
    "        sales_daily_lastweek_agg=sales_daily_lastweek_agg[[\"Email_1\",\"Zip\",\"Timestamp\", \"Conversion_Amount\"]]\n",
    "        sales_daily_lastweek_agg['Conversion_Amount']=sales_daily_lastweek_agg['Conversion_Amount'].apply(lambda x: np.round(x,2)).astype(str)\n",
    "        sales_daily_lastweek_agg['Conversion_Amount']=sales_daily_lastweek_agg['Conversion_Amount'].apply(lambda x: x.split(\".\")[0]+\".\"+x.split(\".\")[1].ljust(2,\"0\"))\n",
    "        sales_daily_lastweek_agg['Product_Group']=\"In_Store\"\n",
    "\n",
    "        sales_daily_lastweek_agg['Zip']=\"00000\"\n",
    "\n",
    "        data_max_date=sales_daily_lastweek_agg['Timestamp'].max()\n",
    "        data_max_date\n",
    "\n",
    "        data_min_date=sales_daily_lastweek_agg['Timestamp'].min()\n",
    "        data_min_date\n",
    "\n",
    "        local_path=writer_pather+\"/BL_LR_BingStoreSales_\"+str(data_min_date)+\"_\"+str(data_max_date)+\"_JL_\"+str(datetime.datetime.now().date())+\".txt\"\n",
    "\n",
    "        sales_daily_lastweek_agg.to_csv(local_path,index=False,sep=\"|\")\n",
    "\n",
    "\n",
    "        import paramiko\n",
    "\n",
    "        host = \"sftp.liveramp.com\" #hard-coded\n",
    "        port = 22\n",
    "        transport = paramiko.Transport((host, port))\n",
    "\n",
    "        password = \"Jubaplus2019!\" #hard-coded\n",
    "        username = \"bing-big-lots\" #hard-coded\n",
    "        transport.connect(username = username, password = password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "    \t# local_path defined above before saving the local txt\n",
    "        remote_path=\"/uploads/\"+os.path.basename(local_path)\n",
    "        sftp.put(local_path,remote_path)\n",
    "        sftp.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Google below from the output of Bing\n",
    "        df_google=sales_daily_lastweek_agg.rename(columns={\"Zip\":\"Zip_Code\",\n",
    "          \"Timestamp\":\"transaction_timestamp\",\n",
    "          \"Product_Group\":\"transaction_category\",\n",
    "          \"Conversion_Amount\":\"transaction_amount\"})\n",
    "\n",
    "        df_google=df_google[['Zip_Code','Email_1','transaction_category','transaction_timestamp','transaction_amount']]\n",
    "\n",
    "        local_path_google=\"/home/jian/celery/Google_LiveRamp/output/BL_LR_GoogleStoreSales_\"+str(data_min_date)+\"_\"+str(data_max_date)+\"_JL_\"+str(datetime.datetime.now().date())+\".txt\"\n",
    "        df_google.to_csv(local_path_google,index=False,sep=\"|\")\n",
    "\n",
    "        import paramiko\n",
    "\n",
    "        host = \"sftp.liveramp.com\" #hard-coded\n",
    "        port = 22\n",
    "        transport = paramiko.Transport((host, port))\n",
    "\n",
    "        password = \"Jubaplus2019!\" #hard-coded\n",
    "        username = \"big-lots-ga-aw\" #hard-coded\n",
    "        transport.connect(username = username, password = password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "        remote_path=\"/uploads/\"+os.path.basename(local_path_google)\n",
    "        sftp.put(local_path_google,remote_path)\n",
    "        sftp.close()\n",
    "\n",
    "\n",
    "        message = \"\"\"From: Juba <jubapluscc@gmail.com>\n",
    "To: Jian <jian@jubaplus.com>, Mike Mahler <mmahler@mediastorm.biz>\n",
    "MIME-Version: 1.0\n",
    "Content-type: text\n",
    "Subject: Big Lots Rewards Sales in Store uploaded to LiveRamp \n",
    "\n",
    "Hi Mike,\n",
    "\n",
    "The last week Big Lots Rewards Sales in Store uploaded to LiveRamp Bing & Google.\n",
    "\n",
    "Thanks,\n",
    "Jian\n",
    "\"\"\"\n",
    "        smtpObj = smtplib.SMTP('smtp.gmail.com',587)\n",
    "        smtpObj.ehlo()\n",
    "        smtpObj.starttls()\n",
    "        smtpObj.login('jubapluscc@gmail.com','jubaplus2015')\n",
    "        \n",
    "        \n",
    "        sender=\"jubapluscc@gmail.com\"\n",
    "        receivers=['jian@jubaplus.com','mmahler@mediastorm.biz']\n",
    "        try:\n",
    "            smtpObj.sendmail(sender, receivers, message)         \n",
    "            print(\"Successfully sent email\")\n",
    "        except:\n",
    "            print(\"Error: unable to send email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biglots_weekly()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
