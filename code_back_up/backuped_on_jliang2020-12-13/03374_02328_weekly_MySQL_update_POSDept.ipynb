{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MySQL: Pred_CRM_table\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import gc\n",
    "import logging\n",
    "import sqlalchemy\n",
    "import glob\n",
    "import paramiko\n",
    "\n",
    "BL_SQL_CONNECTION= 'mysql+pymysql://jian:JubaPlus-2017@localhost/BigLots' \n",
    "BL_engine = sqlalchemy.create_engine(\n",
    "        BL_SQL_CONNECTION, \n",
    "        pool_recycle=1800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred_POS_Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jian/BigLots/static_files/ProductTaxonomy/MediaStormProductTaxonomy20200401-135137-346.txt\n"
     ]
    }
   ],
   "source": [
    "file_product_taxonomy=glob.glob(\"/home/jian/BigLots/static_files/ProductTaxonomy/*.txt\")\n",
    "file_product_taxonomy=[x for x in file_product_taxonomy if \"taxonomy\" in x.lower()]\n",
    "file_product_taxonomy.sort()\n",
    "file_product_taxonomy=file_product_taxonomy[-1]\n",
    "\n",
    "print(file_product_taxonomy)\n",
    "\n",
    "df_prod_taxo_dep=pd.read_csv(file_product_taxonomy,dtype=str,sep=\"|\")\n",
    "df_prod_taxo_dep=df_prod_taxo_dep[['department_id','class_code_id','subclass_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL_max_date_POSDepart: 2020-03-21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-03-28/MediaStormDailySales20200331-112303-873.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recursive_file_gen(my_root_dir):\n",
    "    for root, dirs, files in os.walk(my_root_dir):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "MySQL_max_date_POSDepart=pd.read_sql(\"select max(transaction_dt) from Pred_POS_Department;\",con=BL_engine)\n",
    "MySQL_max_date_POSDepart=str(MySQL_max_date_POSDepart.iloc[0,0])\n",
    "\n",
    "print(\"MySQL_max_date_POSDepart: \"+MySQL_max_date_POSDepart)\n",
    "\n",
    "files_item_POS_plain=list(recursive_file_gen(\"/home/jian/BigLots/\"))\n",
    "files_item_POS_plain=[x for x in files_item_POS_plain if x[-4:]==\".txt\" and \"dailysales\" in x.lower() and \"/MediaStorm_\" in x]\n",
    "files_item_POS_plain=[x for x in files_item_POS_plain if x.split(\"/MediaStorm_\")[1][:10]>MySQL_max_date_POSDepart]\n",
    "files_item_POS_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=pd.read_sql(\"select * from Pred_POS_Department limit 2;\",con=BL_engine).columns.tolist()\n",
    "\n",
    "df_trans_order_by_id=pd.read_sql(\"select customer_id_hashed, max(trans_order_since_18Q1) as trans_order_since_18Q1 from Pred_POS_Department \\\n",
    "group by customer_id_hashed;\",con=BL_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jian/BigLots/2020_by_weeks/MediaStorm_2020-03-28/MediaStormDailySales20200331-112303-873.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_item_POS_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25750288, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans_order_by_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 19:50:52.305015\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 19:50:52.333333 start /home/jian/BigLots/2020_by_weeks/MediaStorm_2020-03-28/MediaStormDailySales20200331-112303-873.txt\n",
      "2020-03-22 2020-03-28 2020-04-05 19:55:31.751217\n",
      "done of file:  1 /home/jian/BigLots/2020_by_weeks/MediaStorm_2020-03-28/MediaStormDailySales20200331-112303-873.txt\n",
      "2020-04-05 21:24:12.881208 done /home/jian/BigLots/2020_by_weeks/MediaStorm_2020-03-28/MediaStormDailySales20200331-112303-873.txt\n"
     ]
    }
   ],
   "source": [
    "total_sales=0\n",
    "i_counter=0\n",
    "for file in files_item_POS_plain:\n",
    "    print(datetime.datetime.now(),\"start\",file)\n",
    "    df=pd.read_csv(file,dtype=str,nrows=None,sep=\"|\")\n",
    "    df=df.rename(columns={\"subclass_transaction_amt\":\"sales\"})\n",
    "    df=df.rename(columns={\"item_transaction_amt\":\"sales\"})\n",
    "    \n",
    "    df=df.rename(columns={\"subclass_transaction_units\":\"units\"})\n",
    "    df=df.rename(columns={\"item_transaction_units\":\"units\"})\n",
    "    \n",
    "    df['sales']=df['sales'].astype(float)\n",
    "    df['units']=df['units'].astype(int)\n",
    "    \n",
    "    \n",
    "    df=pd.merge(df,df_prod_taxo_dep,on=['class_code_id','subclass_id'],how=\"left\")\n",
    "    df['department_id']=df['department_id'].fillna(\"-1\")\n",
    "    df['customer_id_hashed']=df['customer_id_hashed'].fillna(\"non_rewards\")\n",
    "    \n",
    "    df=df.groupby(['location_id','transaction_dt','transaction_id','customer_id_hashed','department_id'])['sales','units'].sum().reset_index()\n",
    "    df=df.sort_values(['customer_id_hashed','transaction_dt','location_id','transaction_id','department_id'])\n",
    "    \n",
    "    # add the transaction_order\n",
    "    df_order_this_week_rewards=df[df['customer_id_hashed']!=\"non_rewards\"]\n",
    "    df_order_this_week_rewards=df_order_this_week_rewards[['customer_id_hashed','transaction_dt','location_id','transaction_id']].drop_duplicates()\n",
    "    df_order_this_week_rewards=df_order_this_week_rewards.sort_values(['customer_id_hashed','transaction_dt','location_id','transaction_id'])\n",
    "    df_order_this_week_rewards['trans_order_in_week']=pd.Categorical(df_order_this_week_rewards['customer_id_hashed']+ '_'+\\\n",
    "                                                                     df_order_this_week_rewards['transaction_dt']+ '_'+\\\n",
    "                                                                     df_order_this_week_rewards['location_id']+ '_'+\\\n",
    "                                                                     df_order_this_week_rewards['transaction_id']\n",
    "                                                                    ).codes\n",
    "\n",
    "    df_min_index_per_id=df_order_this_week_rewards[['customer_id_hashed','trans_order_in_week']].sort_values(['customer_id_hashed','trans_order_in_week'],ascending=[True,True]).drop_duplicates(\"customer_id_hashed\")\n",
    "    df_min_index_per_id=df_min_index_per_id.rename(columns={\"trans_order_in_week\":\"min_order\"})\n",
    "    df_order_this_week_rewards=pd.merge(df_order_this_week_rewards,df_min_index_per_id,on=\"customer_id_hashed\",how=\"left\")\n",
    "    df_order_this_week_rewards['trans_order_in_week']=df_order_this_week_rewards['trans_order_in_week']-df_order_this_week_rewards['min_order']+1\n",
    "    \n",
    "    df_order_this_week_rewards=pd.merge(df_order_this_week_rewards,df_trans_order_by_id,on='customer_id_hashed',how=\"left\")\n",
    "    df_order_this_week_rewards['trans_order_since_18Q1']=df_order_this_week_rewards['trans_order_since_18Q1'].fillna(0)\n",
    "    df_order_this_week_rewards['trans_order_since_18Q1']=df_order_this_week_rewards['trans_order_since_18Q1']+df_order_this_week_rewards['trans_order_in_week']\n",
    "    df_order_this_week_rewards=df_order_this_week_rewards[['customer_id_hashed','transaction_dt','location_id','transaction_id','trans_order_since_18Q1']]\n",
    "    df=pd.merge(df,df_order_this_week_rewards,on=['customer_id_hashed','transaction_dt','location_id','transaction_id'],how=\"left\")\n",
    "\n",
    "    #\n",
    "    df_order_this_week_rewards=df_order_this_week_rewards[['customer_id_hashed','trans_order_since_18Q1']].sort_values([\"customer_id_hashed\",\"trans_order_since_18Q1\"],ascending=[True,False]).drop_duplicates(\"customer_id_hashed\")\n",
    "    df_trans_order_by_id=df_order_this_week_rewards.append(df_trans_order_by_id).drop_duplicates(\"customer_id_hashed\")\n",
    "    \n",
    "    # format\n",
    "    df['location_id']=df['location_id'].astype(int)\n",
    "    df['transaction_dt']=pd.to_datetime(df['transaction_dt'],format=\"%Y-%m-%d\").dt.date\n",
    "    df['customer_id_hashed']=df['customer_id_hashed'].replace(\"non_rewards\",np.nan)\n",
    "    df=df.round({'sales': 2})\n",
    "    \n",
    "    print(df['transaction_dt'].min(),df['transaction_dt'].max(),datetime.datetime.now())\n",
    "    logging.info(str(df['transaction_dt'].min())+\" | \"+str(df['transaction_dt'].max())+\" | \"+str(datetime.datetime.now()))\n",
    "    \n",
    "\n",
    "    df.to_sql(\"Pred_POS_Department\",if_exists='append', con=BL_engine, index=False,chunksize=300000,\n",
    "                    dtype={\n",
    "                        'location_id':sqlalchemy.types.INTEGER(),\n",
    "                        'transaction_dt':sqlalchemy.Date(), \n",
    "                        'transaction_id':sqlalchemy.types.VARCHAR(length=16),\n",
    "                        'customer_id_hashed':sqlalchemy.types.VARCHAR(length=64),\n",
    "                        'department_id':sqlalchemy.types.VARCHAR(length=16),\n",
    "                        'sales':sqlalchemy.types.DECIMAL(precision=10,scale=2,asdecimal=True),\n",
    "                        'units':sqlalchemy.types.INTEGER()\n",
    "                    })\n",
    "\n",
    "    \n",
    "    i_counter+=1\n",
    "    print(\"done of file: \",i_counter,file)\n",
    "    logging.info(\"done of file: \"+str(i_counter)+\" | \"+file)\n",
    "\n",
    "    total_sales+=df['sales'].sum()\n",
    "    print(datetime.datetime.now(),\"done\",file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done of write to sql: 2020-04-05 21:24:12.891180\n",
      "total_sales 85042062.61000018\n"
     ]
    }
   ],
   "source": [
    "print(\"done of write to sql: \"+str(datetime.datetime.now()))\n",
    "logging.info(\"done of write to sql: \"+str(datetime.datetime.now()))\n",
    "print('total_sales',total_sales)\n",
    "logging.info('total_sales'+str(total_sales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred_CRM_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13338163, 6)\n",
      "(13338147, 6)\n"
     ]
    }
   ],
   "source": [
    "Q1_2018_Start_Date=\"'2018-02-03'\"\n",
    "df_all_since_18Q1=pd.read_sql(\"select customer_id_hashed, email_address_hash, sign_up_date, sign_up_channel, \\\n",
    "sign_up_location, customer_zip_code from BL_Rewards_Master \\\n",
    "where sign_up_date >= %s\\\n",
    "order by customer_id_hashed, sign_up_date desc;\" %Q1_2018_Start_Date,con=BL_engine)\n",
    "print(df_all_since_18Q1.shape)\n",
    "df_all_since_18Q1=df_all_since_18Q1.drop_duplicates(\"customer_id_hashed\")\n",
    "print(df_all_since_18Q1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-03 2020-02-29\n",
      "13338147\n"
     ]
    }
   ],
   "source": [
    "print(df_all_since_18Q1['sign_up_date'].min(),df_all_since_18Q1['sign_up_date'].max())\n",
    "print(df_all_since_18Q1['customer_id_hashed'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_since_18Q1['Registration_Online']=np.where(df_all_since_18Q1['sign_up_channel']==\"Online\",1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_since_18Q1.to_sql(\"Pred_CRM_table\",if_exists='replace', con=BL_engine, index=False,chunksize=300000,\n",
    "                    dtype={\n",
    "                        'customer_id_hashed':sqlalchemy.types.VARCHAR(length=64),\n",
    "                        'email_address_hash':sqlalchemy.types.VARCHAR(length=64),\n",
    "                        'sign_up_date':sqlalchemy.Date(),\n",
    "                        'sign_up_channel':sqlalchemy.types.VARCHAR(length=64),\n",
    "                        'sign_up_location':sqlalchemy.types.INTEGER(),\n",
    "                        'customer_zip_code':sqlalchemy.types.VARCHAR(length=16),\n",
    "                        'Registration_Online':sqlalchemy.types.INTEGER()\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-04 19:12:38.725712\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min(sign_up_date)</th>\n",
       "      <th>max(sign_up_date)</th>\n",
       "      <th>count(customer_id_hashed)</th>\n",
       "      <th>count(distinct customer_id_hashed)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>13338147</td>\n",
       "      <td>13338147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  min(sign_up_date) max(sign_up_date)  count(customer_id_hashed)  \\\n",
       "0        2018-02-03        2020-02-29                   13338147   \n",
       "\n",
       "   count(distinct customer_id_hashed)  \n",
       "0                            13338147  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"select min(sign_up_date), max(sign_up_date), count(customer_id_hashed), count(distinct customer_id_hashed) from Pred_CRM_table;\",con=BL_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
